# FalkorDBClient.vector_search - Function Specification

**Level:** 3 (Function)
**Component:** FalkorDBClient
**Module:** zapomni_db
**Author:** Goncharenko Anton aka alienxs2
**Status:** Draft
**Version:** 1.0
**Date:** 2025-11-23

## Function Signature

```python
def vector_search(
    self,
    embedding: List[float],
    limit: int = 10,
    filters: Optional[Dict[str, Any]] = None
) -> List[SearchResult]:
    """
    Perform HNSW vector similarity search on chunk embeddings.

    Executes approximate nearest neighbor search using FalkorDB's HNSW
    (Hierarchical Navigable Small World) index with cosine similarity.
    Returns chunks ranked by similarity score, optionally filtered by metadata.

    This is the primary search interface for semantic memory retrieval in Zapomni.
    The function leverages FalkorDB's unified vector+graph architecture to perform
    fast vector search while maintaining access to rich graph metadata.

    Args:
        embedding: Query embedding vector (must be 768-dimensional float32)
            - Generated by sentence-transformers model (all-MiniLM-L6-v2)
            - Should be normalized for cosine similarity
            - Example: [0.1234, -0.5678, 0.9012, ..., 0.3456] (768 elements)

        limit: Maximum number of results to return (default: 10)
            - Valid range: 1 to 1000
            - Higher values increase search time linearly
            - Recommended: 10-50 for interactive use, up to 1000 for batch
            - Example: limit=20 returns top 20 most similar chunks

        filters: Optional metadata filters to apply (default: None)
            - Structure:
                {
                    "tags": List[str],        # Match ANY tag (OR logic)
                    "source": str,            # Exact match on source field
                    "date_from": str,         # ISO 8601, inclusive (>=)
                    "date_to": str,           # ISO 8601, inclusive (<=)
                    "min_similarity": float   # Minimum score threshold 0.0-1.0
                }
            - All filters are optional and combined with AND logic
            - Empty dict {} means no filtering (same as None)
            - Example:
                {
                    "tags": ["python", "ai"],
                    "source": "documentation",
                    "date_from": "2025-11-01T00:00:00Z",
                    "min_similarity": 0.7
                }

    Returns:
        List[SearchResult]: Ranked search results sorted by similarity (descending)
            - Empty list [] if no results match criteria
            - Each SearchResult contains:
                - memory_id (str): UUID of parent Memory node
                - chunk_id (str): UUID of matching Chunk node
                - text (str): Chunk text content
                - similarity_score (float): Cosine similarity (0.0-1.0)
                - tags (List[str]): Memory tags from metadata
                - source (str): Memory source identifier
                - timestamp (datetime): Memory creation timestamp (UTC)
                - chunk_index (int): Chunk position within memory (0-based)
            - Guarantees:
                - Results sorted by similarity_score DESC
                - All scores >= filters["min_similarity"] if specified
                - Length <= limit parameter
                - No duplicate chunks

    Raises:
        ValidationError: When input validation fails:
            - embedding dimension != 768
            - limit < 1 or > 1000
            - filters["min_similarity"] not in [0.0, 1.0]
            - filters["date_from"]/["date_to"] invalid ISO 8601 format
            - embedding contains NaN or Inf values

        QueryError: When database query fails:
            - FalkorDB connection lost during query
            - HNSW index not initialized
            - Query timeout (> 30 seconds)
            - Invalid Cypher query generation

        DatabaseError: When database operation fails after retries:
            - All 3 retry attempts exhausted for transient errors
            - Non-recoverable database errors

    Example:
        ```python
        # Initialize client
        client = FalkorDBClient(host="localhost", port=6379)

        # Basic search - top 10 similar chunks
        query_embedding = embedder.embed("What is Python?")
        results = client.vector_search(
            embedding=query_embedding,
            limit=10
        )

        for result in results:
            print(f"{result.similarity_score:.3f}: {result.text}")
        # Output:
        # 0.952: Python is a high-level programming language
        # 0.887: Python emphasizes code readability
        # 0.834: Python supports multiple programming paradigms

        # Filtered search - Python docs from last month, high similarity
        results = client.vector_search(
            embedding=query_embedding,
            limit=5,
            filters={
                "tags": ["python", "programming"],
                "source": "documentation",
                "date_from": "2025-10-23T00:00:00Z",
                "min_similarity": 0.8
            }
        )

        # Empty result handling
        if not results:
            print("No matching memories found")
        else:
            print(f"Found {len(results)} relevant memories")
        ```
    """
```

## Purpose & Context

### What It Does

`vector_search()` performs semantic similarity search on stored memory chunks using vector embeddings. It:

1. **Validates Input**: Ensures embedding has correct dimensions (768), limit is within bounds, and filters are well-formed
2. **Builds Cypher Query**: Constructs FalkorDB vector search query using `db.idx.vector.queryNodes()` function
3. **Applies Filters**: Adds WHERE clauses for metadata filtering (tags, source, date range, similarity threshold)
4. **Executes Search**: Runs HNSW approximate nearest neighbor search with cosine similarity metric
5. **Parses Results**: Converts FalkorDB result rows into `SearchResult` objects
6. **Sorts & Returns**: Returns results sorted by similarity score (descending)

The function bridges the gap between high-dimensional vector space (embeddings) and human-readable text (memories), enabling semantic search where exact keyword matching would fail.

### Why It Exists

**Primary Use Case**: Semantic memory retrieval for RAG (Retrieval-Augmented Generation)

When a user asks a question:
1. Question is embedded into vector space (768-dim)
2. `vector_search()` finds semantically similar memories
3. Relevant memories are provided as context to LLM
4. LLM generates answer grounded in retrieved memories

**Technical Reason**: FalkorDB combines vector search and graph database in one system, allowing:
- Fast approximate NN search (HNSW algorithm)
- Rich metadata filtering (graph properties)
- Relationship traversal (future: expand search via entity connections)
- Single query to get both vectors and metadata (no separate vector DB + graph DB)

### When To Use

**Primary Scenarios:**
- User queries memory system ("What did I learn about Python?")
- Contextual memory retrieval for conversations
- Finding related memories for knowledge expansion
- Semantic deduplication (search before adding new memory)

**Invoked By:**
- `zapomni_core.QueryEngine.search()` - Main search orchestrator
- `zapomni_core.MemoryProcessor.find_duplicates()` - Duplicate detection
- `zapomni_mcp.SearchMemoryTool.execute()` - MCP search tool

**Call Pattern:**
```python
# High-level flow
query_text → embedder.embed() → vector_search() → results
```

### When NOT To Use

**Avoid if:**
- **Exact Keyword Search Needed**: Use full-text search or regex instead (future: BM25 hybrid search)
- **No Embedding Available**: Requires pre-computed query embedding (must call embedder first)
- **Non-Semantic Search**: For structured queries (find all Python memories), use `graph_query()` directly
- **Very Large Result Sets**: Vector search not optimized for limit > 1000 (use pagination/streaming)

**Alternative Methods:**
- `graph_query()` - For complex graph traversals or exact matches
- `get_related_entities()` - For entity-centric search
- Future: `hybrid_search()` - Combine vector + BM25 + graph traversal

## Parameters (Detailed)

### embedding: List[float]

**Type:** `List[float]`

**Purpose:** Query vector for semantic similarity search

**Constraints:**
- **Dimension**: Must be exactly 768 elements (validate with `len(embedding) == 768`)
- **Type**: All elements must be float (int accepted, auto-cast to float)
- **Values**: No NaN or Inf allowed (validate with `all(math.isfinite(x) for x in embedding)`)
- **Range**: Typically normalized to [-1, 1] but not enforced (model-dependent)

**Validation:**
```python
if len(embedding) != 768:
    raise ValidationError(
        f"embedding dimension must be 768, got {len(embedding)}"
    )

if not all(isinstance(x, (int, float)) for x in embedding):
    raise ValidationError("embedding must contain only numeric values")

if not all(math.isfinite(x) for x in embedding):
    raise ValidationError("embedding contains NaN or Inf values")
```

**Examples:**

Valid:
```python
# Typical normalized embedding
[0.1234, -0.5678, 0.9012, ..., 0.3456]  # 768 floats

# Integer values (auto-cast to float)
[1, 0, -1, ..., 0]  # 768 ints

# Edge case: all zeros (valid but poor for search)
[0.0] * 768
```

Invalid:
```python
# Wrong dimension
[0.1] * 512  # Only 512 dims → ValidationError

# Contains NaN
embedding = [0.1] * 768
embedding[0] = float('nan')  # → ValidationError

# Contains Inf
embedding = [0.1] * 768
embedding[100] = float('inf')  # → ValidationError

# Wrong type
"not a list"  # → TypeError (before ValidationError)

# Empty list
[]  # → ValidationError
```

**Performance Note:**
- Embedding is serialized to FalkorDB query parameter
- Large dimension (768) means ~3KB per query (768 floats × 4 bytes)
- Consider caching embeddings if same query used repeatedly

---

### limit: int

**Type:** `int`

**Purpose:** Maximum number of search results to return

**Default:** `10`

**Constraints:**
- **Minimum**: 1 (must return at least one result if matches exist)
- **Maximum**: 1000 (hard limit to prevent memory/performance issues)
- **Type**: Must be integer (no floats, even if x.0)

**Validation:**
```python
if not isinstance(limit, int):
    raise ValidationError(
        f"limit must be int, got {type(limit).__name__}"
    )

if limit < 1:
    raise ValidationError(
        f"limit must be >= 1, got {limit}"
    )

if limit > 1000:
    raise ValidationError(
        f"limit cannot exceed 1000, got {limit}"
    )
```

**Examples:**

Valid:
```python
limit=1       # Return single best match
limit=10      # Default, good for interactive search
limit=50      # Larger result set for context
limit=1000    # Maximum allowed (batch processing)
```

Invalid:
```python
limit=0       # Zero results → ValidationError
limit=-5      # Negative → ValidationError
limit=1001    # Exceeds max → ValidationError
limit=10.5    # Float → ValidationError
limit="10"    # String → ValidationError
```

**Performance Characteristics:**
- Search time: O(log k) where k = total chunks (HNSW)
- Result parsing: O(limit) linear in result count
- Network transfer: ~1KB per result × limit
- Recommended limits by use case:
  - Interactive search: 10-20
  - RAG context: 5-10 (quality over quantity)
  - Batch processing: 100-1000
  - Similarity threshold: Use `filters["min_similarity"]` instead of large limit

**Default Rationale:**
- `limit=10` balances relevance vs. coverage
- Most relevant results typically in top 10
- Reduces noise from low-similarity matches
- Keeps response time < 50ms for 10K chunks

---

### filters: Optional[Dict[str, Any]]

**Type:** `Optional[Dict[str, Any]]`

**Purpose:** Optional metadata filters to narrow search results

**Default:** `None` (no filtering, search all chunks)

**Structure (when provided):**
```python
{
    "tags": List[str],           # Optional: filter by tags (ANY match)
    "source": str,               # Optional: filter by source (exact match)
    "date_from": str,            # Optional: ISO 8601, inclusive (>=)
    "date_to": str,              # Optional: ISO 8601, inclusive (<=)
    "min_similarity": float      # Optional: similarity threshold 0.0-1.0
}
```

**All fields are optional** - can provide any subset:
```python
# Only tags filter
{"tags": ["python"]}

# Only date range
{"date_from": "2025-11-01T00:00:00Z", "date_to": "2025-11-23T23:59:59Z"}

# Combined filters
{"tags": ["ai"], "source": "documentation", "min_similarity": 0.8}

# Empty dict = no filtering (same as None)
{}
```

**Field Details:**

#### filters["tags"]: List[str]

- **Purpose**: Filter memories by tag metadata (OR logic within tags)
- **Type**: List of strings
- **Matching**: Memory matches if ANY tag in filter list matches ANY tag in memory.tags
- **Constraints**:
  - Non-empty list (empty list → ValidationError)
  - Each tag: non-empty string, max 50 chars
  - Case-sensitive matching
- **Examples**:
  ```python
  # Match memories with "python" OR "ai" tags
  {"tags": ["python", "ai"]}

  # Match memories with exactly "machine-learning" tag
  {"tags": ["machine-learning"]}
  ```

#### filters["source"]: str

- **Purpose**: Filter memories by source identifier (exact match)
- **Type**: String
- **Matching**: Exact string match (case-sensitive)
- **Constraints**:
  - Non-empty string
  - Max 100 chars
- **Examples**:
  ```python
  # Only documentation memories
  {"source": "documentation"}

  # Only user chat inputs
  {"source": "chat"}
  ```

#### filters["date_from"]: str

- **Purpose**: Filter memories created on or after this timestamp
- **Type**: ISO 8601 datetime string
- **Format**: `YYYY-MM-DDTHH:MM:SSZ` (UTC timezone, Z suffix required)
- **Matching**: `memory.timestamp >= date_from`
- **Constraints**:
  - Valid ISO 8601 format
  - Must include timezone (Z or ±HH:MM)
  - Cannot be in future (warning, not error)
- **Examples**:
  ```python
  # Memories from November 2025 onwards
  {"date_from": "2025-11-01T00:00:00Z"}

  # Memories from last hour
  {"date_from": "2025-11-23T09:00:00Z"}
  ```

#### filters["date_to"]: str

- **Purpose**: Filter memories created on or before this timestamp
- **Type**: ISO 8601 datetime string
- **Format**: Same as date_from
- **Matching**: `memory.timestamp <= date_to`
- **Constraints**: Same as date_from
- **Examples**:
  ```python
  # Memories up to October 2025
  {"date_to": "2025-10-31T23:59:59Z"}

  # Date range: November 1-15
  {
      "date_from": "2025-11-01T00:00:00Z",
      "date_to": "2025-11-15T23:59:59Z"
  }
  ```

#### filters["min_similarity"]: float

- **Purpose**: Minimum cosine similarity threshold (filter low-quality matches)
- **Type**: Float
- **Range**: 0.0 (no similarity) to 1.0 (perfect match)
- **Matching**: Only return results where `similarity_score >= min_similarity`
- **Constraints**:
  - Must be in [0.0, 1.0]
  - Typically use 0.5-0.9 for quality filtering
- **Examples**:
  ```python
  # High quality matches only
  {"min_similarity": 0.8}

  # Moderate threshold
  {"min_similarity": 0.5}

  # No threshold (equivalent to 0.0 or omitting)
  {"min_similarity": 0.0}
  ```

**Validation:**
```python
if filters is not None:
    if not isinstance(filters, dict):
        raise ValidationError("filters must be dict or None")

    # Validate tags
    if "tags" in filters:
        tags = filters["tags"]
        if not isinstance(tags, list):
            raise ValidationError("filters['tags'] must be list")
        if len(tags) == 0:
            raise ValidationError("filters['tags'] cannot be empty list")
        if not all(isinstance(t, str) and t for t in tags):
            raise ValidationError("filters['tags'] must contain non-empty strings")
        if any(len(t) > 50 for t in tags):
            raise ValidationError("filters['tags'] entries max 50 chars")

    # Validate source
    if "source" in filters:
        source = filters["source"]
        if not isinstance(source, str) or not source:
            raise ValidationError("filters['source'] must be non-empty string")
        if len(source) > 100:
            raise ValidationError("filters['source'] max 100 chars")

    # Validate date_from
    if "date_from" in filters:
        try:
            datetime.fromisoformat(filters["date_from"].replace('Z', '+00:00'))
        except ValueError as e:
            raise ValidationError(
                f"filters['date_from'] invalid ISO 8601 format: {e}"
            )

    # Validate date_to
    if "date_to" in filters:
        try:
            datetime.fromisoformat(filters["date_to"].replace('Z', '+00:00'))
        except ValueError as e:
            raise ValidationError(
                f"filters['date_to'] invalid ISO 8601 format: {e}"
            )

    # Validate min_similarity
    if "min_similarity" in filters:
        min_sim = filters["min_similarity"]
        if not isinstance(min_sim, (int, float)):
            raise ValidationError("filters['min_similarity'] must be numeric")
        if not (0.0 <= min_sim <= 1.0):
            raise ValidationError(
                f"filters['min_similarity'] must be in [0.0, 1.0], got {min_sim}"
            )
```

**Examples:**

Valid:
```python
None  # No filtering

{}  # Empty dict, no filtering

{"tags": ["python"]}

{"tags": ["python", "ai"], "source": "documentation"}

{
    "tags": ["machine-learning"],
    "date_from": "2025-11-01T00:00:00Z",
    "date_to": "2025-11-23T23:59:59Z",
    "min_similarity": 0.75
}
```

Invalid:
```python
{"tags": []}  # Empty tags list → ValidationError

{"tags": "python"}  # Not a list → ValidationError

{"source": ""}  # Empty source → ValidationError

{"date_from": "2025-11-01"}  # Missing time → ValidationError

{"date_from": "invalid"}  # Invalid format → ValidationError

{"min_similarity": 1.5}  # Out of range → ValidationError

{"unknown_filter": "value"}  # Unknown key → ignored (permissive)
```

**Performance Impact:**
- No filters: Full vector search (fast, O(log k))
- Tags filter: Adds WHERE clause, minimal overhead
- Source filter: Adds WHERE clause, minimal overhead
- Date range: Adds WHERE clause, requires timestamp index for speed
- min_similarity: Filters results post-search, no overhead

**Filter Combination (AND logic):**
All filters are combined with AND:
```python
filters = {
    "tags": ["python", "ai"],        # (tag=python OR tag=ai)
    "source": "documentation",       # AND source=documentation
    "date_from": "2025-11-01T00:00:00Z",  # AND timestamp >= date_from
    "min_similarity": 0.8            # AND similarity >= 0.8
}
# Result: memories matching ALL conditions
```

## Return Value

**Type:** `List[SearchResult]`

**Structure:**
```python
@dataclass
class SearchResult:
    """Single search result from vector similarity search."""

    memory_id: str               # UUID of parent Memory node
    chunk_id: str                # UUID of matching Chunk node
    text: str                    # Chunk text content
    similarity_score: float      # Cosine similarity (0.0-1.0)
    tags: List[str]              # Memory tags from metadata
    source: str                  # Memory source identifier
    timestamp: datetime          # Memory creation time (UTC)
    chunk_index: int             # Chunk position within memory (0-based)
```

**Return Guarantees:**

1. **Sorted**: Results always sorted by `similarity_score` in descending order (highest first)
2. **Length**: `len(results) <= limit` (may be less if fewer matches exist)
3. **Empty**: Returns `[]` if no results match criteria (not None, not error)
4. **No Duplicates**: Each chunk appears at most once in results
5. **Metadata Complete**: All fields populated (no None values except in edge cases)
6. **Score Range**: All `similarity_score` values in [0.0, 1.0]
7. **Filter Compliance**: All results satisfy filter criteria (if provided)

**Success Case (typical):**
```python
[
    SearchResult(
        memory_id="550e8400-e29b-41d4-a716-446655440000",
        chunk_id="660e8400-e29b-41d4-a716-446655440001",
        text="Python is a high-level programming language",
        similarity_score=0.952,
        tags=["python", "programming"],
        source="documentation",
        timestamp=datetime(2025, 11, 23, 10, 0, 0, tzinfo=timezone.utc),
        chunk_index=0
    ),
    SearchResult(
        memory_id="550e8400-e29b-41d4-a716-446655440000",
        chunk_id="660e8400-e29b-41d4-a716-446655440002",
        text="Python emphasizes code readability",
        similarity_score=0.887,
        tags=["python", "programming"],
        source="documentation",
        timestamp=datetime(2025, 11, 23, 10, 0, 0, tzinfo=timezone.utc),
        chunk_index=1
    ),
    SearchResult(
        memory_id="770e8400-e29b-41d4-a716-446655440003",
        chunk_id="880e8400-e29b-41d4-a716-446655440004",
        text="Python supports multiple programming paradigms",
        similarity_score=0.834,
        tags=["python", "paradigms"],
        source="user_input",
        timestamp=datetime(2025, 11, 22, 15, 30, 0, tzinfo=timezone.utc),
        chunk_index=0
    )
]
```

**Empty Result (no matches):**
```python
[]  # Empty list (not None, not error)
```

**Single Result:**
```python
[
    SearchResult(
        memory_id="...",
        chunk_id="...",
        text="Only one matching chunk found",
        similarity_score=0.65,
        tags=["rare-topic"],
        source="chat",
        timestamp=datetime(2025, 11, 20, 8, 0, 0, tzinfo=timezone.utc),
        chunk_index=0
    )
]
```

**Field Details:**

- **memory_id**: UUID of parent Memory node (source of chunk)
  - Format: `"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"` (UUID v4)
  - Can be used with `graph_query()` to retrieve full memory
  - Multiple chunks from same memory will have same memory_id

- **chunk_id**: UUID of specific Chunk node
  - Format: Same as memory_id
  - Unique identifier for this chunk
  - Used for deduplication and exact retrieval

- **text**: Actual text content of the chunk
  - Type: str
  - Length: 1-10,000 chars (depends on chunking strategy)
  - May include partial sentences (chunk boundaries)
  - Example: "Python is a high-level programming language"

- **similarity_score**: Cosine similarity between query and chunk embeddings
  - Type: float
  - Range: 0.0 (orthogonal) to 1.0 (identical)
  - Typical range: 0.3-0.95 for real results
  - Score interpretation:
    - 0.9-1.0: Very high similarity (near-exact semantic match)
    - 0.7-0.9: High similarity (strong relevance)
    - 0.5-0.7: Moderate similarity (related content)
    - 0.3-0.5: Low similarity (weak relation)
    - 0.0-0.3: Very low (likely irrelevant)

- **tags**: List of tags associated with parent memory
  - Type: List[str]
  - Source: memory.metadata["tags"]
  - Empty list `[]` if no tags
  - Example: `["python", "programming", "tutorial"]`

- **source**: Source identifier of parent memory
  - Type: str
  - Source: memory.metadata["source"]
  - Common values: "documentation", "chat", "user_input", "web_scrape"
  - Example: `"documentation"`

- **timestamp**: Creation timestamp of parent memory (UTC)
  - Type: datetime (timezone-aware, UTC)
  - Source: memory.created_at
  - Format: `datetime(2025, 11, 23, 10, 0, 0, tzinfo=timezone.utc)`
  - Used for date filtering and recency sorting

- **chunk_index**: Position of chunk within parent memory
  - Type: int (0-based)
  - Range: 0 to (num_chunks - 1)
  - Used to reconstruct chunk order for context
  - Example: chunk_index=0 is first chunk, chunk_index=1 is second

**Usage Pattern:**
```python
results = client.vector_search(embedding, limit=10)

if not results:
    print("No matching memories found")
else:
    print(f"Found {len(results)} relevant memories:")
    for i, result in enumerate(results, 1):
        print(f"{i}. [{result.similarity_score:.3f}] {result.text}")
        print(f"   Source: {result.source}, Tags: {result.tags}")
        print(f"   Created: {result.timestamp.isoformat()}")
        print()

# Access top result
if results:
    best_match = results[0]
    print(f"Best match (score={best_match.similarity_score:.3f}):")
    print(best_match.text)

# Filter by minimum score
high_quality = [r for r in results if r.similarity_score >= 0.8]
print(f"{len(high_quality)} high-quality matches (score >= 0.8)")
```

## Exceptions

### ValidationError

**When Raised:**
1. `embedding` dimension != 768
2. `embedding` contains NaN or Inf
3. `limit` < 1 or > 1000
4. `limit` not an integer
5. `filters["tags"]` is empty list or contains non-strings
6. `filters["source"]` is empty string or > 100 chars
7. `filters["date_from"]` or `filters["date_to"]` invalid ISO 8601 format
8. `filters["min_similarity"]` not in [0.0, 1.0]

**Message Format:**
```python
f"Validation failed: {specific_reason}"
```

**Examples:**
```python
# Wrong dimension
raise ValidationError(
    "Validation failed: embedding dimension must be 768, got 512"
)

# Invalid limit
raise ValidationError(
    "Validation failed: limit must be >= 1, got 0"
)

# Invalid filter
raise ValidationError(
    "Validation failed: filters['date_from'] invalid ISO 8601 format: ..."
)
```

**Recovery:**
- **Not retryable** - fix input parameters at caller
- Log error with full context (embedding dim, limit, filters)
- Return error to user with actionable message

---

### QueryError

**When Raised:**
1. Cypher query generation fails (internal bug)
2. HNSW vector index not initialized
3. Query execution timeout (> 30 seconds)
4. FalkorDB returns malformed result
5. Vector index corruption detected

**Message Format:**
```python
f"Query execution failed: {error_details}"
```

**Examples:**
```python
# Index not found
raise QueryError(
    "Query execution failed: HNSW index 'chunk_embedding_idx' does not exist"
)

# Query timeout
raise QueryError(
    "Query execution failed: query timeout after 30 seconds"
)

# Malformed result
raise QueryError(
    "Query execution failed: result missing required field 'score'"
)
```

**Recovery:**
- **May be retryable** for transient errors (timeout, connection)
- **Not retryable** for index errors (requires schema reinitialization)
- Log full query and parameters for debugging
- Escalate to operator if index missing

---

### DatabaseError

**When Raised:**
1. FalkorDB connection lost during query execution
2. All 3 retry attempts exhausted for transient errors
3. Database disk full (write errors)
4. Unexpected database exception (catch-all)

**Inheritance:**
```python
QueryError inherits from DatabaseError
ConnectionError inherits from DatabaseError
```

**Message Format:**
```python
f"Database operation failed after {retries} retries: {last_error}"
```

**Examples:**
```python
# Connection lost
raise DatabaseError(
    "Database operation failed after 3 retries: connection reset by peer"
)

# Disk full
raise DatabaseError(
    "Database operation failed: disk quota exceeded"
)
```

**Recovery:**
- Automatic retry with exponential backoff (1s, 2s, 4s)
- Max 3 retry attempts
- If all retries fail → propagate to caller
- Caller (zapomni_core) can retry entire operation or fail gracefully

**Retry Logic (internal):**
```python
for attempt in range(1, max_retries + 1):
    try:
        return execute_query(...)
    except (ConnectionError, TimeoutError) as e:
        if attempt == max_retries:
            raise DatabaseError(f"Failed after {max_retries} retries: {e}")
        delay = 2 ** (attempt - 1)  # 1s, 2s, 4s
        logger.warning(f"Retry {attempt}/{max_retries} after {delay}s: {e}")
        time.sleep(delay)
```

## Algorithm (Pseudocode)

```
FUNCTION vector_search(embedding, limit, filters):
    """
    Execute HNSW vector similarity search with optional filters.

    High-level steps:
    1. Validate inputs
    2. Build Cypher query with vector search + filters
    3. Execute query with retry logic
    4. Parse results into SearchResult objects
    5. Sort by similarity (descending)
    6. Return results
    """

    # ===== STEP 1: INPUT VALIDATION =====

    # Validate embedding dimension
    IF len(embedding) != 768:
        RAISE ValidationError("embedding dimension must be 768, got {len}")

    # Validate embedding values (no NaN/Inf)
    FOR value IN embedding:
        IF NOT isfinite(value):
            RAISE ValidationError("embedding contains NaN or Inf values")

    # Validate limit range
    IF NOT isinstance(limit, int):
        RAISE ValidationError("limit must be int")
    IF limit < 1 OR limit > 1000:
        RAISE ValidationError("limit must be in [1, 1000], got {limit}")

    # Validate filters (if provided)
    IF filters IS NOT None:
        IF NOT isinstance(filters, dict):
            RAISE ValidationError("filters must be dict or None")

        # Validate each filter field
        IF "tags" IN filters:
            IF NOT isinstance(filters["tags"], list) OR len(filters["tags"]) == 0:
                RAISE ValidationError("filters['tags'] must be non-empty list")
            FOR tag IN filters["tags"]:
                IF NOT isinstance(tag, str) OR tag == "" OR len(tag) > 50:
                    RAISE ValidationError("invalid tag in filters['tags']")

        IF "source" IN filters:
            IF NOT isinstance(filters["source"], str) OR filters["source"] == "":
                RAISE ValidationError("filters['source'] must be non-empty string")
            IF len(filters["source"]) > 100:
                RAISE ValidationError("filters['source'] max 100 chars")

        IF "date_from" IN filters:
            TRY:
                parse_iso8601(filters["date_from"])
            CATCH ValueError:
                RAISE ValidationError("filters['date_from'] invalid ISO 8601")

        IF "date_to" IN filters:
            TRY:
                parse_iso8601(filters["date_to"])
            CATCH ValueError:
                RAISE ValidationError("filters['date_to'] invalid ISO 8601")

        IF "min_similarity" IN filters:
            min_sim = filters["min_similarity"]
            IF NOT isinstance(min_sim, (int, float)):
                RAISE ValidationError("filters['min_similarity'] must be numeric")
            IF NOT (0.0 <= min_sim <= 1.0):
                RAISE ValidationError("filters['min_similarity'] must be in [0.0, 1.0]")

    # ===== STEP 2: BUILD CYPHER QUERY =====

    # Start with base vector search query
    # Use FalkorDB's db.idx.vector.queryNodes() function
    cypher = """
        CALL db.idx.vector.queryNodes(
            'Chunk',                    -- Node label
            'embedding',                -- Property containing vector
            $limit,                     -- Number of results
            $embedding                  -- Query vector
        ) YIELD node, score
    """

    # Add WHERE clauses for filters
    where_clauses = ["score IS NOT NULL"]  # Basic sanity check

    IF filters IS NOT None:
        # Tag filter (OR logic within tags)
        IF "tags" IN filters:
            # MATCH parent Memory node to access tags
            cypher += """
                MATCH (node)<-[:HAS_CHUNK]-(memory:Memory)
            """
            # Build tag condition: ANY(tag IN memory.tags WHERE tag IN $filter_tags)
            where_clauses.append(
                "ANY(tag IN memory.tags WHERE tag IN $filter_tags)"
            )
        ELSE:
            # Still need to MATCH Memory for metadata
            cypher += """
                MATCH (node)<-[:HAS_CHUNK]-(memory:Memory)
            """

        # Source filter (exact match)
        IF "source" IN filters:
            where_clauses.append("memory.source = $filter_source")

        # Date range filters
        IF "date_from" IN filters:
            where_clauses.append("memory.timestamp >= $filter_date_from")

        IF "date_to" IN filters:
            where_clauses.append("memory.timestamp <= $filter_date_to")

        # Minimum similarity threshold
        IF "min_similarity" IN filters:
            where_clauses.append("score >= $filter_min_similarity")
    ELSE:
        # No filters, still need Memory for metadata
        cypher += """
            MATCH (node)<-[:HAS_CHUNK]-(memory:Memory)
        """

    # Combine WHERE clauses with AND
    IF len(where_clauses) > 0:
        cypher += "WHERE " + " AND ".join(where_clauses) + "\n"

    # Return all fields needed for SearchResult
    cypher += """
        RETURN
            memory.id AS memory_id,
            node.id AS chunk_id,
            node.text AS text,
            score AS similarity_score,
            memory.tags AS tags,
            memory.source AS source,
            memory.timestamp AS timestamp,
            node.index AS chunk_index
        ORDER BY similarity_score DESC
    """

    # Build parameters dict
    parameters = {
        "embedding": embedding,
        "limit": limit
    }

    IF filters IS NOT None:
        IF "tags" IN filters:
            parameters["filter_tags"] = filters["tags"]
        IF "source" IN filters:
            parameters["filter_source"] = filters["source"]
        IF "date_from" IN filters:
            parameters["filter_date_from"] = filters["date_from"]
        IF "date_to" IN filters:
            parameters["filter_date_to"] = filters["date_to"]
        IF "min_similarity" IN filters:
            parameters["filter_min_similarity"] = filters["min_similarity"]

    # ===== STEP 3: EXECUTE QUERY WITH RETRY =====

    start_time = current_time()

    TRY:
        result = _retry_operation(
            func=_execute_cypher,
            query=cypher,
            parameters=parameters
        )
    EXCEPT QuerySyntaxError AS e:
        # Query syntax error (should never happen if query builder correct)
        LOG_ERROR("vector_search_query_syntax_error", error=str(e), query=cypher)
        RAISE QueryError(f"Query execution failed: {e}")
    EXCEPT ConnectionError AS e:
        # Connection lost, retries exhausted
        LOG_ERROR("vector_search_connection_error", error=str(e), retries=3)
        RAISE DatabaseError(f"Database operation failed after 3 retries: {e}")
    EXCEPT Exception AS e:
        # Unexpected error
        LOG_ERROR("vector_search_unexpected_error", error=str(e), type=type(e))
        RAISE DatabaseError(f"Unexpected database error: {e}")

    execution_time_ms = (current_time() - start_time) * 1000

    # ===== STEP 4: PARSE RESULTS =====

    search_results = []

    FOR row IN result.rows:
        TRY:
            # Extract fields from row
            memory_id = row["memory_id"]
            chunk_id = row["chunk_id"]
            text = row["text"]
            similarity_score = float(row["similarity_score"])
            tags = row["tags"] IF row["tags"] IS NOT None ELSE []
            source = row["source"] IF row["source"] IS NOT None ELSE ""
            timestamp_str = row["timestamp"]
            chunk_index = int(row["chunk_index"])

            # Parse timestamp from ISO 8601 string to datetime
            timestamp = parse_iso8601(timestamp_str)

            # Create SearchResult object
            search_result = SearchResult(
                memory_id=memory_id,
                chunk_id=chunk_id,
                text=text,
                similarity_score=similarity_score,
                tags=tags,
                source=source,
                timestamp=timestamp,
                chunk_index=chunk_index
            )

            search_results.append(search_result)

        EXCEPT (KeyError, ValueError, TypeError) AS e:
            # Malformed row (missing field or wrong type)
            LOG_WARNING("vector_search_malformed_row", row=row, error=str(e))
            # Skip this row, continue with next
            CONTINUE

    # ===== STEP 5: SORT BY SIMILARITY (should already be sorted by query) =====

    # Double-check sort order (query has ORDER BY, but ensure correctness)
    search_results.sort(key=lambda r: r.similarity_score, reverse=True)

    # ===== STEP 6: LOG AND RETURN =====

    LOG_INFO(
        "vector_search_complete",
        results_count=len(search_results),
        execution_time_ms=execution_time_ms,
        had_filters=bool(filters),
        limit=limit
    )

    RETURN search_results

END FUNCTION
```

## Preconditions

Before calling `vector_search()`, the following must be true:

1. **Client Initialized**
   - `FalkorDBClient.__init__()` called successfully
   - Connection to FalkorDB established
   - No connection errors during initialization

2. **Schema Initialized**
   - `_init_schema()` completed successfully
   - HNSW vector index created on Chunk.embedding property
   - Index name: `chunk_embedding_idx`
   - Index parameters: dimension=768, similarity=cosine, m=16, ef_construction=200

3. **Database Accessible**
   - FalkorDB server running and reachable at host:port
   - Network connectivity available
   - No authentication errors (if password required)

4. **Graph Exists**
   - Graph with name `graph_name` exists in FalkorDB
   - Graph contains at least schema (may have zero data nodes)

5. **Valid Query Embedding**
   - Caller has generated 768-dimensional embedding
   - Embedding generated by same model as stored embeddings (all-MiniLM-L6-v2)
   - Embedding is semantically meaningful (not random noise)

**Validation:**
```python
# Client checks these preconditions in __init__:
assert self.db is not None, "Database connection required"
assert self.graph is not None, "Graph instance required"
assert self._schema_initialized, "Schema must be initialized before search"
```

**Caller Responsibilities:**
- Generate embedding from query text using same embedder model
- Ensure embedding has correct dimensionality (768)
- Provide valid limit and filters (if any)

## Postconditions

After `vector_search()` completes successfully:

1. **Return Value Guarantees**
   - Returns List[SearchResult] (never None)
   - Empty list `[]` if no results (not error condition)
   - All results satisfy filter criteria (if filters provided)
   - Results sorted by similarity_score descending
   - Length <= limit parameter

2. **Database State**
   - Database state unchanged (read-only operation)
   - No side effects (no writes, no schema changes)
   - Connection pool returned to ready state
   - No resource leaks (connections properly released)

3. **Logging**
   - Operation logged with:
     - execution_time_ms
     - results_count
     - had_filters flag
     - limit parameter
   - Errors logged if validation fails or query errors

4. **Performance**
   - Execution time logged for monitoring
   - Typical latency: < 50ms for 10K chunks
   - Max latency: < 500ms (with retries)

5. **Error State**
   - If ValidationError raised: inputs invalid, fix and retry
   - If QueryError raised: database issue, investigate
   - If DatabaseError raised: transient failure, may retry at higher level
   - Original inputs unchanged (no mutation)

**Success Postcondition:**
```python
results = client.vector_search(embedding, limit=10)
assert isinstance(results, list)
assert all(isinstance(r, SearchResult) for r in results)
assert len(results) <= 10
assert results == sorted(results, key=lambda r: r.similarity_score, reverse=True)
```

**Failure Postcondition:**
```python
try:
    results = client.vector_search(invalid_embedding, limit=10)
except ValidationError as e:
    # Client state unchanged, safe to retry with corrected input
    assert client.db is not None  # Still connected
```

## Edge Cases & Handling

### Edge Case 1: Invalid Embedding Dimension

**Scenario:** Caller provides embedding with wrong dimension (e.g., 512 instead of 768)

**Input:**
```python
embedding = [0.1] * 512  # Wrong dimension
client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
raise ValidationError(
    "Validation failed: embedding dimension must be 768, got 512"
)
```

**Test Scenario:**
```python
def test_vector_search_invalid_dimension():
    client = FalkorDBClient()

    # Test various wrong dimensions
    for wrong_dim in [0, 1, 512, 1024, 1536]:
        embedding = [0.1] * wrong_dim
        with pytest.raises(ValidationError, match=f"must be 768, got {wrong_dim}"):
            client.vector_search(embedding, limit=10)
```

**Rationale:** Dimension mismatch would cause FalkorDB query to fail or return nonsensical results. Fail fast with clear error message.

---

### Edge Case 2: Embedding Contains NaN or Inf

**Scenario:** Embedding generation produced invalid float values (NaN or Inf)

**Input:**
```python
embedding = [0.1] * 768
embedding[0] = float('nan')  # Corrupted value
client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
raise ValidationError(
    "Validation failed: embedding contains NaN or Inf values"
)
```

**Test Scenario:**
```python
def test_vector_search_nan_inf_values():
    client = FalkorDBClient()
    embedding_base = [0.1] * 768

    # Test NaN
    embedding_nan = embedding_base.copy()
    embedding_nan[100] = float('nan')
    with pytest.raises(ValidationError, match="NaN or Inf"):
        client.vector_search(embedding_nan, limit=10)

    # Test Inf
    embedding_inf = embedding_base.copy()
    embedding_inf[200] = float('inf')
    with pytest.raises(ValidationError, match="NaN or Inf"):
        client.vector_search(embedding_inf, limit=10)

    # Test -Inf
    embedding_ninf = embedding_base.copy()
    embedding_ninf[300] = float('-inf')
    with pytest.raises(ValidationError, match="NaN or Inf"):
        client.vector_search(embedding_ninf, limit=10)
```

**Rationale:** NaN/Inf would break cosine similarity calculation. Detect early to avoid cryptic database errors.

---

### Edge Case 3: Invalid Limit Values

**Scenario:** Limit is zero, negative, or exceeds maximum

**Input:**
```python
# Zero limit
client.vector_search(embedding, limit=0)

# Negative limit
client.vector_search(embedding, limit=-5)

# Exceeds maximum
client.vector_search(embedding, limit=1001)
```

**Expected Behavior:**
```python
# limit=0
raise ValidationError("Validation failed: limit must be >= 1, got 0")

# limit=-5
raise ValidationError("Validation failed: limit must be >= 1, got -5")

# limit=1001
raise ValidationError("Validation failed: limit cannot exceed 1000, got 1001")
```

**Test Scenario:**
```python
def test_vector_search_invalid_limit():
    client = FalkorDBClient()
    embedding = [0.1] * 768

    # Test zero
    with pytest.raises(ValidationError, match="must be >= 1"):
        client.vector_search(embedding, limit=0)

    # Test negative
    with pytest.raises(ValidationError, match="must be >= 1"):
        client.vector_search(embedding, limit=-10)

    # Test exceeds max
    with pytest.raises(ValidationError, match="cannot exceed 1000"):
        client.vector_search(embedding, limit=1001)

    # Test non-integer
    with pytest.raises(ValidationError, match="must be int"):
        client.vector_search(embedding, limit=10.5)
```

**Rationale:** Protect against nonsensical limits and prevent excessive memory usage (limit > 1000).

---

### Edge Case 4: Database Has No Data

**Scenario:** Search executed on empty database (no Memory or Chunk nodes)

**Input:**
```python
# Freshly initialized database
client = FalkorDBClient()
client.clear_all()  # Ensure empty

embedding = [0.1] * 768
results = client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
results == []  # Empty list, not error
```

**Test Scenario:**
```python
def test_vector_search_empty_database():
    client = FalkorDBClient()
    client.clear_all()  # Ensure no data

    embedding = [0.1] * 768
    results = client.vector_search(embedding, limit=10)

    assert results == []
    assert isinstance(results, list)
```

**Rationale:** Empty database is valid state (initial startup). Return empty results, not error.

---

### Edge Case 5: No Results Match Filters

**Scenario:** Database has data, but no results match filter criteria

**Input:**
```python
# Database has memories, but none with tag "nonexistent"
results = client.vector_search(
    embedding=[0.1] * 768,
    limit=10,
    filters={"tags": ["nonexistent-tag"]}
)
```

**Expected Behavior:**
```python
results == []  # Empty list
```

**Test Scenario:**
```python
def test_vector_search_no_filter_matches():
    client = FalkorDBClient()

    # Add some memories with specific tags
    memory = Memory(
        text="Test memory",
        chunks=[Chunk(text="Test", index=0)],
        embeddings=[[0.1] * 768],
        metadata={"tags": ["python"], "source": "test"}
    )
    client.add_memory(memory)

    # Search with non-matching filter
    results = client.vector_search(
        embedding=[0.1] * 768,
        limit=10,
        filters={"tags": ["javascript"]}  # No memories with this tag
    )

    assert results == []
```

**Rationale:** Valid query scenario - filters are too restrictive. Return empty, allow caller to relax filters.

---

### Edge Case 6: Database Connection Lost During Query

**Scenario:** FalkorDB connection drops mid-query

**Input:**
```python
# Simulate connection loss (mock)
with mock.patch.object(client.graph, 'query', side_effect=ConnectionError("Connection reset")):
    results = client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
# After 3 retries (1s, 2s, 4s delays):
raise DatabaseError(
    "Database operation failed after 3 retries: Connection reset"
)
```

**Test Scenario:**
```python
def test_vector_search_connection_lost(mocker):
    client = FalkorDBClient()
    embedding = [0.1] * 768

    # Mock connection failure
    mocker.patch.object(
        client.graph,
        'query',
        side_effect=ConnectionError("Connection reset")
    )

    # Should retry 3 times then fail
    with pytest.raises(DatabaseError, match="after 3 retries"):
        client.vector_search(embedding, limit=10)

    # Verify retries happened (1 + 3 retries = 4 calls)
    assert client.graph.query.call_count == 4
```

**Rationale:** Transient connection errors should retry. After exhausting retries, propagate error to caller.

---

### Edge Case 7: Invalid Date Format in Filters

**Scenario:** Caller provides malformed date string in filters

**Input:**
```python
results = client.vector_search(
    embedding=[0.1] * 768,
    limit=10,
    filters={"date_from": "2025-11-01"}  # Missing time component
)
```

**Expected Behavior:**
```python
raise ValidationError(
    "Validation failed: filters['date_from'] invalid ISO 8601 format: ..."
)
```

**Test Scenario:**
```python
def test_vector_search_invalid_date_format():
    client = FalkorDBClient()
    embedding = [0.1] * 768

    # Missing time
    with pytest.raises(ValidationError, match="invalid ISO 8601"):
        client.vector_search(
            embedding, limit=10,
            filters={"date_from": "2025-11-01"}
        )

    # Missing timezone
    with pytest.raises(ValidationError, match="invalid ISO 8601"):
        client.vector_search(
            embedding, limit=10,
            filters={"date_from": "2025-11-01T00:00:00"}
        )

    # Invalid format
    with pytest.raises(ValidationError, match="invalid ISO 8601"):
        client.vector_search(
            embedding, limit=10,
            filters={"date_from": "invalid-date"}
        )
```

**Rationale:** Date parsing errors are common. Validate early with clear message about required format.

---

### Edge Case 8: Minimum Similarity Filter Eliminates All Results

**Scenario:** min_similarity threshold too high, no results meet criteria

**Input:**
```python
# Database has results with max similarity 0.7
results = client.vector_search(
    embedding=[0.1] * 768,
    limit=10,
    filters={"min_similarity": 0.9}  # Too high
)
```

**Expected Behavior:**
```python
results == []  # Empty list
```

**Test Scenario:**
```python
def test_vector_search_high_similarity_threshold():
    client = FalkorDBClient()

    # Add memory
    memory = Memory(
        text="Test memory",
        chunks=[Chunk(text="Test", index=0)],
        embeddings=[[0.1] * 768],
        metadata={}
    )
    client.add_memory(memory)

    # Search with very different embedding (low similarity)
    different_embedding = [-0.1] * 768

    # With high threshold
    results = client.vector_search(
        embedding=different_embedding,
        limit=10,
        filters={"min_similarity": 0.95}
    )

    assert results == []  # No results above threshold
```

**Rationale:** Valid scenario - threshold filters out low-quality matches. Return empty, let caller adjust threshold.

---

### Edge Case 9: HNSW Index Not Initialized

**Scenario:** Schema initialization failed or index was dropped

**Input:**
```python
# Mock index missing
with mock.patch.object(client.graph, 'query', side_effect=QueryError("Index not found")):
    results = client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
raise QueryError(
    "Query execution failed: HNSW index 'chunk_embedding_idx' does not exist"
)
```

**Test Scenario:**
```python
def test_vector_search_missing_index(mocker):
    client = FalkorDBClient()
    embedding = [0.1] * 768

    # Mock index error from FalkorDB
    mocker.patch.object(
        client.graph,
        'query',
        side_effect=Exception("Index 'chunk_embedding_idx' does not exist")
    )

    with pytest.raises(QueryError, match="Index.*does not exist"):
        client.vector_search(embedding, limit=10)
```

**Rationale:** Index errors indicate schema initialization failure. Propagate error for operator to fix.

---

### Edge Case 10: Query Timeout (Very Large Database)

**Scenario:** Query takes > 30 seconds (database too large or slow)

**Input:**
```python
# Simulate slow query
with mock.patch.object(client.graph, 'query', side_effect=TimeoutError("Query timeout")):
    results = client.vector_search(embedding, limit=10)
```

**Expected Behavior:**
```python
# After retries:
raise DatabaseError(
    "Database operation failed after 3 retries: Query timeout"
)
```

**Test Scenario:**
```python
def test_vector_search_timeout(mocker):
    client = FalkorDBClient()
    embedding = [0.1] * 768

    # Mock timeout
    mocker.patch.object(
        client.graph,
        'query',
        side_effect=TimeoutError("Query timeout after 30s")
    )

    # Should retry then fail
    with pytest.raises(DatabaseError, match="after 3 retries.*timeout"):
        client.vector_search(embedding, limit=10)
```

**Rationale:** Timeout may be transient (server load). Retry, then fail if persistent.

## Test Scenarios (Complete List)

### Happy Path Tests

1. **test_vector_search_success_basic**
   - Input: Valid 768-dim embedding, limit=10, no filters
   - Setup: Database with 20 memories
   - Expected: List of 10 SearchResult objects, sorted by similarity DESC
   - Assertions:
     - `len(results) == 10`
     - `results[0].similarity_score >= results[-1].similarity_score`
     - All results have valid fields (no None)

2. **test_vector_search_success_with_filters**
   - Input: Valid embedding, limit=5, filters with tags, source, date_from
   - Setup: Database with memories of different tags/sources/dates
   - Expected: Max 5 results matching ALL filter criteria
   - Assertions:
     - All results have matching tags (ANY of filter tags)
     - All results have matching source (exact)
     - All results have timestamp >= date_from

3. **test_vector_search_success_min_similarity_filter**
   - Input: Valid embedding, limit=10, filters={"min_similarity": 0.8}
   - Setup: Database with mixed similarity results (0.5-0.95)
   - Expected: Only results with score >= 0.8
   - Assertions:
     - All `result.similarity_score >= 0.8`
     - Results still sorted by score DESC

4. **test_vector_search_boundary_limit_1**
   - Input: Valid embedding, limit=1
   - Expected: Single best result
   - Assertions:
     - `len(results) == 1`
     - `results[0]` is highest similarity chunk in DB

5. **test_vector_search_boundary_limit_1000**
   - Input: Valid embedding, limit=1000
   - Setup: Database with 500 chunks
   - Expected: All 500 results returned
   - Assertions:
     - `len(results) == 500`
     - All chunks returned

### Error Tests

6. **test_vector_search_invalid_dimension_512**
   - Input: 512-dim embedding
   - Expected: ValidationError("dimension must be 768, got 512")

7. **test_vector_search_invalid_dimension_1024**
   - Input: 1024-dim embedding
   - Expected: ValidationError("dimension must be 768, got 1024")

8. **test_vector_search_embedding_contains_nan**
   - Input: 768-dim embedding with NaN at index 100
   - Expected: ValidationError("contains NaN or Inf")

9. **test_vector_search_embedding_contains_inf**
   - Input: 768-dim embedding with Inf at index 200
   - Expected: ValidationError("contains NaN or Inf")

10. **test_vector_search_limit_zero**
    - Input: limit=0
    - Expected: ValidationError("limit must be >= 1")

11. **test_vector_search_limit_negative**
    - Input: limit=-10
    - Expected: ValidationError("limit must be >= 1")

12. **test_vector_search_limit_exceeds_max**
    - Input: limit=1001
    - Expected: ValidationError("limit cannot exceed 1000")

13. **test_vector_search_limit_not_integer**
    - Input: limit=10.5 (float)
    - Expected: ValidationError("limit must be int")

14. **test_vector_search_filters_tags_empty_list**
    - Input: filters={"tags": []}
    - Expected: ValidationError("tags cannot be empty list")

15. **test_vector_search_filters_source_empty_string**
    - Input: filters={"source": ""}
    - Expected: ValidationError("source must be non-empty string")

16. **test_vector_search_filters_invalid_date_format**
    - Input: filters={"date_from": "2025-11-01"}  # Missing time
    - Expected: ValidationError("invalid ISO 8601 format")

17. **test_vector_search_filters_min_similarity_out_of_range**
    - Input: filters={"min_similarity": 1.5}
    - Expected: ValidationError("must be in [0.0, 1.0]")

### Edge Case Tests

18. **test_vector_search_empty_database**
    - Setup: Empty database (no chunks)
    - Input: Valid embedding, limit=10
    - Expected: `results == []`

19. **test_vector_search_no_filter_matches**
    - Setup: Database with memories (no "nonexistent" tag)
    - Input: filters={"tags": ["nonexistent"]}
    - Expected: `results == []`

20. **test_vector_search_all_below_min_similarity**
    - Setup: Database with low similarity results (max 0.5)
    - Input: filters={"min_similarity": 0.9}
    - Expected: `results == []`

21. **test_vector_search_connection_lost_retries_succeed**
    - Mock: Connection fails twice, succeeds third time
    - Expected: Results returned after 2 retries
    - Assertions: `client.graph.query.call_count == 3`

22. **test_vector_search_connection_lost_all_retries_fail**
    - Mock: Connection fails all 3 retries
    - Expected: DatabaseError("after 3 retries")

23. **test_vector_search_query_timeout**
    - Mock: Query times out
    - Expected: DatabaseError (after retries)

24. **test_vector_search_index_not_found**
    - Mock: HNSW index missing
    - Expected: QueryError("Index does not exist")

### Integration Tests

25. **test_vector_search_integration_add_and_search**
    - Setup: Real FalkorDB instance
    - Steps:
      1. Add memory with known text
      2. Search with same embedding
      3. Verify found with high similarity (> 0.99)
    - Expected: Top result is added memory

26. **test_vector_search_integration_multiple_chunks**
    - Setup: Add memory with 5 chunks
    - Input: Search with embedding matching chunk 3
    - Expected: Chunk 3 appears in top results

27. **test_vector_search_integration_filtered_search**
    - Setup: Add memories with different tags/sources
    - Input: Search with filters
    - Expected: Only matching memories returned

28. **test_vector_search_integration_performance**
    - Setup: Database with 10,000 chunks
    - Input: Search with limit=10
    - Expected: Results in < 100ms (p95 latency)
    - Measure: execution_time_ms logged

### Performance Tests

29. **test_vector_search_performance_small_db_100_chunks**
    - Setup: 100 chunks
    - Input: limit=10
    - Expected: < 20ms

30. **test_vector_search_performance_medium_db_10k_chunks**
    - Setup: 10,000 chunks
    - Input: limit=10
    - Expected: < 50ms (HNSW scales logarithmically)

31. **test_vector_search_performance_large_limit_1000**
    - Setup: 10,000 chunks
    - Input: limit=1000
    - Expected: < 200ms (parsing overhead for large result set)

## Performance Requirements

### Latency

**Target Latencies (p50):**
- Small DB (< 1K chunks): < 10ms
- Medium DB (1K-10K chunks): < 30ms
- Large DB (10K-100K chunks): < 50ms
- Very large DB (100K-1M chunks): < 100ms

**Target Latencies (p95):**
- Small DB: < 20ms
- Medium DB: < 50ms
- Large DB: < 100ms
- Very large DB: < 200ms

**Maximum Allowed Latency:**
- 500ms before timeout/retry

**Latency Breakdown:**
```
Total latency = Query build + Network + FalkorDB execution + Result parsing

Typical for 10K chunks, limit=10:
- Query build: ~1ms (negligible)
- Network: ~2ms (localhost) or ~10ms (remote)
- FalkorDB HNSW search: ~20ms (logarithmic in chunk count)
- Result parsing: ~5ms (linear in limit)
Total: ~30ms (localhost) or ~40ms (remote)
```

### Throughput

**Target Throughput:**
- Concurrent queries: Up to 100 queries/second (limited by pool_size=10)
- Sequential queries: ~30 queries/second (limited by latency)

**Scaling:**
- Increase `pool_size` to 20-50 for higher concurrency
- Multiple FalkorDB instances (sharding) for > 100 QPS

### Resource Usage

**Memory:**
- Query embedding: ~3KB (768 floats × 4 bytes)
- Result set: ~1KB per SearchResult × limit
- Total per query: ~10KB for limit=10
- Concurrent queries: pool_size × 10KB (e.g., 10 queries × 10KB = 100KB)

**CPU:**
- Query building: negligible (< 1% CPU)
- Result parsing: O(limit) linear
- FalkorDB handles vector search (HNSW optimized)

**Network:**
- Request size: ~4KB (query + embedding)
- Response size: ~limit KB (1KB per result)
- Bandwidth for 10 QPS: ~50 KB/s request + ~100 KB/s response

### Optimization Recommendations

1. **Tune HNSW Parameters** (schema level)
   - Increase `ef_search` (default 100) for better accuracy (slower)
   - Decrease for faster search (lower recall)
   - Trade-off: accuracy vs. speed

2. **Limit Result Set Size**
   - Use `min_similarity` filter instead of large `limit`
   - Avoid `limit > 100` unless necessary (parsing overhead)

3. **Connection Pooling**
   - Increase `pool_size` for high concurrency
   - Monitor pool exhaustion rate

4. **Batch Queries** (future optimization)
   - Group multiple searches into single query
   - Reduce network round-trips

5. **Caching** (future optimization)
   - Cache frequent query embeddings
   - Invalidate on database updates

## Security Considerations

### Input Validation

**All inputs validated before use:**
- ✅ Embedding dimension checked (prevent buffer overflow in DB)
- ✅ Embedding values checked for NaN/Inf (prevent calculation errors)
- ✅ Limit bounded [1, 1000] (prevent memory exhaustion)
- ✅ Filter fields validated (prevent injection attacks)
- ✅ Date formats validated (prevent parsing exploits)

**No injection vulnerabilities:**
- ✅ All Cypher queries use parameterized queries ($param syntax)
- ✅ No string concatenation of user input into queries
- ✅ Filter values passed as parameters, not inline

### Error Messages

**Safe error messages:**
- ✅ No sensitive data leaked in error messages
- ✅ No internal paths or credentials exposed
- ✅ Generic errors for database failures (avoid info disclosure)

**Examples:**
```python
# Good: Generic message
"Database operation failed after 3 retries"

# Bad: Leaks internal info
"Connection to db.internal.company.com:6379 failed with password abc123"
```

### Data Protection

**Embedding Privacy:**
- Embeddings may encode sensitive information (PII, secrets)
- Logged at DEBUG level only (not INFO/WARN)
- Never logged in full (only dimension logged)

**Result Privacy:**
- Search results may contain sensitive text
- Caller responsible for access control (not DB layer)
- Consider audit logging at application layer (zapomni_core)

### Rate Limiting

**Currently:** No rate limiting at DB client level

**Future Consideration:**
- Add rate limiting to prevent DoS via excessive searches
- Limit QPS per user/session
- Implement at zapomni_core or zapomni_mcp level

### Authentication

**FalkorDB Authentication:**
- Optional password authentication (Redis protocol)
- Password passed securely via connection string
- No password logging

**Authorization:**
- No row-level security in FalkorDB (all data accessible)
- Implement authorization at application layer (zapomni_core)

## Related Functions

### Calls

**Internal Methods:**

1. **`_execute_cypher(query: str, parameters: dict) -> QueryResult`**
   - Purpose: Execute Cypher query with timing and logging
   - Called for: Running built vector search query
   - Returns: Raw query results

2. **`_retry_operation(func: Callable, *args, **kwargs) -> Any`**
   - Purpose: Retry transient failures with exponential backoff
   - Called for: Wrapping `_execute_cypher` call
   - Retries: 3 attempts with 1s, 2s, 4s delays

**External Dependencies:**

3. **`CypherQueryBuilder.build_vector_search_query(...) -> (str, dict)`**
   - Purpose: Build Cypher query for vector search
   - Called for: Generating query string and parameters
   - Returns: (cypher_query, parameters_dict)
   - Location: `zapomni_db.cypher_query_builder`

### Called By

**From zapomni_core:**

1. **`QueryEngine.search(query: str, ...) -> List[SearchResult]`**
   - Purpose: Main search orchestrator
   - Calls: `vector_search()` after embedding query
   - Flow: query_text → embed() → vector_search() → results

2. **`MemoryProcessor.find_duplicates(text: str) -> List[SearchResult]`**
   - Purpose: Semantic deduplication
   - Calls: `vector_search()` to find similar existing memories
   - Flow: new_text → embed() → vector_search() → check similarity

**From zapomni_mcp:**

3. **`SearchMemoryTool.execute(arguments: dict) -> dict`**
   - Purpose: MCP search tool
   - Calls: `vector_search()` via QueryEngine
   - Flow: MCP request → SearchMemoryTool → QueryEngine → vector_search()

### Call Graph

```
User Query
    ↓
zapomni_mcp.SearchMemoryTool.execute()
    ↓
zapomni_core.QueryEngine.search()
    ↓
zapomni_core.Embedder.embed(query_text)
    ↓ (query_embedding)
FalkorDBClient.vector_search(query_embedding, limit, filters)
    ↓
CypherQueryBuilder.build_vector_search_query(...)
    ↓ (cypher, parameters)
FalkorDBClient._retry_operation(
    FalkorDBClient._execute_cypher(cypher, parameters)
)
    ↓
FalkorDB.graph.query(cypher, parameters)
    ↓ (raw results)
Parse to List[SearchResult]
    ↓
Return to QueryEngine
    ↓
Return to SearchMemoryTool
    ↓
Format as MCP response
    ↓
Return to user
```

## Implementation Notes

### Libraries Used

**Core:**
- `falkordb` (>=4.0.0): FalkorDB Python client
  - Usage: `graph.query()` for Cypher execution
  - Vector search: `db.idx.vector.queryNodes()` function

**Standard Library:**
- `typing`: Type hints (List, Dict, Any, Optional)
- `math`: `isfinite()` for NaN/Inf validation
- `datetime`: ISO 8601 parsing (`fromisoformat()`)
- `time`: Retry delays (`time.sleep()`)

**Internal:**
- `zapomni_db.models`: SearchResult, ValidationError, QueryError, DatabaseError
- `zapomni_db.cypher_query_builder`: CypherQueryBuilder class
- `structlog`: Structured logging

### Known Limitations

1. **Single Embedding Model**
   - All searches assume embeddings from same model (all-MiniLM-L6-v2)
   - Searching with different model's embeddings will give poor results
   - Mitigation: Document embedding model in schema, validate at add_memory

2. **No Multi-Vector Search**
   - Cannot search with multiple query embeddings at once
   - Future: Support batch search for efficiency

3. **No Re-ranking**
   - Results returned in HNSW similarity order
   - No cross-encoder re-ranking for improved accuracy
   - Future: Add optional re-ranking step

4. **No Streaming Results**
   - All results loaded into memory at once
   - Large `limit` values may cause memory pressure
   - Future: Implement cursor-based pagination

5. **No Hybrid Search**
   - Pure vector search, no BM25 or graph traversal integration
   - Future: Combine vector + BM25 + graph (Phase 2)

6. **Filter Limitations**
   - Tags filter uses OR logic (cannot specify AND of multiple tags)
   - Date filters are inclusive only (no exclusive ranges)
   - No regex or fuzzy matching on source

7. **HNSW Approximation**
   - HNSW is approximate NN, not exact
   - May miss some relevant results (trade-off for speed)
   - Tune `ef_search` parameter for accuracy vs. speed

### Future Enhancements

**Phase 2 (Hybrid Search):**
- Combine vector search + BM25 keyword search
- Integrate graph traversal (expand via entity relationships)
- Reciprocal Rank Fusion (RRF) for result merging

**Phase 3 (Advanced Features):**
- Multi-vector search (batch queries)
- Cross-encoder re-ranking
- Streaming/pagination for large result sets
- Query performance caching
- Search analytics (track popular queries, click-through rate)

**Optimizations:**
- Query result caching (Redis)
- Embedding caching (avoid re-computing for same query)
- Connection pool tuning based on load patterns

## References

### Component Spec
- `/home/dev/zapomni/.spec-workflow/specs/level2/falkordb_client_component.md`
  - Section: Method 2 - `vector_search`

### Module Spec
- `/home/dev/zapomni/.spec-workflow/specs/level1/zapomni_db_module.md`
  - Section: Vector Search Operations

### Related Function Specs
- (To be created) `CypherQueryBuilder.build_vector_search_query_function.md`
- (To be created) `FalkorDBClient._execute_cypher_function.md`
- (To be created) `Embedder.embed_function.md` (zapomni_core)

### External Documentation
- **FalkorDB Vector Search**: https://docs.falkordb.com/vector_search.html
- **HNSW Algorithm**: https://arxiv.org/abs/1603.09320
- **Cypher Query Language**: https://neo4j.com/docs/cypher-manual/current/
- **Cosine Similarity**: https://en.wikipedia.org/wiki/Cosine_similarity

### Data Models
- `/home/dev/zapomni/zapomni_db/models.py`
  - `SearchResult` dataclass
  - `ValidationError`, `QueryError`, `DatabaseError` exceptions

---

**Document Status:** Draft v1.0
**Created:** 2025-11-23
**Author:** Goncharenko Anton aka alienxs2
**License:** MIT

**Completeness Metrics:**
- ✅ Function signature: Complete
- ✅ Parameters: 3/3 detailed (embedding, limit, filters)
- ✅ Return value: Fully specified (SearchResult structure)
- ✅ Exceptions: 3 types detailed (ValidationError, QueryError, DatabaseError)
- ✅ Edge cases: 10 identified and handled
- ✅ Test scenarios: 31 test cases defined
- ✅ Algorithm: Pseudocode complete
- ✅ Examples: Basic and advanced usage included

**Implementation Readiness:** ✅ High (ready for TDD implementation)

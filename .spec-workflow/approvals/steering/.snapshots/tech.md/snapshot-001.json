{
  "id": "snapshot_1763844154209_304ukckft",
  "approvalId": "approval_1763844154207_pysqmmcg3",
  "approvalTitle": "Technical Architecture Document (tech.md)",
  "version": 1,
  "timestamp": "2025-11-22T20:42:34.209Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Technical Architecture: Zapomni\n\n**Document Version**: 1.0\n**Created**: 2025-11-22\n**Authors**: Tony + Claude Code\n**Status**: Draft\n**Alignment**: Consistent with product.md vision\n\n---\n\n## Executive Summary\n\n### System Overview\n\nZapomni is a **local-first MCP (Model Context Protocol) memory server** that provides AI agents with intelligent, contextual, and private long-term memory. Built on a unified vector + graph database architecture (FalkorDB), powered by local LLM runtime (Ollama), and implemented in Python 3.10+, Zapomni delivers enterprise-grade RAG capabilities with zero external dependencies and guaranteed data privacy.\n\nThe system combines semantic search, knowledge graph intelligence, and code-aware analysis in a single, cohesive architecture optimized for AI agent workflows. By leveraging proven open-source technologies and modern RAG best practices, Zapomni achieves production-quality performance while maintaining complete local operation.\n\n### Key Technical Decisions\n\n#### 1. FalkorDB (Unified Vector + Graph Database)\n\n**Decision**: Use FalkorDB as the sole database, eliminating separate vector and graph databases.\n\n**Rationale**:\n- **Unified Storage**: Combines vector embeddings and property graph in one system, eliminating synchronization complexity\n- **Superior Performance**: 496x faster P99 latency vs alternatives (GraphRAG benchmark by FalkorDB)\n- **Memory Efficiency**: 6x better memory usage compared to separate database architectures\n- **Redis Protocol**: Familiar, battle-tested interface with excellent client libraries\n- **GraphRAG Optimized**: Native support for hybrid vector + graph queries\n- **Operational Simplicity**: Single database connection, single backup strategy, single monitoring system\n\n**Alternatives Considered**:\n- ChromaDB (vector) + Neo4j (graph): More mature but complex sync, data duplication\n- Qdrant (vector) + NetworkX (in-memory graph): Good performance but separate systems\n- Weaviate: All-in-one but commercial features, resource-intensive\n\n**Trade-offs Accepted**:\n- **Pros**: Simpler architecture, better performance, lower operational overhead\n- **Cons**: Newer technology (smaller community than Neo4j), less mature ecosystem\n- **Risk Mitigation**: Database abstraction layer allows fallback to ChromaDB + Neo4j if needed\n\n---\n\n#### 2. Ollama (Local LLM Runtime)\n\n**Decision**: Use Ollama for both embeddings and LLM inference, exclusively local operation.\n\n**Rationale**:\n- **Best Local Experience**: Easiest setup and model management for local LLMs\n- **Dual Purpose**: Provides both embedding generation and LLM inference in single runtime\n- **API Compatibility**: OpenAI-compatible REST API for embeddings\n- **Large Model Library**: 100+ models, easy pull and update\n- **Zero Cost**: No API fees, no rate limits, no external dependencies\n- **Privacy Guarantee**: Data never leaves local machine\n\n**Alternatives Considered**:\n- OpenAI API: Better quality but $$$, privacy concerns, requires internet\n- llama.cpp: Lower-level control but complex integration, manual model management\n- vLLM: Server-focused, overkill for local single-user deployment\n- Direct model loading (Hugging Face): Memory-intensive, complex model lifecycle\n\n**Trade-offs Accepted**:\n- **Pros**: Simple, well-maintained, good API, local-first aligned\n- **Cons**: Sequential embedding processing (no batch API), slightly slower than cloud\n- **Risk Mitigation**: Fallback to sentence-transformers for embeddings if Ollama fails\n\n---\n\n#### 3. Python 3.10+ (Programming Language)\n\n**Decision**: Implement entire system in Python 3.10 or newer.\n\n**Rationale**:\n- **ML/AI Ecosystem**: Best-in-class libraries (sentence-transformers, SpaCy, LangChain)\n- **MCP SDK**: Official Python SDK from Anthropic with excellent support\n- **FalkorDB Client**: Native Python support via falkordb-py package\n- **Proven Success**: Cognee demonstrates Python viability for similar systems\n- **Community**: Largest AI developer community, abundant resources and examples\n- **Type Safety**: Modern type hints (3.10+) provide excellent tooling and IDE support\n\n**Alternatives Considered**:\n- TypeScript: Good MCP support (Claude Context uses), but weaker ML ecosystem\n- Go: Fast execution but limited AI libraries and LLM integrations\n- Rust: Maximum performance but steep learning curve, longer development time\n\n**Trade-offs Accepted**:\n- **Pros**: Ecosystem, productivity, rapid development, type hints\n- **Cons**: Performance vs compiled languages, GIL limitations for CPU-bound tasks\n- **Risk Mitigation**: Async I/O for concurrency, profiling for hot path optimization\n\n---\n\n#### 4. MCP Stdio Protocol (Communication Layer)\n\n**Decision**: Use stdio transport as default, JSON-RPC 2.0 over standard input/output.\n\n**Rationale**:\n- **Simplicity**: Easiest to implement, debug, and maintain\n- **Security**: Process isolation by default, no network exposure\n- **Standard**: JSON-RPC 2.0 specification, well-documented\n- **Compatibility**: Works with Claude CLI, Claude Desktop, Cursor, Cline out of the box\n- **Debugging**: All messages visible in stderr logs, easily reproducible\n\n**Alternatives Considered**:\n- HTTP transport: Adds network complexity, requires authentication, overkill for local\n- SSE (Server-Sent Events): For streaming responses, useful for future but unnecessary for MVP\n- WebSockets: Bidirectional but complex, not needed for request-response pattern\n\n**Trade-offs Accepted**:\n- **Pros**: Simple, secure, debuggable, no configuration needed\n- **Cons**: Single user per instance, no remote access, no streaming (initially)\n- **Future**: Can add HTTP/SSE transport in Phase 5+ for multi-user or web deployments\n\n---\n\n## Architecture Overview\n\n### High-Level Architecture Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                        USER / AI AGENT                              │\n│                   (Claude CLI, Cursor, Cline)                       │\n└────────────────────────────┬────────────────────────────────────────┘\n                             │ MCP Protocol (stdio)\n                             │ JSON-RPC 2.0\n                             ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                     ZAPOMNI MCP SERVER                              │\n│  ┌───────────────────────────────────────────────────────────────┐ │\n│  │  MCP Interface Layer                                          │ │\n│  │  - Stdio Transport (default)                                  │ │\n│  │  - Tool Definitions (3-10 functions)                          │ │\n│  │  - Pydantic Validation                                        │ │\n│  │  - Error Handling & Logging (stderr)                          │ │\n│  └───────────────────────────────────────────────────────────────┘ │\n│                             │                                       │\n│                             ▼                                       │\n│  ┌───────────────────────────────────────────────────────────────┐ │\n│  │  Memory Processing Engine                                     │ │\n│  │                                                                │ │\n│  │  ┌──────────────────────────────────────────────────────┐    │ │\n│  │  │  Document Processor                                   │    │ │\n│  │  │  - PDF, DOCX, MD, TXT extraction                     │    │ │\n│  │  │  - Semantic chunking (256-512 tokens, 10-20% overlap)│    │ │\n│  │  │  - Metadata extraction                                │    │ │\n│  │  │  - Code AST parsing (tree-sitter)                     │    │ │\n│  │  └──────────────────────────────────────────────────────┘    │ │\n│  │                                                                │ │\n│  │  ┌──────────────────────────────────────────────────────┐    │ │\n│  │  │  Embedding Generator (Ollama)                         │    │ │\n│  │  │  - Model: nomic-embed-text (768 dim, 2048 ctx)       │    │ │\n│  │  │  - Batch processing (sequential via Ollama API)      │    │ │\n│  │  │  - Semantic cache (Redis)                             │    │ │\n│  │  │  - Fallback: all-MiniLM-L6-v2 (speed mode)           │    │ │\n│  │  └──────────────────────────────────────────────────────┘    │ │\n│  │                                                                │ │\n│  │  ┌──────────────────────────────────────────────────────┐    │ │\n│  │  │  Entity & Relationship Extractor                      │    │ │\n│  │  │  - Hybrid: SpaCy NER (fast) + Ollama LLM (accurate)  │    │ │\n│  │  │  - Models: Llama 3.1 / DeepSeek-R1 / Qwen2.5        │    │ │\n│  │  │  - Confidence scoring and validation                  │    │ │\n│  │  │  - Entity deduplication (fuzzy matching)              │    │ │\n│  │  └──────────────────────────────────────────────────────┘    │ │\n│  │                                                                │ │\n│  │  ┌──────────────────────────────────────────────────────┐    │ │\n│  │  │  Hybrid Search Engine                                 │    │ │\n│  │  │  - Vector Similarity (cosine, HNSW index)            │    │ │\n│  │  │  - BM25 Keyword Search                                │    │ │\n│  │  │  - Reciprocal Rank Fusion (RRF)                       │    │ │\n│  │  │  - Cross-Encoder Reranking                            │    │ │\n│  │  │  - Graph Traversal (Cypher queries)                   │    │ │\n│  │  └──────────────────────────────────────────────────────┘    │ │\n│  │                                                                │ │\n│  │  ┌──────────────────────────────────────────────────────┐    │ │\n│  │  │  Background Task Manager                              │    │ │\n│  │  │  - Async job queue (cognify, codify)                 │    │ │\n│  │  │  - Progress tracking (percentage-based)               │    │ │\n│  │  │  - Status monitoring (pending/running/completed)      │    │ │\n│  │  │  - Error handling with retry logic                    │    │ │\n│  │  └──────────────────────────────────────────────────────┘    │ │\n│  └───────────────────────────────────────────────────────────────┘ │\n│                             │                                       │\n│                             ▼                                       │\n│  ┌───────────────────────────────────────────────────────────────┐ │\n│  │  FalkorDB Storage Layer (Unified Vector + Graph)             │ │\n│  │                                                                │ │\n│  │  ┌────────────────────────┐  ┌─────────────────────────────┐ │ │\n│  │  │  Vector Index          │  │  Property Graph             │ │ │\n│  │  │  - Embeddings (768d)   │  │  - Nodes: Memory, Entity,   │ │ │\n│  │  │  - HNSW/Flat index     │  │    Document, Chunk          │ │ │\n│  │  │  - Cosine similarity   │  │  - Edges: MENTIONS,         │ │ │\n│  │  │  - Top-K retrieval     │  │    RELATED_TO, HAS_CHUNK,   │ │ │\n│  │  │  - Metadata filtering  │  │    CALLS (code)             │ │ │\n│  │  └────────────────────────┘  │  - Cypher query engine      │ │ │\n│  │                               │  - Graph traversal (depth)  │ │ │\n│  │                               └─────────────────────────────┘ │ │\n│  │                                                                │ │\n│  │  Redis Protocol (port 6379) - Single database, no sync needed │ │\n│  └───────────────────────────────────────────────────────────────┘ │\n│                             │                                       │\n│                             ▼                                       │\n│  ┌───────────────────────────────────────────────────────────────┐ │\n│  │  Semantic Cache Layer (Redis)                                 │ │\n│  │  - Embedding cache (68% hit rate target)                      │ │\n│  │  - Query result cache (LRU eviction)                          │ │\n│  │  - Similarity threshold: 0.8                                  │ │\n│  │  - TTL: configurable (default 24h)                            │ │\n│  └───────────────────────────────────────────────────────────────┘ │\n└─────────────────────────────────────────────────────────────────────┘\n                             │\n                             ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                     EXTERNAL SERVICES (Local)                       │\n│  ┌─────────────────────┐  ┌────────────────────┐                  │\n│  │  Ollama             │  │  FalkorDB          │                  │\n│  │  localhost:11434    │  │  localhost:6379    │                  │\n│  │  - Embeddings API   │  │  - Graph + Vector  │                  │\n│  │  - LLM inference    │  │  - Cypher queries  │                  │\n│  └─────────────────────┘  └────────────────────┘                  │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\n### Component Responsibilities\n\n#### 1. MCP Server Layer\n\n**Purpose**: Accept tool calls from MCP clients, validate inputs, route to appropriate handlers.\n\n**Responsibilities**:\n- Listen on stdio for JSON-RPC 2.0 messages\n- Parse and validate MCP tool call requests using Pydantic schemas\n- Route requests to appropriate business logic handlers\n- Return structured JSON responses conforming to MCP spec\n- Log all operations to stderr (stdout reserved for MCP protocol)\n- Handle errors gracefully with meaningful messages\n\n**Technologies**:\n- `mcp` Python SDK (official Anthropic implementation)\n- `pydantic` for input/output validation\n- `asyncio` for async request handling\n- `structlog` for structured logging to stderr\n\n**Configuration**:\n```python\n{\n    \"server_name\": \"zapomni-memory\",\n    \"version\": \"0.1.0\",\n    \"transport\": \"stdio\",  # Default, can add SSE/HTTP later\n    \"log_level\": \"INFO\",\n    \"max_concurrent_tasks\": 4  # For parallel processing\n}\n```\n\n---\n\n#### 2. Document Processor\n\n**Purpose**: Extract text from various document formats and chunk into optimal sizes.\n\n**Supported Formats**:\n- PDF: PyMuPDF (fitz) for text extraction\n- DOCX: python-docx for Word documents\n- Markdown: Direct parsing, preserve structure\n- TXT: UTF-8 text files\n- HTML: trafilatura for web content extraction (future)\n\n**Chunking Strategy**:\n- **Method**: Semantic chunking with LangChain RecursiveCharacterTextSplitter\n- **Size**: 256-512 tokens per chunk (configurable)\n- **Overlap**: 10-20% (50-100 tokens)\n- **Separators**: Hierarchical: `\\n\\n` (paragraphs) → `\\n` (lines) → `. ` (sentences) → ` ` (words)\n- **Validation**: Ensure chunks don't break mid-sentence\n\n**Metadata Extraction**:\n- `title`: Document title (from filename or metadata)\n- `source`: Original file path or URL\n- `date`: Creation/modification timestamp\n- `section`: Document section (if structured)\n- `page_num`: Page number for PDFs\n- `language`: Auto-detected language (langdetect)\n- `chunk_index`: Position in original document\n\n**Example**:\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=512,\n    chunk_overlap=100,\n    length_function=len,  # Token count function\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n\nchunks = splitter.split_text(document_text)\n```\n\n---\n\n#### 3. Embedding Generator\n\n**Purpose**: Generate vector embeddings for text chunks using local models.\n\n**Primary Model**: nomic-embed-text (via Ollama)\n- **Dimensions**: 768\n- **Context Length**: 2048 tokens\n- **Accuracy**: 81.2% (MTEB benchmark)\n- **Languages**: Multilingual including English, Russian\n- **API**: Ollama embeddings endpoint\n\n**Fallback Model**: all-MiniLM-L6-v2 (via sentence-transformers)\n- **Dimensions**: 384\n- **Speed**: ~5x faster than nomic-embed-text\n- **Use Case**: Speed mode or Ollama unavailable\n\n**Processing**:\n- **Batch Size**: Sequential processing (Ollama limitation)\n- **Optimization**: Semantic cache reduces redundant calls\n- **Error Handling**: Automatic retry with exponential backoff\n- **Monitoring**: Track embedding latency and cache hit rate\n\n**Configuration**:\n```python\nOLLAMA_HOST = \"http://localhost:11434\"\nOLLAMA_TIMEOUT = 60  # seconds\nEMBEDDING_MODEL = \"nomic-embed-text\"\nFALLBACK_EMBEDDING_MODEL = \"all-minilm:l6-v2\"\n```\n\n---\n\n#### 4. Entity & Relationship Extractor\n\n**Purpose**: Extract entities and relationships from text to build knowledge graph.\n\n**Hybrid Approach** (Best of Both Worlds):\n\n**Step 1: Fast NER with SpaCy**\n- Model: `en_core_web_lg` (or language-specific)\n- Entities: PERSON, ORG, GPE (locations), PRODUCT, DATE, MONEY\n- Speed: ~100ms per document\n- Use Case: Standard entities\n\n**Step 2: Domain-Specific Extraction with LLM**\n- Models: Llama 3.1 8B (default), DeepSeek-R1 (advanced reasoning), Qwen2.5 (code-focused)\n- Entities: TECHNOLOGY, CONCEPT, ALGORITHM (domain-specific)\n- Format: JSON schema with `format` parameter\n- Confidence Scoring: 0-1 range, threshold 0.7\n\n**Step 3: Merge and Deduplicate**\n- Fuzzy matching with RapidFuzz (85% similarity threshold)\n- Conflict resolution: Higher confidence wins\n- Output: Unified entity list with source attribution\n\n**Relationship Detection**:\n- LLM-based extraction with few-shot prompting\n- Prompt templates: Subject-Predicate-Object triples\n- Confidence scoring and validation\n- Relationship types: CREATED_BY, USES, RELATED_TO, PART_OF, etc.\n\n---\n\n#### 5. Hybrid Search Engine\n\n**Purpose**: Combine keyword, semantic, and graph-based retrieval for optimal results.\n\n**Three-Stage Retrieval**:\n\n**Stage 1: Vector Similarity Search**\n- HNSW index in FalkorDB\n- Cosine similarity metric\n- Top 50 candidates\n- Metadata filtering (date, tags, source)\n\n**Stage 2: BM25 Keyword Search**\n- rank-bm25 library\n- Tokenized corpus\n- Top 50 candidates\n- Exact term matching\n\n**Stage 3: Reciprocal Rank Fusion (RRF)**\n- Combine vector and BM25 rankings\n- Formula: `RRF_score = Σ(1 / (k + rank_i))` where k=60\n- No score normalization needed\n- Robust to score scale differences\n\n**Stage 4: Cross-Encoder Reranking** (Optional, Phase 2)\n- Model: `cross-encoder/ms-marco-MiniLM-L-6-v2`\n- Rerank top 20 candidates\n- Query-document pairs processed together\n- Final top-K selection\n\n**Graph Enhancement** (Phase 3):\n- For top results, traverse graph to find related entities\n- Include graph context in results\n- Multi-hop reasoning (depth 1-2)\n\n**Performance**:\n- **Target**: < 500ms end-to-end latency (P95)\n- **Accuracy**: 3.4x better than vector-only (research benchmark)\n\n---\n\n#### 6. Background Task Manager\n\n**Purpose**: Handle long-running operations (graph building, code indexing) asynchronously.\n\n**Design Pattern**: Cognee-inspired async task queue\n\n**Features**:\n- Immediate task acceptance with unique task ID\n- Asynchronous execution in background\n- Progress percentage updates\n- Status tracking (pending/running/completed/failed)\n- Error handling with retry logic\n- Queryable status endpoint\n\n**Implementation**:\n```python\nimport asyncio\nimport uuid\nfrom enum import Enum\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\nclass BackgroundTask:\n    def __init__(self, task_id: str, func, *args, **kwargs):\n        self.task_id = task_id\n        self.status = TaskStatus.PENDING\n        self.progress = 0.0\n        self.result = None\n        self.error = None\n        # Execute in background\n        asyncio.create_task(self._run(func, *args, **kwargs))\n```\n\n---\n\n#### 7. Semantic Cache Layer\n\n**Purpose**: Reduce embedding computation by caching results.\n\n**Strategies**:\n\n**Exact Match Cache**:\n- Key: SHA256 hash of text\n- Value: Embedding vector\n- Storage: Redis\n- TTL: 24 hours (configurable)\n\n**Semantic Cache** (Phase 2):\n- Store embeddings with vector similarity search\n- Find similar queries (threshold 0.8)\n- Reuse cached embeddings for similar text\n- Expected hit rate: 60-68% (research benchmark)\n\n**LRU Eviction**:\n- Max cache size: 1GB (configurable)\n- Least Recently Used eviction policy\n- Performance: 20%+ latency reduction\n\n---\n\n### Data Flow\n\n#### Ingestion Flow (add_memory)\n\n```\nText Input\n    ↓\n[1. Validation]\n    - Non-empty text check\n    - Size limit check (10MB)\n    - Metadata validation\n    ↓\n[2. Chunking]\n    - Semantic chunking (256-512 tokens)\n    - 10-20% overlap\n    - Metadata preservation\n    ↓\n[3. Embedding Generation]\n    - Check semantic cache\n    - Generate via Ollama (if cache miss)\n    - Store in cache\n    ↓\n[4. FalkorDB Storage]\n    - Create Memory node\n    - Create Chunk nodes\n    - Create HAS_CHUNK relationships\n    - Store embeddings in vector index\n    - Store metadata\n    ↓\n[5. Confirmation]\n    - Return memory_id (UUID)\n    - Return chunk count\n    - Return text preview\n```\n\n**Expected Performance**:\n- 100 documents/minute target\n- 3-5 seconds per document (including embedding)\n- Parallelizable for batch uploads\n\n---\n\n#### Search Flow (search_memory)\n\n```\nQuery Input\n    ↓\n[1. Query Embedding]\n    - Check cache (exact + semantic)\n    - Generate via Ollama (if miss)\n    - Cache result\n    ↓\n[2. Parallel Retrieval]\n    ┌─────────────┬─────────────┐\n    │ Vector      │ BM25        │\n    │ Search      │ Search      │\n    │ (Top 50)    │ (Top 50)    │\n    └─────────────┴─────────────┘\n    ↓\n[3. Metadata Filtering]\n    - Apply date filters\n    - Apply tag filters\n    - Apply source filters\n    - Apply minimum score threshold\n    ↓\n[4. Rank Fusion (RRF)]\n    - Combine vector + BM25 rankings\n    - k=60 constant\n    - Score: Σ(1 / (k + rank))\n    ↓\n[5. Reranking] (Phase 2)\n    - Cross-encoder top 20\n    - Query-document pairs\n    - Final scoring\n    ↓\n[6. Results]\n    - Top K (default 10)\n    - Include similarity scores\n    - Include metadata\n    - Include graph context (Phase 3)\n```\n\n**Expected Performance**:\n- < 500ms end-to-end (P95)\n- Breakdown:\n  * Embedding: 50ms (cached) / 150ms (uncached)\n  * Vector search: 100ms\n  * BM25 search: 50ms\n  * Fusion: 50ms\n  * Reranking: 150ms (Phase 2)\n  * Overhead: 50ms\n\n---\n\n#### Graph Building Flow (build_graph)\n\n```\nMemory IDs Input (or all unprocessed)\n    ↓\n[1. Background Task Creation]\n    - Generate task_id\n    - Return immediately to client\n    - Start async processing\n    ↓\n[2. Entity Extraction] (async)\n    ┌─────────────┬─────────────┐\n    │ SpaCy NER   │ Ollama LLM  │\n    │ (fast)      │ (accurate)  │\n    │ Standard    │ Domain      │\n    │ entities    │ entities    │\n    └─────────────┴─────────────┘\n    ↓\n[3. Entity Merging]\n    - Fuzzy matching (85% threshold)\n    - Deduplication\n    - Confidence scoring\n    ↓\n[4. Relationship Extraction]\n    - LLM-based (few-shot prompting)\n    - Subject-Predicate-Object triples\n    - Confidence scoring\n    ↓\n[5. Graph Construction]\n    - Create Entity nodes in FalkorDB\n    - Create MENTIONS edges (Chunk → Entity)\n    - Create RELATED_TO edges (Entity → Entity)\n    - Update progress percentage\n    ↓\n[6. Completion]\n    - Mark task as completed\n    - Store result statistics\n    - Update status\n```\n\n**Expected Performance**:\n- 1K documents in < 10 minutes\n- Entity extraction: 80%+ precision target\n- Relationship detection: 70%+ precision target\n\n---\n\n## Technology Stack Deep Dive\n\n### 1. FalkorDB - Unified Vector + Graph Database\n\n#### Why FalkorDB?\n\n**Decision Rationale**:\n\n1. **Unified Storage**: Combines vector and graph in single database\n   - Eliminates sync complexity between ChromaDB + Neo4j\n   - Consistent transactions across vector and graph operations\n   - Simpler operational model (one connection, one backup, one monitoring system)\n\n2. **Performance**: 496x faster P99 latency, 6x memory efficiency\n   - Benchmark: GraphRAG vs Vector-only RAG accuracy test by FalkorDB/Diffbot\n   - GraphBLAS backend optimization for graph operations\n   - Redis protocol for low latency operations\n\n3. **Vector + Graph Integration**: Native support for hybrid queries\n   - Cypher queries can include vector similarity\n   - Graph traversal with embedding context\n   - GraphRAG patterns optimized\n\n4. **Redis Protocol**: Familiar, battle-tested communication\n   - Port 6379 (standard Redis)\n   - Excellent Python client (redis-py compatible)\n   - Connection pooling support\n\n**Alternatives Considered**:\n- **ChromaDB + Neo4j**: More mature, larger community, but complex sync and data duplication\n- **Qdrant + NetworkX**: Good performance, but separate systems requiring coordination\n- **Weaviate**: All-in-one vector + search, but commercial features, resource-intensive\n\n**Trade-offs**:\n- **Pros**:\n  - Unified architecture (simpler)\n  - Superior performance (496x faster)\n  - Better memory efficiency (6x)\n  - Single point of management\n  - GraphRAG optimized\n\n- **Cons**:\n  - Newer technology (launched 2023)\n  - Smaller community than Neo4j\n  - Less extensive documentation\n  - Fewer third-party integrations\n\n- **Risk Mitigation**: Database abstraction layer allows fallback to ChromaDB + Neo4j if FalkorDB proves problematic\n\n---\n\n#### FalkorDB Configuration\n\n**Connection Settings**:\n```python\n# Environment variables\nFALKORDB_HOST = \"localhost\"  # or custom host\nFALKORDB_PORT = 6379\nGRAPH_NAME = \"zapomni_memory\"\nFALKORDB_PASSWORD = \"\"  # Empty for local, set for production\n\n# Connection pool settings\nCONNECTION_POOL_SIZE = 10\nCONNECTION_TIMEOUT_SECONDS = 30\nSOCKET_KEEPALIVE = True\nSOCKET_KEEPALIVE_OPTIONS = {\n    \"TCP_KEEPIDLE\": 60,\n    \"TCP_KEEPINTVL\": 10,\n    \"TCP_KEEPCNT\": 3\n}\n```\n\n**Schema Design**:\n\n```cypher\n// Node Types\n\n// Memory: Top-level document/memory entry\n(:Memory {\n  id: string,           // UUID\n  text: string,         // Original full text\n  embedding: vector,    // 768-dim document-level embedding\n  tags: [string],       // User-defined tags\n  source: string,       // Origin (file path, URL, manual)\n  timestamp: datetime,  // Creation time\n  chunk_count: int      // Number of chunks\n})\n\n// Chunk: Semantic segment of a document\n(:Chunk {\n  id: string,           // UUID\n  text: string,         // Chunk text content\n  doc_id: string,       // Parent Memory ID\n  index: int,           // Chunk sequence number\n  embedding: vector,    // 768-dim chunk embedding\n  metadata: map         // Flexible metadata (page_num, section, etc.)\n})\n\n// Entity: Extracted named entity or concept\n(:Entity {\n  id: string,           // UUID\n  name: string,         // Entity name (canonical)\n  type: string,         // PERSON, ORG, TECHNOLOGY, CONCEPT, etc.\n  description: string,  // Entity description\n  confidence: float,    // Extraction confidence (0.0-1.0)\n  source: string        // Extraction source (spacy, llm)\n})\n\n// Document: Higher-level document metadata\n(:Document {\n  id: string,           // UUID\n  title: string,        // Document title\n  source: string,       // File path or URL\n  date: datetime,       // Document date\n  type: string,         // pdf, md, txt, code, etc.\n  language: string      // Detected language\n})\n\n// Code-specific nodes (Phase 4)\n(:Function {\n  id: string,\n  name: string,         // Function name\n  signature: string,    // Full signature\n  file_path: string,    // Source file\n  language: string,     // Programming language\n  docstring: string,    // Function documentation\n  start_line: int,\n  end_line: int\n})\n\n(:Class {\n  id: string,\n  name: string,\n  methods: [string],    // Method names\n  file_path: string,\n  language: string,\n  docstring: string\n})\n\n// Edge Types\n\n// Document structure relationships\n(:Document)-[:HAS_CHUNK {index: int}]->(:Chunk)\n\n// Entity mentions in text\n(:Chunk)-[:MENTIONS {confidence: float, count: int}]->(:Entity)\n\n// Entity-to-entity relationships\n(:Entity)-[:RELATED_TO {\n  type: string,         // CREATED_BY, USES, PART_OF, etc.\n  strength: float,      // Relationship strength (0.0-1.0)\n  evidence: string      // Text evidence for relationship\n}]->(:Entity)\n\n// Code relationships (Phase 4)\n(:Function)-[:CALLS]->(:Function)\n(:Class)-[:INHERITS_FROM]->(:Class)\n(:Function)-[:DEFINED_IN]->(:Document)\n(:Class)-[:CONTAINS]->(:Function)\n```\n\n**Indexes**:\n\n```cypher\n// Vector index for chunk embeddings (primary search)\nCREATE VECTOR INDEX FOR (c:Chunk) ON (c.embedding)\nOPTIONS {\n  dimension: 768,\n  similarityFunction: 'cosine',\n  indexType: 'HNSW',\n  m: 16,                    // HNSW parameter: connections per layer\n  efConstruction: 200,      // Build-time accuracy\n  efSearch: 100             // Query-time accuracy\n}\n\n// Vector index for memory embeddings (document-level search)\nCREATE VECTOR INDEX FOR (m:Memory) ON (m.embedding)\nOPTIONS {\n  dimension: 768,\n  similarityFunction: 'cosine',\n  indexType: 'HNSW',\n  m: 16,\n  efConstruction: 200,\n  efSearch: 100\n}\n\n// Property indexes for fast filtering\nCREATE INDEX FOR (m:Memory) ON (m.id)\nCREATE INDEX FOR (m:Memory) ON (m.timestamp)\nCREATE INDEX FOR (m:Memory) ON (m.source)\nCREATE INDEX FOR (c:Chunk) ON (c.id)\nCREATE INDEX FOR (c:Chunk) ON (c.doc_id)\nCREATE INDEX FOR (e:Entity) ON (e.name)\nCREATE INDEX FOR (e:Entity) ON (e.type)\nCREATE INDEX FOR (d:Document) ON (d.id)\nCREATE INDEX FOR (d:Document) ON (d.source)\n```\n\n**Performance Tuning**:\n\n```python\n# HNSW Index Parameters\n#\n# M (connections per layer):\n#   - Higher M = better recall, slower search, more memory\n#   - Lower M = faster search, lower recall, less memory\n#   - Recommended: 16-32\n#\n# efConstruction (build-time parameter):\n#   - Higher ef = better index quality, slower build\n#   - Lower ef = faster build, lower quality\n#   - Recommended: 100-400\n#\n# efSearch (query-time parameter):\n#   - Higher ef = better recall, slower queries\n#   - Lower ef = faster queries, lower recall\n#   - Recommended: 50-200\n\n# For MVP (10K docs):\nHNSW_M = 16\nHNSW_EF_CONSTRUCTION = 200\nHNSW_EF_SEARCH = 100\n\n# For production (100K+ docs):\nHNSW_M = 32\nHNSW_EF_CONSTRUCTION = 400\nHNSW_EF_SEARCH = 200\n\n# Memory estimation:\n# Per vector: dimension * 4 bytes (float32) + HNSW overhead\n# HNSW overhead: ~M * 4 bytes per vector\n# Example: 10K vectors, 768 dim, M=16\n# = 10K * (768 * 4 + 16 * 4) = 10K * 3136 = 31MB\n```\n\n#### FalkorDB Client Implementation\n\n```python\nfrom falkordb import FalkorDB\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass FalkorDBClient:\n    \"\"\"FalkorDB client for Zapomni memory storage.\"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        graph_name: str = \"zapomni_memory\",\n        password: str = \"\"\n    ):\n        self.host = host\n        self.port = port\n        self.graph_name = graph_name\n        self.client = None\n        self.graph = None\n\n    async def connect(self):\n        \"\"\"Establish connection to FalkorDB.\"\"\"\n        try:\n            self.client = FalkorDB(\n                host=self.host,\n                port=self.port,\n                password=self.password if self.password else None\n            )\n            self.graph = self.client.select_graph(self.graph_name)\n\n            # Initialize schema and indexes\n            await self._ensure_schema()\n\n            logger.info(f\"Connected to FalkorDB at {self.host}:{self.port}\")\n        except Exception as e:\n            logger.error(f\"FalkorDB connection failed: {e}\")\n            raise\n\n    async def _ensure_schema(self):\n        \"\"\"Create indexes and constraints if they don't exist.\"\"\"\n        try:\n            # Vector index for chunks\n            self.graph.query(\"\"\"\n                CREATE VECTOR INDEX FOR (c:Chunk) ON (c.embedding)\n                OPTIONS {\n                    dimension: 768,\n                    similarityFunction: 'cosine',\n                    indexType: 'HNSW',\n                    m: 16,\n                    efConstruction: 200,\n                    efSearch: 100\n                }\n            \"\"\")\n\n            # Property indexes\n            indexes = [\n                \"CREATE INDEX FOR (m:Memory) ON (m.id)\",\n                \"CREATE INDEX FOR (c:Chunk) ON (c.id)\",\n                \"CREATE INDEX FOR (e:Entity) ON (e.name)\",\n            ]\n\n            for index_query in indexes:\n                try:\n                    self.graph.query(index_query)\n                except:\n                    pass  # Index may already exist\n\n            logger.info(\"Schema initialized\")\n        except Exception as e:\n            logger.warning(f\"Schema initialization: {e}\")\n\n    async def add_memory(\n        self,\n        text: str,\n        embedding: List[float],\n        metadata: Dict[str, Any] = None\n    ) -> str:\n        \"\"\"\n        Add memory to graph database.\n\n        Args:\n            text: Memory text content\n            embedding: 768-dim embedding vector\n            metadata: Optional metadata (tags, source, etc.)\n\n        Returns:\n            memory_id: UUID of created memory\n        \"\"\"\n        import uuid\n\n        memory_id = str(uuid.uuid4())\n        metadata = metadata or {}\n\n        query = \"\"\"\n            CREATE (m:Memory {\n                id: $id,\n                text: $text,\n                embedding: $embedding,\n                tags: $tags,\n                source: $source,\n                timestamp: timestamp(),\n                chunk_count: 0\n            })\n            RETURN m.id as id\n        \"\"\"\n\n        params = {\n            \"id\": memory_id,\n            \"text\": text,\n            \"embedding\": embedding,\n            \"tags\": metadata.get(\"tags\", []),\n            \"source\": metadata.get(\"source\", \"user\")\n        }\n\n        result = self.graph.query(query, params)\n\n        logger.info(f\"Memory created: {memory_id}\")\n        return memory_id\n\n    async def vector_search(\n        self,\n        query_embedding: List[float],\n        limit: int = 10,\n        min_score: float = 0.5,\n        filters: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for similar memories using vector similarity.\n\n        Args:\n            query_embedding: Query vector (768 dim)\n            limit: Maximum results to return\n            min_score: Minimum similarity score (0-1)\n            filters: Optional metadata filters\n\n        Returns:\n            List of matching memories with scores\n        \"\"\"\n        # Build filter clause\n        filter_clause = \"\"\n        if filters:\n            conditions = []\n            if \"tags\" in filters:\n                conditions.append(\"ANY(tag IN $tags WHERE tag IN node.tags)\")\n            if \"date_from\" in filters:\n                conditions.append(\"node.timestamp >= $date_from\")\n            if \"date_to\" in filters:\n                conditions.append(\"node.timestamp <= $date_to\")\n            if \"source\" in filters:\n                conditions.append(\"node.source = $source\")\n\n            if conditions:\n                filter_clause = \"WHERE \" + \" AND \".join(conditions)\n\n        query = f\"\"\"\n            CALL db.idx.vector.queryNodes('Chunk', 'embedding', $limit, $query_embedding)\n            YIELD node, score\n            {filter_clause}\n            RETURN\n                node.id as id,\n                node.text as text,\n                node.doc_id as doc_id,\n                node.metadata as metadata,\n                score\n            ORDER BY score DESC\n        \"\"\"\n\n        params = {\n            \"query_embedding\": query_embedding,\n            \"limit\": limit * 2,  # Fetch more to account for filtering\n            **(filters or {})\n        }\n\n        result = self.graph.query(query, params)\n\n        # Parse results\n        memories = []\n        for record in result.result_set:\n            if len(record) >= 5:\n                score = record[4]\n                if score >= min_score:\n                    memories.append({\n                        \"id\": record[0],\n                        \"text\": record[1],\n                        \"doc_id\": record[2],\n                        \"metadata\": record[3],\n                        \"similarity_score\": score\n                    })\n\n        # Limit to requested count\n        return memories[:limit]\n\n    async def graph_traverse(\n        self,\n        entity_name: str,\n        depth: int = 1,\n        limit: int = 20\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Find entities related to given entity via graph traversal.\n\n        Args:\n            entity_name: Starting entity name\n            depth: Graph traversal depth (1-3)\n            limit: Maximum results\n\n        Returns:\n            List of related entities with relationship info\n        \"\"\"\n        query = \"\"\"\n            MATCH (e:Entity {name: $entity_name})\n            MATCH path = (e)-[:RELATED_TO*1..$depth]-(related:Entity)\n            RETURN\n                related.name AS name,\n                related.type AS type,\n                related.description AS description,\n                length(path) AS distance,\n                relationships(path) AS rels\n            ORDER BY distance ASC\n            LIMIT $limit\n        \"\"\"\n\n        params = {\n            \"entity_name\": entity_name,\n            \"depth\": depth,\n            \"limit\": limit\n        }\n\n        result = self.graph.query(query, params)\n\n        related_entities = []\n        for record in result.result_set:\n            if len(record) >= 5:\n                related_entities.append({\n                    \"name\": record[0],\n                    \"type\": record[1],\n                    \"description\": record[2],\n                    \"distance\": record[3],\n                    \"relationships\": [\n                        r.properties for r in record[4]\n                    ] if record[4] else []\n                })\n\n        return related_entities\n\n    async def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get database statistics.\"\"\"\n\n        queries = {\n            \"total_memories\": \"MATCH (m:Memory) RETURN count(m) as count\",\n            \"total_chunks\": \"MATCH (c:Chunk) RETURN count(c) as count\",\n            \"total_entities\": \"MATCH (e:Entity) RETURN count(e) as count\",\n            \"total_relationships\": \"MATCH ()-[r:RELATED_TO]->() RETURN count(r) as count\",\n        }\n\n        stats = {\n            \"graph_name\": self.graph_name,\n            \"database_host\": self.host\n        }\n\n        for key, query in queries.items():\n            result = self.graph.query(query)\n            stats[key] = result.result_set[0][0] if result.result_set else 0\n\n        return stats\n```\n\n---\n\n### 2. Ollama - Local LLM Runtime\n\n#### Why Ollama?\n\n**Decision Rationale**:\n\n1. **Best Local Experience**: Easiest setup, best model management for local deployment\n   - Single command installation: `curl -fsSL https://ollama.com/install.sh | sh`\n   - Model management: `ollama pull <model>`, `ollama list`, `ollama rm <model>`\n   - Automatic updates: `ollama pull <model>` updates to latest version\n\n2. **Dual Purpose**: Both embeddings and LLM inference in one runtime\n   - Embeddings API: `/api/embeddings` endpoint\n   - LLM inference: `/api/generate` and `/api/chat` endpoints\n   - Consistent API across all models\n\n3. **API Compatibility**: OpenAI-compatible REST API\n   - HTTP requests with JSON payloads\n   - Async support via `stream: false`\n   - Structured output via `format: \"json\"`\n\n4. **Large Model Library**: 100+ models, easy discovery\n   - Browse: https://ollama.com/library\n   - Pull with one command: `ollama pull <model>`\n   - Community models supported\n\n5. **Zero Cost, Complete Privacy**:\n   - No API keys required\n   - No rate limits\n   - No data sent to external servers\n   - Offline capability (after model download)\n\n**Alternatives Considered**:\n- **OpenAI API**: Better quality but $$$ costs, privacy concerns, requires internet\n- **llama.cpp**: More control but complex integration, manual model management\n- **vLLM**: Server-optimized but overkill for local single-user\n- **Hugging Face Transformers**: Direct loading but memory-intensive, complex lifecycle\n\n**Trade-offs**:\n- **Pros**:\n  - Simple installation and usage\n  - Well-maintained (official Ollama team)\n  - Good API design\n  - Local-first aligned\n  - Active community\n\n- **Cons**:\n  - Sequential processing only (no batch embeddings API)\n  - Slightly slower than cloud APIs\n  - Requires local compute (CPU/GPU)\n  - Model quality varies\n\n- **Risk Mitigation**: Fallback to sentence-transformers for embeddings if Ollama unavailable\n\n---\n\n#### Ollama Configuration\n\n**Models Required**:\n\n```bash\n# Phase 1: Embedding model (REQUIRED)\nollama pull nomic-embed-text\n# Specs: 768 dimensions, 2048 token context, 81.2% MTEB accuracy\n# Size: ~274MB\n# Use: Document and query embeddings\n\n# Phase 2: Fallback embedding model (OPTIONAL)\nollama pull all-minilm:l6-v2\n# Specs: 384 dimensions, 512 token context, 80% accuracy\n# Size: ~67MB\n# Use: Speed mode or fallback if nomic-embed-text fails\n\n# Phase 3: LLM for entity extraction (REQUIRED for knowledge graph)\nollama pull llama3.1:8b\n# Size: ~4.7GB\n# Use: Entity extraction, relationship detection, general inference\n# Alternative: deepseek-r1:14b (better reasoning, larger), qwen2.5:7b (code-focused)\n\n# Phase 4: Code-specific model (OPTIONAL)\nollama pull qwen2.5:7b\n# Size: ~4.7GB\n# Use: Code analysis, function extraction\n# Specialization: Strong at code understanding\n```\n\n**API Configuration**:\n\n```python\n# Ollama server settings\nOLLAMA_HOST = \"http://localhost:11434\"\nOLLAMA_TIMEOUT = 60  # seconds for embedding requests\nOLLAMA_LLM_TIMEOUT = 120  # seconds for LLM inference\nOLLAMA_CONCURRENT_REQUESTS = 1  # Sequential only (Ollama limitation)\n\n# Model selection\nEMBEDDING_MODEL = \"nomic-embed-text\"\nFALLBACK_EMBEDDING_MODEL = \"all-minilm:l6-v2\"\nLLM_MODEL = \"llama3.1:8b\"  # For entity extraction\nCODE_LLM_MODEL = \"qwen2.5:7b\"  # For code analysis (Phase 4)\n\n# Performance settings\nEMBEDDING_CACHE_ENABLED = True\nLLM_MAX_TOKENS = 2048\nLLM_TEMPERATURE = 0.1  # Low temperature for consistent extraction\nLLM_TOP_P = 0.9\n```\n\n**Hardware Requirements**:\n\n```\nMinimum (for MVP with basic models):\n- CPU: 4 cores\n- RAM: 8GB\n- Storage: 10GB free space\n- OS: Linux, macOS, Windows\n\nRecommended (for good performance):\n- CPU: 8 cores (or GPU)\n- RAM: 16GB\n- GPU: 8GB+ VRAM (NVIDIA with CUDA preferred)\n- Storage: SSD with 50GB free\n\nOptimal (for best performance):\n- CPU: 16 cores\n- RAM: 32GB\n- GPU: NVIDIA RTX 3090/4090 (24GB VRAM)\n- Storage: NVMe SSD\n```\n\n---\n\n#### Ollama Client Implementation\n\n```python\nimport httpx\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\n\nlogger = logging.getLogger(__name__)\n\nclass OllamaClient:\n    \"\"\"Ollama client for local LLM operations.\"\"\"\n\n    def __init__(\n        self,\n        host: str = \"http://localhost:11434\",\n        embedding_model: str = \"nomic-embed-text\",\n        llm_model: str = \"llama3.1:8b\",\n        timeout: int = 60\n    ):\n        self.host = host\n        self.embedding_model = embedding_model\n        self.llm_model = llm_model\n        self.timeout = timeout\n        self.client = httpx.AsyncClient(timeout=timeout)\n\n    async def embed(self, text: str) -> List[float]:\n        \"\"\"\n        Generate embedding for text.\n\n        Args:\n            text: Input text (up to 2048 tokens for nomic-embed-text)\n\n        Returns:\n            embedding: 768-dim vector (for nomic-embed-text)\n        \"\"\"\n        try:\n            response = await self.client.post(\n                f\"{self.host}/api/embeddings\",\n                json={\n                    \"model\": self.embedding_model,\n                    \"prompt\": text\n                }\n            )\n            response.raise_for_status()\n\n            data = response.json()\n            embedding = data.get(\"embedding\")\n\n            if not embedding:\n                raise ValueError(\"No embedding in response\")\n\n            logger.debug(f\"Generated embedding of dimension: {len(embedding)}\")\n            return embedding\n\n        except httpx.HTTPError as e:\n            logger.error(f\"Ollama HTTP error: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Embedding generation error: {e}\")\n            raise\n\n    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings for multiple texts (sequential).\n\n        Note: Ollama doesn't support batch embeddings API,\n        so this processes sequentially. Use semantic cache\n        to mitigate performance impact.\n\n        Args:\n            texts: List of input texts\n\n        Returns:\n            embeddings: List of 768-dim vectors\n        \"\"\"\n        embeddings = []\n        for i, text in enumerate(texts):\n            embedding = await self.embed(text)\n            embeddings.append(embedding)\n\n            if (i + 1) % 10 == 0:\n                logger.info(f\"Embedded {i + 1}/{len(texts)} texts\")\n\n        return embeddings\n\n    async def extract_entities(\n        self,\n        text: str,\n        model: str = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Extract entities and relationships using LLM.\n\n        Args:\n            text: Input text to analyze\n            model: LLM model to use (default: self.llm_model)\n\n        Returns:\n            dict: {\n                \"entities\": [{\"name\": str, \"type\": str, \"description\": str, \"confidence\": float}],\n                \"relationships\": [{\"subject\": str, \"predicate\": str, \"object\": str, \"confidence\": float}]\n            }\n        \"\"\"\n        model = model or self.llm_model\n\n        # Define JSON schema for structured output\n        schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"entities\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\"},\n                            \"type\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"PERSON\", \"ORGANIZATION\", \"TECHNOLOGY\", \"CONCEPT\", \"EVENT\", \"LOCATION\"]\n                            },\n                            \"description\": {\"type\": \"string\"},\n                            \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                        },\n                        \"required\": [\"name\", \"type\", \"confidence\"]\n                    }\n                },\n                \"relationships\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"subject\": {\"type\": \"string\"},\n                            \"predicate\": {\"type\": \"string\"},\n                            \"object\": {\"type\": \"string\"},\n                            \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                        },\n                        \"required\": [\"subject\", \"predicate\", \"object\", \"confidence\"]\n                    }\n                }\n            }\n        }\n\n        # Extraction prompt\n        prompt = f\"\"\"Extract entities and relationships from the following text.\n\nText:\n{text}\n\nExtract:\n1. Entities: Important concepts, people, organizations, technologies, events, locations\n2. Relationships: How entities relate to each other (e.g., CREATED_BY, USES, PART_OF)\n\nProvide confidence scores (0-1) for each extraction.\nReturn ONLY valid JSON matching the schema, no other text.\n\"\"\"\n\n        try:\n            response = await self.client.post(\n                f\"{self.host}/api/generate\",\n                json={\n                    \"model\": model,\n                    \"prompt\": prompt,\n                    \"format\": \"json\",  # Force JSON output\n                    \"temperature\": 0.1,  # Low temp for consistency\n                    \"stream\": False\n                },\n                timeout=120  # Longer timeout for LLM\n            )\n            response.raise_for_status()\n\n            data = response.json()\n            response_text = data.get(\"response\", \"\")\n\n            # Parse JSON response\n            extracted = json.loads(response_text)\n\n            logger.info(\n                f\"Extracted {len(extracted.get('entities', []))} entities, \"\n                f\"{len(extracted.get('relationships', []))} relationships\"\n            )\n\n            return extracted\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to parse LLM response as JSON: {e}\")\n            return {\"entities\": [], \"relationships\": []}\n        except Exception as e:\n            logger.error(f\"Entity extraction error: {e}\")\n            return {\"entities\": [], \"relationships\": []}\n\n    async def test_connection(self) -> bool:\n        \"\"\"Test Ollama connection and model availability.\"\"\"\n        try:\n            # Check Ollama is running\n            response = await self.client.get(f\"{self.host}/api/tags\")\n            response.raise_for_status()\n\n            models = response.json().get(\"models\", [])\n            model_names = [m[\"name\"] for m in models]\n\n            # Check embedding model\n            if self.embedding_model not in model_names:\n                logger.warning(\n                    f\"Embedding model {self.embedding_model} not found. \"\n                    f\"Run: ollama pull {self.embedding_model}\"\n                )\n                return False\n\n            logger.info(f\"Ollama connected, model {self.embedding_model} available\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Ollama connection test failed: {e}\")\n            return False\n```\n\n---\n\n#### Ollama Performance Considerations\n\n**Embedding Generation**:\n- **Speed**: ~50-100ms per embedding (CPU), ~20-50ms (GPU)\n- **Limitation**: Sequential only (no batch API endpoint)\n- **Optimization Strategy**: Semantic caching (60%+ hit rate target) reduces calls dramatically\n- **Throughput**: ~10-20 embeddings/second (CPU), ~20-50/second (GPU)\n\n**LLM Inference** (Entity Extraction):\n- **Speed**: ~10-20 tokens/sec (CPU), ~50-100 tokens/sec (GPU)\n- **Context Window**: Up to 8192 tokens (llama3.1), 131K tokens (qwen2.5)\n- **Optimization**: Short prompts, low temperature (0.1), structured JSON output\n- **Typical Extraction Time**: 5-15 seconds per document\n\n**Memory Usage**:\n- **Model Loading**: ~2-5GB RAM per loaded model\n- **Concurrent Models**: Can load multiple models simultaneously (RAM permitting)\n- **Model Caching**: Models stay in RAM until manually unloaded or system restart\n\n**Hardware Optimization**:\n```python\n# CPU-only configuration (minimum hardware)\nOLLAMA_NUM_THREAD = 4  # CPU threads\nOLLAMA_NUM_GPU = 0  # Disable GPU\n\n# GPU configuration (recommended)\nOLLAMA_NUM_GPU = 1  # Use GPU\nOLLAMA_GPU_LAYERS = 32  # Offload layers to GPU (adjust based on VRAM)\n```\n\n---\n\n### 3. Python 3.10+ - Programming Language\n\n#### Why Python?\n\n**Decision Rationale**:\n\n1. **ML/AI Ecosystem Dominance**: Best-in-class libraries\n   - `sentence-transformers`: Local embeddings\n   - `transformers`: Hugging Face models\n   - `LangChain`: Document processing, RAG pipelines\n   - `SpaCy`: Fast NLP and NER\n   - `tree-sitter`: AST parsing (via bindings)\n   - `numpy`, `scipy`: Numerical operations\n\n2. **MCP SDK Native Support**: Official Python SDK from Anthropic\n   - Package: `mcp` on PyPI\n   - Well-documented, actively maintained\n   - Async/await support built-in\n   - Type hints for IDE support\n\n3. **FalkorDB & Ollama Clients**: Excellent Python support\n   - `falkordb`: Official FalkorDB Python client\n   - `httpx`: Modern async HTTP for Ollama API\n   - `redis`: If needed for caching\n\n4. **Proven Success**: Cognee demonstrates viability\n   - Similar architecture (memory + graph)\n   - Production deployments\n   - Python works well for this use case\n\n5. **Developer Productivity**: Fast iteration, extensive tooling\n   - Type hints (3.10+) for IDE autocomplete\n   - `pydantic` for validation\n   - `pytest` for testing\n   - Rich ecosystem of dev tools\n\n**Alternatives Considered**:\n- **TypeScript**: Claude Context uses it successfully, good MCP support, but weaker ML ecosystem\n- **Go**: Fast, good concurrency, but limited AI libraries and LLM integrations\n- **Rust**: Maximum performance, memory safety, but steep learning curve and longer dev time\n\n**Trade-offs**:\n- **Pros**:\n  - Ecosystem (unmatched for AI/ML)\n  - Productivity (fast development)\n  - Type hints (modern Python is type-safe)\n  - Community (largest AI dev community)\n\n- **Cons**:\n  - Performance vs compiled languages (mitigated with async I/O)\n  - GIL for CPU-bound tasks (not a problem for I/O-heavy RAG)\n  - Packaging can be complex (solved with modern tools)\n\n- **Risk Mitigation**:\n  - Use async I/O for concurrency\n  - Profile and optimize hot paths\n  - Consider Cython/numba for critical performance sections if needed\n\n---\n\n#### Python Environment\n\n**Version Requirements**:\n```python\n# Minimum Python version: 3.10\n# Recommended: Python 3.11 or 3.12\n# Reason: Performance improvements (10-20% faster), better type hints, improved error messages\n\npython_requires = \">=3.10\"\n```\n\n**Key Dependencies**:\n\n```toml\n[project]\nname = \"zapomni-mcp\"\nversion = \"0.1.0\"\ndescription = \"Local-first MCP memory server with knowledge graphs\"\nauthors = [{name = \"Tony\", email = \"tony@example.com\"}]\nlicense = {text = \"Apache-2.0\"}\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\n\ndependencies = [\n    # MCP Protocol\n    \"mcp>=0.1.0\",                    # Anthropic MCP SDK (official)\n\n    # Database\n    \"falkordb>=4.0.0\",               # Unified vector + graph DB\n    \"redis>=5.0.0\",                  # Semantic cache (optional)\n\n    # LLM & Embeddings\n    \"httpx>=0.25.0\",                 # Async HTTP for Ollama API\n    \"sentence-transformers>=2.5.0\",  # Fallback embeddings\n\n    # NLP & Text Processing\n    \"langchain>=0.1.0\",              # Document chunking, text splitting\n    \"spacy>=3.7.0\",                  # Fast NER for entity extraction\n    \"rank-bm25>=0.2.2\",              # BM25 keyword search\n\n    # Code Parsing (Phase 4)\n    \"tree-sitter>=0.20.0\",           # AST parsing\n    \"tree-sitter-python>=0.20.0\",    # Python grammar\n    \"tree-sitter-javascript>=0.20.0\", # JavaScript grammar\n\n    # Utilities\n    \"pydantic>=2.0.0\",               # Validation & settings\n    \"pydantic-settings>=2.0.0\",      # Settings management from env\n    \"python-dotenv>=1.0.0\",          # Load .env files\n    \"rapidfuzz>=3.0.0\",              # Fuzzy string matching (entity dedup)\n\n    # Async & I/O\n    \"asyncio>=3.4.3\",                # Async operations (stdlib but explicit)\n    \"aiofiles>=23.0.0\",              # Async file operations\n\n    # Logging\n    \"structlog>=23.0.0\",             # Structured logging\n]\n\n[project.optional-dependencies]\ndev = [\n    # Testing\n    \"pytest>=7.4.0\",\n    \"pytest-asyncio>=0.21.0\",\n    \"pytest-cov>=4.1.0\",\n    \"pytest-mock>=3.12.0\",\n\n    # Type Checking\n    \"mypy>=1.7.0\",\n    \"types-redis>=4.6.0\",\n\n    # Code Quality\n    \"black>=23.0.0\",                 # Code formatter\n    \"isort>=5.12.0\",                 # Import sorter\n    \"flake8>=6.1.0\",                 # Linter\n    \"pylint>=3.0.0\",                 # Advanced linter\n\n    # Documentation\n    \"mkdocs>=1.5.0\",\n    \"mkdocs-material>=9.4.0\",\n]\n\nspeed = [\n    # Optional performance dependencies\n    \"orjson>=3.9.0\",                 # Faster JSON (C-based)\n    \"uvloop>=0.19.0\",                # Faster asyncio event loop (Unix only)\n]\n\n[build-system]\nrequires = [\"setuptools>=68.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n```\n\n---\n\n#### Code Style & Conventions\n\n**Type Hints**: 100% coverage for public APIs\n\n```python\nfrom typing import List, Dict, Any, Optional, Union\nfrom pydantic import BaseModel\n\n# Function signatures with type hints\nasync def add_memory(\n    text: str,\n    metadata: Optional[Dict[str, Any]] = None\n) -> str:\n    \"\"\"\n    Add memory to storage.\n\n    Args:\n        text: Memory text content\n        metadata: Optional metadata dict\n\n    Returns:\n        memory_id: UUID string\n    \"\"\"\n    pass\n\n# Pydantic models for validation\nclass AddMemoryRequest(BaseModel):\n    text: str\n    metadata: Optional[Dict[str, Any]] = None\n\nclass SearchMemoryRequest(BaseModel):\n    query: str\n    limit: int = 10\n    filters: Optional[Dict[str, Any]] = None\n```\n\n**Async/Await**: For all I/O operations\n\n```python\n# Database operations\nasync def store_embedding(chunk_id: str, embedding: List[float]) -> None:\n    await db.execute(...)\n\n# HTTP requests\nasync def fetch_from_ollama(prompt: str) -> Dict[str, Any]:\n    async with httpx.AsyncClient() as client:\n        response = await client.post(...)\n        return response.json()\n\n# File operations\nasync def read_document(file_path: str) -> str:\n    async with aiofiles.open(file_path, 'r') as f:\n        return await f.read()\n```\n\n**Error Handling**: Explicit and informative\n\n```python\nclass ZapomniError(Exception):\n    \"\"\"Base exception for Zapomni errors.\"\"\"\n    pass\n\nclass DatabaseError(ZapomniError):\n    \"\"\"Database operation failed.\"\"\"\n    pass\n\nclass EmbeddingError(ZapomniError):\n    \"\"\"Embedding generation failed.\"\"\"\n    pass\n\n# Usage\ntry:\n    embedding = await generate_embedding(text)\nexcept httpx.TimeoutException:\n    raise EmbeddingError(\n        f\"Ollama timeout. Is Ollama running? \"\n        f\"Check: http://localhost:11434\"\n    ) from None\nexcept Exception as e:\n    raise EmbeddingError(f\"Embedding failed: {e}\") from e\n```\n\n**Logging**: Structured logging to stderr\n\n```python\nimport structlog\n\nlogger = structlog.get_logger()\n\n# Structured logs with context\nlogger.info(\n    \"memory_added\",\n    memory_id=memory_id,\n    chunk_count=len(chunks),\n    duration_ms=duration_ms,\n    user=\"system\"\n)\n\nlogger.error(\n    \"search_failed\",\n    query=query,\n    error=str(e),\n    exc_info=True  # Include stack trace\n)\n```\n\n---\n\n### 4. MCP Protocol - Communication Layer\n\n#### Why MCP Stdio?\n\n**Decision Rationale**:\n\n1. **Simplicity**: Stdio is the easiest transport to implement and debug\n   - No network configuration (ports, firewalls)\n   - No authentication/authorization complexity\n   - No TLS/SSL certificates\n   - Just stdin/stdout/stderr\n\n2. **Security**: Process isolation provides natural security boundary\n   - Each MCP server runs in separate process\n   - OS-level isolation\n   - No network exposure\n   - Minimal attack surface\n\n3. **Standard Protocol**: JSON-RPC 2.0 over newline-delimited JSON\n   - Well-specified protocol\n   - Language-agnostic\n   - Easy to parse and validate\n   - Extensive tooling support\n\n4. **Compatibility**: Works with all major MCP clients out of the box\n   - Claude CLI\n   - Claude Desktop\n   - Cursor IDE\n   - Cline extension\n   - Any MCP-compatible tool\n\n5. **Debugging**: Transparent message flow\n   - All messages visible in stderr logs\n   - Easy to replay and test\n   - No encrypted traffic to decrypt\n   - Simple to trace issues\n\n**Alternatives Considered**:\n- **HTTP Transport**: More complex (server, routes, auth), better for multi-user but overkill for local\n- **SSE (Server-Sent Events)**: Good for streaming, but adds complexity for MVP\n- **WebSockets**: Bidirectional and persistent, but unnecessary overhead for request-response\n\n**Trade-offs**:\n- **Pros**:\n  - Extremely simple\n  - Secure by default\n  - Easy debugging\n  - No configuration\n  - Universal compatibility\n\n- **Cons**:\n  - Single user per instance\n  - No remote access\n  - No streaming responses (initially)\n  - Process-based scaling only\n\n- **Future Enhancement**: Can add HTTP/SSE in Phase 5+ for web integrations while keeping stdio as default\n\n---\n\n#### MCP Server Implementation\n\n```python\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom pydantic import Field\nimport asyncio\nimport json\nimport logging\nimport sys\nfrom typing import Dict, Any, List, Optional\n\n# Configure logging to stderr (stdout reserved for MCP)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    stream=sys.stderr\n)\nlogger = logging.getLogger(__name__)\n\n# Create MCP server instance\nserver = Server(\"zapomni-memory\")\n\n# Import dependencies (initialized in main())\ndb = None  # FalkorDBClient\nembedder = None  # OllamaClient\nsearcher = None  # HybridSearchEngine\n\n@server.call_tool()\nasync def add_memory(\n    text: str = Field(description=\"Text to remember\"),\n    metadata: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"Optional metadata (tags, source, date)\"\n    )\n) -> Dict[str, Any]:\n    \"\"\"\n    Add new information to memory system.\n\n    This tool ingests text, generates embeddings locally,\n    and stores in FalkorDB for later retrieval.\n    \"\"\"\n    try:\n        logger.info(f\"Adding memory: {text[:50]}...\")\n\n        # Validate input\n        if not text or len(text.strip()) == 0:\n            return {\n                \"content\": [{\n                    \"type\": \"text\",\n                    \"text\": json.dumps({\n                        \"status\": \"error\",\n                        \"message\": \"Text cannot be empty\"\n                    })\n                }],\n                \"isError\": True\n            }\n\n        # Chunk text if needed (Phase 1: simple, Phase 2: semantic)\n        chunks = chunk_text(text)  # Returns list of text chunks\n\n        # Generate embeddings\n        embeddings = []\n        for chunk in chunks:\n            embedding = await embedder.embed(chunk)\n            embeddings.append(embedding)\n\n        # Store in FalkorDB\n        memory_id = await db.add_memory(\n            text=text,\n            embedding=embeddings[0],  # Use first chunk embedding for doc-level\n            metadata=metadata or {}\n        )\n\n        # Store chunks\n        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n            await db.add_chunk(\n                chunk_id=f\"{memory_id}_chunk_{i}\",\n                text=chunk,\n                doc_id=memory_id,\n                index=i,\n                embedding=embedding\n            )\n\n        logger.info(f\"Memory added: {memory_id} ({len(chunks)} chunks)\")\n\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"success\",\n                    \"memory_id\": memory_id,\n                    \"chunks_created\": len(chunks),\n                    \"text_preview\": text[:100]\n                }, indent=2)\n            }]\n        }\n\n    except Exception as e:\n        logger.error(f\"Error adding memory: {e}\", exc_info=True)\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"error\",\n                    \"message\": str(e)\n                })\n            }],\n            \"isError\": True\n        }\n\n@server.call_tool()\nasync def search_memory(\n    query: str = Field(description=\"Search query\"),\n    limit: int = Field(default=10, description=\"Maximum results to return\"),\n    filters: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"Optional filters (date_from, date_to, tags, source)\"\n    )\n) -> Dict[str, Any]:\n    \"\"\"\n    Search memories using hybrid retrieval (vector + BM25).\n\n    Returns relevant memories ranked by similarity.\n    \"\"\"\n    try:\n        logger.info(f\"Searching: {query}\")\n\n        # Generate query embedding\n        query_embedding = await embedder.embed(query)\n\n        # Hybrid search (Phase 1: vector only, Phase 2: add BM25)\n        results = await searcher.search(\n            query_embedding=query_embedding,\n            query_text=query,\n            limit=limit,\n            filters=filters or {}\n        )\n\n        logger.info(f\"Found {len(results)} results\")\n\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"success\",\n                    \"count\": len(results),\n                    \"results\": results\n                }, indent=2)\n            }]\n        }\n\n    except Exception as e:\n        logger.error(f\"Search error: {e}\", exc_info=True)\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"error\",\n                    \"message\": str(e)\n                })\n            }],\n            \"isError\": True\n        }\n\n@server.call_tool()\nasync def get_stats() -> Dict[str, Any]:\n    \"\"\"Get memory system statistics and health metrics.\"\"\"\n    try:\n        stats = await db.get_statistics()\n\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"success\",\n                    \"statistics\": stats\n                }, indent=2)\n            }]\n        }\n    except Exception as e:\n        logger.error(f\"Stats error: {e}\", exc_info=True)\n        return {\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": json.dumps({\n                    \"status\": \"error\",\n                    \"message\": str(e)\n                })\n            }],\n            \"isError\": True\n        }\n\nasync def main():\n    \"\"\"Main server entry point.\"\"\"\n    global db, embedder, searcher\n\n    logger.info(\"Starting Zapomni MCP Server...\")\n\n    # Initialize dependencies\n    from zapomni_db.client import FalkorDBClient\n    from zapomni_core.embeddings import OllamaClient\n    from zapomni_core.search import HybridSearchEngine\n\n    db = FalkorDBClient()\n    await db.connect()\n    logger.info(\"Connected to FalkorDB\")\n\n    embedder = OllamaClient()\n    connection_ok = await embedder.test_connection()\n    if not connection_ok:\n        logger.warning(\"Ollama connection issues, some features may not work\")\n\n    searcher = HybridSearchEngine(db, embedder)\n\n    # Run MCP server with stdio transport\n    async with stdio_server() as (read_stream, write_stream):\n        logger.info(\"MCP server running on stdio\")\n        await server.run(\n            read_stream,\n            write_stream,\n            server.create_initialization_options()\n        )\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        logger.info(\"Server stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {e}\", exc_info=True)\n        sys.exit(1)\n```\n\n---\n\n#### MCP Configuration (Claude Desktop)\n\n**Configuration File**: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n\n```json\n{\n  \"mcpServers\": {\n    \"zapomni\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"zapomni_mcp.server\"],\n      \"env\": {\n        \"FALKORDB_HOST\": \"localhost\",\n        \"FALKORDB_PORT\": \"6379\",\n        \"OLLAMA_HOST\": \"http://localhost:11434\",\n        \"EMBEDDING_MODEL\": \"nomic-embed-text\",\n        \"LLM_MODEL\": \"llama3.1:8b\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\n**Alternative: Using Script Path**:\n\n```json\n{\n  \"mcpServers\": {\n    \"zapomni\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/zapomni-mcp/src/server.py\"],\n      \"env\": {\n        \"FALKORDB_HOST\": \"localhost\",\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## System Components Detail\n\n### Document Processor\n\n**Purpose**: Extract text from documents and create semantic chunks.\n\n**Supported Formats** (Phase 1-2):\n- **PDF**: PyMuPDF (fitz) for text extraction, preserves structure\n- **DOCX**: python-docx for Microsoft Word documents\n- **Markdown**: Direct UTF-8 parsing, preserve headers and structure\n- **Plain Text**: UTF-8 text files\n- **HTML** (Phase 2): trafilatura for clean text extraction from web pages\n\n**Future Formats** (Phase 3+):\n- **Images**: OCR with tesseract\n- **Tables**: PDF table extraction with tabula-py\n- **Code**: AST-based parsing with tree-sitter (Phase 4)\n\n**Chunking Implementation**:\n\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom typing import List, Dict, Any\n\nclass DocumentProcessor:\n    \"\"\"Process documents into searchable chunks.\"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 512,\n        chunk_overlap: int = 100\n    ):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n        # Hierarchical separators (paragraphs → sentences → words)\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,  # Can use tokenizer instead\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n        )\n\n    def chunk_text(\n        self,\n        text: str,\n        metadata: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Chunk text into semantic segments.\n\n        Args:\n            text: Input text to chunk\n            metadata: Optional metadata to attach to chunks\n\n        Returns:\n            List of chunk dicts with text and metadata\n        \"\"\"\n        chunks = self.splitter.split_text(text)\n\n        result = []\n        for i, chunk in enumerate(chunks):\n            chunk_data = {\n                \"text\": chunk,\n                \"index\": i,\n                \"metadata\": metadata or {}\n            }\n            result.append(chunk_data)\n\n        return result\n\n    async def process_pdf(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Extract text from PDF file.\"\"\"\n        import fitz  # PyMuPDF\n\n        doc = fitz.open(file_path)\n\n        text_parts = []\n        for page_num, page in enumerate(doc):\n            text = page.get_text()\n            text_parts.append(text)\n\n        full_text = \"\\n\\n\".join(text_parts)\n\n        return {\n            \"text\": full_text,\n            \"metadata\": {\n                \"source\": file_path,\n                \"type\": \"pdf\",\n                \"pages\": len(doc)\n            }\n        }\n\n    async def process_markdown(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Extract text from Markdown file.\"\"\"\n        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:\n            text = await f.read()\n\n        return {\n            \"text\": text,\n            \"metadata\": {\n                \"source\": file_path,\n                \"type\": \"markdown\"\n            }\n        }\n```\n\n---\n\n### Embedding Generator\n\nSee **Ollama Client Implementation** in section 2 above for detailed code.\n\n**Key Features**:\n- Sequential embedding generation (Ollama API limitation)\n- Semantic caching for performance\n- Fallback to sentence-transformers if Ollama unavailable\n- Error handling with retries\n\n---\n\n### Hybrid Search Engine\n\n**Purpose**: Combine vector, keyword, and graph retrieval for best results.\n\n**Implementation**:\n\n```python\nfrom rank_bm25 import BM25Okapi\nfrom typing import List, Dict, Any\nimport numpy as np\n\nclass HybridSearchEngine:\n    \"\"\"Hybrid search combining BM25, vector, and graph retrieval.\"\"\"\n\n    def __init__(self, db, embedder):\n        self.db = db\n        self.embedder = embedder\n        self.bm25 = None\n        self.corpus = []\n\n    async def index_for_bm25(self, documents: List[str]):\n        \"\"\"Build BM25 index for keyword search.\"\"\"\n        self.corpus = documents\n        tokenized = [doc.lower().split() for doc in documents]\n        self.bm25 = BM25Okapi(tokenized)\n\n    async def search(\n        self,\n        query_embedding: List[float],\n        query_text: str,\n        limit: int = 10,\n        filters: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Hybrid search with vector + BM25 + graph.\n\n        Phase 1: Vector only\n        Phase 2: Vector + BM25 with RRF fusion\n        Phase 3: Add graph context\n        \"\"\"\n        # Vector search\n        vector_results = await self.db.vector_search(\n            query_embedding=query_embedding,\n            limit=50,  # Get more candidates for fusion\n            filters=filters\n        )\n\n        # Phase 1: Return vector results directly\n        # Phase 2+: Add BM25 and fusion here\n\n        return vector_results[:limit]\n\n    async def hybrid_search_v2(\n        self,\n        query_embedding: List[float],\n        query_text: str,\n        limit: int = 10,\n        filters: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Phase 2: Hybrid search with RRF fusion.\n        \"\"\"\n        # Vector search\n        vector_results = await self.db.vector_search(\n            query_embedding=query_embedding,\n            limit=50,\n            filters=filters\n        )\n\n        # BM25 search\n        tokenized_query = query_text.lower().split()\n        bm25_scores = self.bm25.get_scores(tokenized_query)\n\n        # Reciprocal Rank Fusion\n        k = 60  # RRF constant\n        rrf_scores = {}\n\n        # Add vector scores\n        for rank, result in enumerate(vector_results, start=1):\n            doc_id = result[\"id\"]\n            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank)\n\n        # Add BM25 scores\n        bm25_ranked = sorted(\n            enumerate(bm25_scores),\n            key=lambda x: x[1],\n            reverse=True\n        )[:50]\n\n        for rank, (idx, score) in enumerate(bm25_ranked, start=1):\n            doc_id = self.corpus[idx]  # Map to document ID\n            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank)\n\n        # Sort by RRF score\n        sorted_ids = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n\n        # Fetch full results\n        results = []\n        for doc_id, score in sorted_ids[:limit]:\n            # Fetch from vector_results or database\n            result = next((r for r in vector_results if r[\"id\"] == doc_id), None)\n            if result:\n                result[\"rrf_score\"] = score\n                results.append(result)\n\n        return results\n```\n\n---\n\n## Design Decisions & Rationale\n\n### Decision 1: Unified Database (FalkorDB) vs Separate Systems\n\n**Context**: Need both vector search and knowledge graph capabilities.\n\n**Options Considered**:\n1. **FalkorDB** (unified vector + graph in one system)\n2. **ChromaDB (vector) + Neo4j (graph)** (separate best-of-breed)\n3. **Qdrant (vector) + NetworkX (in-memory graph)**\n\n**Decision**: **FalkorDB** (unified)\n\n**Rationale**:\n- **Eliminates Synchronization Complexity**: No need to keep two databases in sync\n  - Single source of truth\n  - Consistent transactions (no dual-phase commits)\n  - Simpler backup and restore\n\n- **Superior Performance**: 496x faster P99 latency (FalkorDB benchmark)\n  - GraphBLAS backend for graph operations\n  - Redis protocol for vector operations\n  - Optimized for GraphRAG workloads\n\n- **Better Memory Efficiency**: 6x better than separate systems\n  - No data duplication\n  - Shared metadata storage\n  - Unified index structures\n\n- **Simpler Operations**:\n  - One connection pool\n  - One monitoring system\n  - One backup strategy\n  - Easier troubleshooting\n\n**Trade-offs Accepted**:\n- **Risk**: FalkorDB is newer (launched 2023), smaller community than Neo4j\n- **Mitigation**: Database abstraction layer allows fallback to ChromaDB + Neo4j if needed\n- **Assumption**: Performance and simplicity outweigh maturity concerns for MVP\n\n**Consequences**:\n- Simpler codebase (single DB client vs dual clients + sync logic)\n- Faster development (no sync logic to implement and test)\n- Better performance for hybrid queries\n- Lower operational overhead\n- Dependency on FalkorDB's roadmap and stability\n\n---\n\n### Decision 2: Local-First Architecture (Ollama) vs Cloud APIs\n\n**Context**: Need LLM for embeddings and entity extraction.\n\n**Options Considered**:\n1. **Local-only (Ollama)** - all processing on user's machine\n2. **Hybrid (local + optional cloud)** - default local, allow cloud for better quality\n3. **Cloud-first (OpenAI API)** - best quality, requires API keys\n\n**Decision**: **Local-only (Ollama)**\n\n**Rationale**:\n- **Complete Privacy Guarantee**: Data never leaves user's machine\n  - Legal documents (attorney-client privilege)\n  - Healthcare data (HIPAA compliance)\n  - Corporate IP (trade secrets, NDAs)\n  - Personal journals and research\n\n- **Zero API Costs**: No recurring fees\n  - $0 vs $100-500/month for cloud services\n  - Unlimited usage without throttling\n  - No rate limits or quotas\n\n- **Offline Capability**: Works without internet\n  - Air-gapped environments\n  - Low/no connectivity situations\n  - No dependency on external services\n\n- **Product Differentiation**: Unique position vs Cognee (cloud-dependent)\n  - Clear value proposition\n  - No compromise on privacy\n  - Aligns with open-source ethos\n\n**Trade-offs Accepted**:\n- **Quality**: Local models (Llama 3.1, DeepSeek-R1) slightly lower quality than GPT-4\n  - Mitigation: Hybrid approach (SpaCy + LLM) and confidence filtering\n  - Reality: DeepSeek-R1 approaches GPT-4 performance for reasoning\n\n- **Speed**: Local inference slower than cloud APIs\n  - Mitigation: Semantic caching (60%+ hit rate)\n  - Reality: For single-user, speed acceptable (5-15s per document)\n\n- **Hardware Requirements**: Requires user to have sufficient compute (8GB+ RAM)\n  - Mitigation: Clear hardware requirements in docs\n  - Fallback: Lighter models (all-MiniLM-L6-v2) for lower-end hardware\n\n**Consequences**:\n- Target market: Privacy-conscious users, cost-sensitive developers, researchers\n- Setup complexity: Users must install Ollama and pull models\n- Performance variability: Depends on user's hardware (CPU vs GPU)\n- **Strong differentiator**: Only local-first MCP memory with knowledge graphs\n\n---\n\n### Decision 3: Hybrid Search (BM25 + Vector) vs Vector-Only\n\n**Context**: How to retrieve relevant documents for queries.\n\n**Options Considered**:\n1. **Vector-only** (semantic similarity via embeddings)\n2. **BM25-only** (keyword-based lexical matching)\n3. **Hybrid (BM25 + Vector with fusion)**\n\n**Decision**: **Hybrid (BM25 + Vector)**\n\n**Rationale**:\n- **3.4x Better Accuracy**: Research benchmark (Diffbot) shows hybrid >> vector-only\n  - BM25 catches exact terminology matches\n  - Vector catches semantic similarity\n  - Fusion combines strengths of both\n\n- **Complementary Strengths**:\n  - **BM25**: Excels at exact terms, acronyms, names, technical jargon\n  - **Vector**: Excels at paraphrases, concepts, semantic similarity\n  - Together: Cover all query types\n\n- **Production Best Practice**: Modern RAG systems use hybrid (2024 standard)\n  - Weaviate, Qdrant, Pinecone all recommend hybrid\n  - Research papers consistently show improvement\n\n- **Minimal Overhead**: BM25 is fast (< 50ms for 10K docs)\n  - rank-bm25 library is pure Python, simple\n  - Fusion (RRF) is O(n) complexity, negligible cost\n\n**Trade-offs Accepted**:\n- **Complexity**: More code than vector-only (BM25 index + fusion logic)\n  - Mitigation: Clean abstraction, well-tested libraries\n\n- **Memory**: BM25 index requires additional RAM\n  - Mitigation: Tokenized corpus is small (~10MB for 10K docs)\n\n**Implementation Details**:\n- **Phase 1**: Vector-only (ship fast)\n- **Phase 2**: Add BM25 + RRF fusion (enhance quality)\n- **Fusion Method**: Reciprocal Rank Fusion (RRF) with k=60\n\n**Consequences**:\n- Better retrieval quality (measurable improvement)\n- Handles diverse query types (keywords + semantic)\n- Industry-standard approach (proven best practice)\n\n---\n\n### Decision 4: Semantic Chunking vs Fixed-Size Chunking\n\n**Context**: How to split documents into searchable units.\n\n**Options Considered**:\n1. **Fixed-size** (e.g., 512 tokens, no overlap)\n2. **Fixed-size with overlap** (e.g., 512 tokens, 100 token overlap)\n3. **Semantic chunking** (split on meaningful boundaries: paragraphs, sentences)\n\n**Decision**: **Semantic chunking with overlap** (RecursiveCharacterTextSplitter)\n\n**Rationale**:\n- **Preserves Meaning**: Respects document structure\n  - Doesn't break mid-sentence\n  - Keeps paragraphs together when possible\n  - Maintains context\n\n- **Research-Backed**: 256-512 tokens with 10-20% overlap is best practice (2024 studies)\n  - Unstructured.io benchmark\n  - LangChain documentation\n  - Weaviate blog research\n\n- **Hierarchical Separators**: Tries paragraphs first, then sentences, then words\n  - Maximizes semantic coherence\n  - Falls back gracefully for dense text\n\n- **Overlap Prevents Information Loss**: Edge information not lost between chunks\n  - 10-20% overlap ensures context continuity\n  - Slightly more storage, much better retrieval\n\n**Trade-offs Accepted**:\n- **Storage**: ~10-20% more chunks due to overlap\n  - Mitigation: Storage is cheap (embeddings are main cost)\n\n- **Processing Time**: Slightly slower than naive split\n  - Mitigation: Still very fast (< 1s for typical document)\n\n**Implementation**:\n```python\nRecursiveCharacterTextSplitter(\n    chunk_size=512,           # Target size\n    chunk_overlap=100,        # 20% overlap\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Hierarchical\n)\n```\n\n**Consequences**:\n- Better retrieval quality (chunks are meaningful units)\n- Fewer broken contexts\n- Industry-standard approach\n\n---\n\n### Decision 5: Background Task Processing for Knowledge Graph\n\n**Context**: Knowledge graph building takes 5-15 minutes for 1K documents (entity extraction, relationship detection).\n\n**Options Considered**:\n1. **Synchronous** (block until complete)\n2. **Asynchronous with callbacks** (return immediately, callback when done)\n3. **Background task queue** (return task ID, queryable status)\n\n**Decision**: **Background task queue** (Cognee pattern)\n\n**Rationale**:\n- **Better UX**: Immediate response to user\n  - No 15-minute wait\n  - User can continue working\n  - Progress queryable anytime\n\n- **MCP Compatibility**: MCP tools should respond quickly\n  - Timeout constraints\n  - User expects fast responses\n\n- **Proven Pattern**: Cognee uses this successfully\n  - Task ID for tracking\n  - Status endpoint (pending/running/completed/failed)\n  - Progress percentage\n\n- **Flexibility**: Can run multiple tasks concurrently (future)\n\n**Trade-offs Accepted**:\n- **Complexity**: More code than synchronous (task manager, status tracking)\n  - Mitigation: Reuse Cognee pattern, well-understood\n\n- **State Management**: Must track task state persistently\n  - Mitigation: In-memory for MVP (Phase 1-2), consider persistence later\n\n**Implementation**:\n```python\n# build_graph() returns immediately\n{\n    \"status\": \"accepted\",\n    \"task_id\": \"uuid-here\",\n    \"message\": \"Graph building started in background\"\n}\n\n# graph_status(task_id) checks progress\n{\n    \"status\": \"running\",\n    \"progress\": 67.5,  # percentage\n    \"started_at\": \"2025-11-22T10:00:00Z\"\n}\n```\n\n**Consequences**:\n- Responsive UX (tool calls return immediately)\n- Can handle long-running operations gracefully\n- Adds ~200 lines of task manager code\n\n---\n\n## Performance Architecture\n\n### Latency Targets\n\n**Query Performance (P95)**:\n- **Target**: < 500ms end-to-end for search_memory\n\n**Breakdown** (Phase 2 with hybrid search + reranking):\n- Embedding generation: 50ms (cached) / 150ms (uncached)\n- Vector search (FalkorDB): 100ms\n- BM25 search: 50ms\n- Fusion (RRF): 50ms\n- Cross-encoder reranking: 150ms\n- Overhead (network, parsing, logging): 50ms\n- **Total**: ~450ms (cached) / ~550ms (uncached)\n\n**Optimization Strategies**:\n- **Semantic cache**: Reduce embedding time from 150ms → 50ms for 60%+ of queries\n- **HNSW indexing**: Fast approximate search (vs brute-force)\n- **Parallel retrieval**: Run vector + BM25 concurrently\n- **Top-K limiting**: Only rerank top 20 candidates, not all\n\n---\n\n**Ingestion Performance**:\n- **Target**: > 100 documents/minute\n\n**Breakdown** (for typical document: 1000 words, 3 chunks):\n- PDF extraction: 500ms\n- Chunking: 10ms\n- Embedding (3 chunks × 100ms): 300ms\n- FalkorDB storage: 50ms\n- **Total**: ~860ms per document = ~70 docs/minute\n\n**With Batching** (Phase 2):\n- Process 10 documents in parallel\n- Bottleneck: Embedding (sequential via Ollama)\n- With semantic cache (60% hit rate): Effective ~40ms per embedding\n- Achievable: 100-150 documents/minute\n\n---\n\n### Memory Management\n\n**Target**: < 4GB RAM for 10K documents\n\n**Breakdown**:\n- FalkorDB: 1-2GB (10K chunks × 768 dims × 4 bytes + graph overhead)\n- Ollama (model loaded): 1GB (nomic-embed-text)\n- Python process: 500MB (server + libraries)\n- Redis cache: 500MB (semantic cache)\n- **Total**: ~3.5GB (within target)\n\n**For 100K documents**:\n- FalkorDB: 10-15GB\n- Ollama: 1GB\n- Python: 500MB\n- Redis: 2GB\n- **Total**: ~14-18GB (requires 16GB+ RAM system)\n\n**Optimization Strategies**:\n- **Streaming processing**: Don't load all documents into RAM at once\n- **Generator patterns**: Yield chunks instead of returning giant lists\n- **LRU cache eviction**: Limit cache size, evict old entries\n- **Lazy loading**: Load embeddings/chunks on-demand\n\n---\n\n### Scalability Architecture\n\n**Horizontal Scaling**: Not applicable for MVP (local single-user design)\n\n**Vertical Scaling** (document capacity on single machine):\n\n| Document Count | RAM Required | Storage | Query Latency (P95) | Notes |\n|----------------|--------------|---------|---------------------|-------|\n| 1K | 2GB | 100MB | < 300ms | MVP target |\n| 10K | 4GB | 1GB | < 500ms | Phase 1-2 target |\n| 100K | 16GB | 10GB | < 1s | Phase 3+ (with optimization) |\n| 1M | 64GB+ | 100GB | < 3s | Requires partitioning, HNSW tuning |\n\n**Scaling Strategies** (for large corpora):\n- **Partitioning**: Divide by date/topic, search relevant partitions only\n- **HNSW tuning**: Adjust M, efConstruction, efSearch for size/speed trade-off\n- **Approximate search**: Accept slightly lower recall for much faster queries\n- **Tiered storage**: Hot data (recent) in RAM, cold data on disk\n\n---\n\n## Security & Privacy Architecture\n\n### Local-Only Guarantees\n\n**Data Flow**: Everything stays local\n```\nUser Machine:\n  User → Claude CLI → Zapomni MCP Server → FalkorDB (localhost:6379) → Ollama (localhost:11434)\n\nNEVER:\n  User → External API\n  User → Cloud Storage\n  User → Telemetry Service\n```\n\n**Network Isolation**:\n- **MCP Server**: Stdio only, no network sockets\n- **FalkorDB**: Bind to 127.0.0.1 (localhost) only, not 0.0.0.0\n- **Ollama**: Bind to 127.0.0.1 only\n- **No Telemetry**: Zero external calls, no crash reporting, no analytics\n\n**Configuration Enforcement**:\n```python\n# FalkorDB: Localhost only\nFALKORDB_HOST = \"localhost\"  # NOT \"0.0.0.0\" or public IP\n\n# Ollama: Localhost only\nOLLAMA_HOST = \"http://localhost:11434\"  # NOT http://0.0.0.0\n\n# No telemetry\nTELEMETRY_ENABLED = False\nSENTRY_DSN = None\nANALYTICS_ENABLED = False\n```\n\n---\n\n### Input Validation\n\n**Pydantic Schemas**: All inputs validated\n\n```python\nfrom pydantic import BaseModel, Field, validator\n\nclass AddMemoryRequest(BaseModel):\n    text: str = Field(..., min_length=1, max_length=100000)\n    metadata: Optional[Dict[str, Any]] = Field(default=None)\n\n    @validator('text')\n    def text_not_empty(cls, v):\n        if not v.strip():\n            raise ValueError('Text cannot be empty or whitespace-only')\n        return v\n\n    @validator('metadata')\n    def metadata_size_limit(cls, v):\n        if v and len(json.dumps(v)) > 10000:\n            raise ValueError('Metadata too large (max 10KB)')\n        return v\n\nclass SearchMemoryRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=1000)\n    limit: int = Field(default=10, ge=1, le=100)\n    filters: Optional[Dict[str, Any]] = None\n```\n\n**Injection Prevention**:\n- **Cypher Queries**: Parameterized only, never string concatenation\n  ```python\n  # SAFE\n  query = \"MATCH (m:Memory {id: $id}) RETURN m\"\n  result = graph.query(query, {\"id\": memory_id})\n\n  # UNSAFE (never do this)\n  query = f\"MATCH (m:Memory {{id: '{memory_id}'}}) RETURN m\"\n  ```\n\n- **SQL Injection**: N/A (no SQL, only Cypher via FalkorDB client which handles escaping)\n- **Path Traversal**: Whitelist approach for file access (Phase 4 code indexing)\n  ```python\n  import os\n\n  def safe_path(user_path: str, allowed_dir: str) -> str:\n      # Resolve to absolute path\n      abs_path = os.path.abspath(user_path)\n      abs_allowed = os.path.abspath(allowed_dir)\n\n      # Check if within allowed directory\n      if not abs_path.startswith(abs_allowed):\n          raise ValueError(\"Path traversal not allowed\")\n\n      return abs_path\n  ```\n\n---\n\n### Data Encryption\n\n**At Rest**: User responsibility (OS-level encryption)\n- **Linux**: LUKS, dm-crypt, eCryptfs\n- **macOS**: FileVault\n- **Windows**: BitLocker\n\n**Recommendation**: Enable full-disk encryption on dev machine\n\n**In Transit**: N/A (local-only, no network transmission)\n\n**In Memory**: Cleartext (performance vs security trade-off for local single-user)\n\n---\n\n## Testing Strategy\n\n### Test Pyramid\n\n```\n          E2E (5%)\n         Integration (25%)\n        Unit Tests (70%)\n```\n\n**Unit Tests** (70%):\n- Pure functions, business logic\n- Fast (< 1ms per test)\n- No external dependencies (mock DB, Ollama)\n- Coverage target: 90%+\n\n**Integration Tests** (25%):\n- FalkorDB integration (real database in Docker)\n- Ollama integration (real Ollama instance)\n- MCP protocol (stdio communication)\n- Slower (< 100ms per test)\n\n**E2E Tests** (5%):\n- Full workflow (add → search → verify)\n- Claude CLI integration (manual + automated)\n- Slowest (< 5s per test)\n\n---\n\n### Test Organization\n\n```\ntests/\n├── unit/\n│   ├── test_chunker.py           # Document chunking logic\n│   ├── test_embedder.py          # Embedding generation (mocked)\n│   ├── test_search.py            # Search algorithms\n│   ├── test_entity_extractor.py  # Entity extraction (mocked)\n│   └── test_utils.py             # Utility functions\n│\n├── integration/\n│   ├── test_falkordb_client.py   # Real FalkorDB operations\n│   ├── test_ollama_client.py     # Real Ollama API calls\n│   ├── test_mcp_server.py        # MCP protocol communication\n│   └── test_hybrid_search.py     # End-to-end search pipeline\n│\n├── e2e/\n│   ├── test_full_workflow.py     # Add → Search → Graph\n│   └── test_claude_integration.py # Claude CLI integration\n│\n├── fixtures/\n│   ├── sample_documents.py       # Test documents\n│   ├── sample_queries.py         # Test queries\n│   └── mock_embeddings.py        # Mock embedding vectors\n│\n└── conftest.py                   # Pytest configuration\n```\n\n---\n\n### Performance Testing\n\n**Load Testing**:\n```python\nimport pytest\nimport time\n\n@pytest.mark.benchmark\ndef test_search_performance(benchmark):\n    \"\"\"Search should complete in < 500ms.\"\"\"\n    result = benchmark(search_memory, query=\"test query\", limit=10)\n    assert benchmark.stats.stats.mean < 0.5  # seconds\n\n@pytest.mark.load\nasync def test_concurrent_searches():\n    \"\"\"Handle 10 concurrent searches.\"\"\"\n    queries = [\"query {i}\" for i in range(10)]\n\n    start = time.time()\n    results = await asyncio.gather(*[\n        search_memory(q) for q in queries\n    ])\n    duration = time.time() - start\n\n    assert all(len(r) > 0 for r in results)\n    assert duration < 5.0  # All 10 searches in < 5s\n```\n\n**Memory Profiling**:\n```python\nimport tracemalloc\nimport pytest\n\n@pytest.mark.memory\ndef test_ingestion_memory():\n    \"\"\"Memory usage should stay under 4GB for 10K docs.\"\"\"\n    tracemalloc.start()\n\n    # Ingest 10K test documents\n    for i in range(10000):\n        add_memory(f\"Test document {i}\")\n\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n\n    assert peak < 4 * 1024**3  # 4GB in bytes\n```\n\n---\n\n## Deployment Architecture\n\n### Development Environment\n\n**Docker Compose** (recommended for dev):\n\n```yaml\n# docker-compose.dev.yml\nversion: '3.8'\n\nservices:\n  falkordb:\n    image: falkordb/falkordb:latest\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - falkordb_data:/data\n    environment:\n      - FALKORDB_ARGS=--save 60 1 --appendonly yes\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"  # Different port to avoid conflict\n    volumes:\n      - redis_cache_data:/data\n    command: redis-server --appendonly yes\n\nvolumes:\n  falkordb_data:\n  redis_cache_data:\n```\n\n**Start services**:\n```bash\ndocker-compose -f docker-compose.dev.yml up -d\n```\n\n**Ollama** (run natively, not in Docker for best performance):\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama service\nollama serve\n\n# Pull models\nollama pull nomic-embed-text\nollama pull llama3.1:8b\n```\n\n---\n\n### Production Deployment (Local Install)\n\n**Prerequisites**:\n1. Python 3.10+ installed\n2. Docker Desktop installed (for FalkorDB + Redis)\n3. Ollama installed (native, not Docker)\n\n**Step 1: Install Ollama**\n```bash\n# macOS / Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows: Download from https://ollama.com/download\n\n# Pull required models\nollama pull nomic-embed-text\nollama pull llama3.1:8b\n```\n\n**Step 2: Start FalkorDB + Redis**\n```bash\n# Create docker-compose.yml\ncat > docker-compose.yml <<EOF\nversion: '3.8'\nservices:\n  falkordb:\n    image: falkordb/falkordb:latest\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - ./data/falkordb:/data\n    restart: unless-stopped\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"\n    volumes:\n      - ./data/redis:/data\n    restart: unless-stopped\nEOF\n\n# Start services\ndocker-compose up -d\n```\n\n**Step 3: Install Zapomni**\n```bash\n# Install from PyPI (when published)\npip install zapomni-mcp\n\n# Or install from source\ngit clone https://github.com/yourusername/zapomni.git\ncd zapomni\npip install -e .\n```\n\n**Step 4: Configure Claude Desktop**\n```bash\n# macOS\nnano ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Linux\nnano ~/.config/Claude/claude_desktop_config.json\n\n# Add configuration:\n{\n  \"mcpServers\": {\n    \"zapomni\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"zapomni_mcp.server\"],\n      \"env\": {\n        \"FALKORDB_HOST\": \"localhost\",\n        \"FALKORDB_PORT\": \"6379\",\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n```\n\n**Step 5: Restart Claude Desktop**\n\n---\n\n## Monitoring & Observability\n\n### Logging\n\n**Structured Logs** (JSON format via structlog):\n\n```python\nimport structlog\nimport sys\n\n# Configure structured logging\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.format_exc_info,\n        structlog.processors.UnicodeDecoder(),\n        structlog.processors.JSONRenderer()\n    ],\n    wrapper_class=structlog.stdlib.BoundLogger,\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\n\n# Usage\nlogger.info(\n    \"memory_added\",\n    memory_id=memory_id,\n    chunk_count=len(chunks),\n    duration_ms=duration_ms,\n    user_id=\"system\"\n)\n\nlogger.error(\n    \"search_failed\",\n    query=query,\n    error_type=type(e).__name__,\n    error_message=str(e),\n    exc_info=True\n)\n```\n\n**Log Levels**:\n- **DEBUG**: Detailed debugging (off in production)\n- **INFO**: Normal operations (default)\n- **WARN**: Degraded performance, fallbacks, recoverable errors\n- **ERROR**: Failures requiring attention (with stack traces)\n\n**Log Destination**: stderr (stdout reserved for MCP protocol)\n\n---\n\n### Metrics (Optional, Phase 5+)\n\n**Prometheus-Compatible Metrics**:\n\n```python\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Counters\nmemory_adds_total = Counter('zapomni_memory_adds_total', 'Total memory additions')\nsearch_queries_total = Counter('zapomni_search_queries_total', 'Total search queries')\nerrors_total = Counter('zapomni_errors_total', 'Total errors', ['error_type'])\n\n# Histograms\nsearch_latency_seconds = Histogram(\n    'zapomni_search_latency_seconds',\n    'Search query latency',\n    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0]\n)\n\nembedding_latency_seconds = Histogram(\n    'zapomni_embedding_latency_seconds',\n    'Embedding generation latency'\n)\n\n# Gauges\ntotal_memories = Gauge('zapomni_total_memories', 'Total memories stored')\ncache_hit_rate = Gauge('zapomni_cache_hit_rate', 'Semantic cache hit rate')\n```\n\n---\n\n### Tracing (Future)\n\n**OpenTelemetry** (for distributed tracing):\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Configure tracer\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\n\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(jaeger_exporter)\n)\n\n# Usage\nwith tracer.start_as_current_span(\"search_memory\"):\n    with tracer.start_as_current_span(\"generate_embedding\"):\n        embedding = await embedder.embed(query)\n\n    with tracer.start_as_current_span(\"vector_search\"):\n        results = await db.vector_search(embedding)\n\n    return results\n```\n\n---\n\n## Migration & Versioning\n\n### Database Migrations\n\n**Schema Versioning**:\n\n```python\nSCHEMA_VERSION = \"1.0.0\"\n\nasync def get_schema_version(graph) -> str:\n    \"\"\"Get current schema version from database.\"\"\"\n    query = \"MATCH (v:SchemaVersion) RETURN v.version as version\"\n    result = graph.query(query)\n    if result.result_set:\n        return result.result_set[0][0]\n    return \"0.0.0\"\n\nasync def migrate_to_v1_1(graph):\n    \"\"\"Migration: Add language field to Chunk nodes.\"\"\"\n    # Add new property\n    query = \"\"\"\n    MATCH (c:Chunk)\n    WHERE NOT EXISTS(c.language)\n    SET c.language = 'unknown'\n    \"\"\"\n    graph.query(query)\n\n    # Create new index\n    graph.query(\"CREATE INDEX FOR (c:Chunk) ON (c.language)\")\n\n    # Update schema version\n    graph.query(\"\"\"\n        MERGE (v:SchemaVersion)\n        SET v.version = '1.1.0'\n    \"\"\")\n\nasync def run_migrations(graph):\n    \"\"\"Run pending migrations.\"\"\"\n    current_version = await get_schema_version(graph)\n\n    if current_version < \"1.1.0\":\n        logger.info(\"Running migration to v1.1.0\")\n        await migrate_to_v1_1(graph)\n```\n\n**Backwards Compatibility Strategy**:\n- **Additive changes only** (for MVP/Phase 1-3)\n- **Breaking changes**: Require major version bump and migration tool\n- **Data preservation**: Always preserve existing data during migrations\n\n---\n\n### API Versioning\n\n**MCP Tool Versioning**:\n\n```python\nTOOL_VERSION = \"0.1.0\"\n\n# Tool definition includes version\n@server.tool()\nasync def add_memory_v1(text: str, metadata: dict = None) -> dict:\n    \"\"\"\n    Add memory (v1).\n\n    Version: 0.1.0\n    Breaking changes: N/A\n    \"\"\"\n    pass\n\n# For breaking changes, create new versioned tool\n@server.tool()\nasync def add_memory_v2(text: str, metadata: dict = None, tags: list = None) -> dict:\n    \"\"\"\n    Add memory (v2) with new tags parameter.\n\n    Version: 0.2.0\n    Breaking changes: Added required 'tags' parameter\n    \"\"\"\n    pass\n```\n\n**Semantic Versioning**:\n- **Major** (1.0.0 → 2.0.0): Breaking changes\n- **Minor** (0.1.0 → 0.2.0): New features, backwards compatible\n- **Patch** (0.1.0 → 0.1.1): Bug fixes\n\n---\n\n## Disaster Recovery\n\n### Backup Strategy\n\n**FalkorDB Backup**:\n\n```bash\n# RDB Snapshots (point-in-time backups)\n# Configured in docker-compose.yml:\nFALKORDB_ARGS=--save 900 1 --save 300 10 --save 60 10000\n\n# Translation:\n# - Save if 1+ keys changed after 900s (15 min)\n# - Save if 10+ keys changed after 300s (5 min)\n# - Save if 10000+ keys changed after 60s (1 min)\n\n# AOF (Append-Only File) for durability\nFALKORDB_ARGS=--appendonly yes --appendfsync everysec\n\n# Manual backup\ndocker exec zapomni_falkordb redis-cli --rdb /data/backup-$(date +%Y%m%d-%H%M%S).rdb\n```\n\n**Backup Location**:\n```\n~/.zapomni/backups/\n├── falkordb-20251122-1430.rdb\n├── falkordb-20251122-1500.rdb\n└── falkordb-20251122-1530.rdb\n```\n\n**Automated Backup Script**:\n\n```bash\n#!/bin/bash\n# backup-zapomni.sh\n\nBACKUP_DIR=\"$HOME/.zapomni/backups\"\nTIMESTAMP=$(date +%Y%m%d-%H%M%S)\n\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup FalkorDB\ndocker exec zapomni_falkordb redis-cli BGSAVE\nsleep 5\ndocker cp zapomni_falkordb:/data/dump.rdb \"$BACKUP_DIR/falkordb-$TIMESTAMP.rdb\"\n\n# Keep only last 7 days of backups\nfind \"$BACKUP_DIR\" -name \"falkordb-*.rdb\" -mtime +7 -delete\n\necho \"Backup completed: falkordb-$TIMESTAMP.rdb\"\n```\n\n**Cron job** (daily backups at 2 AM):\n```cron\n0 2 * * * /home/user/zapomni/backup-zapomni.sh\n```\n\n---\n\n### Recovery Procedure\n\n**Restore from Backup**:\n\n```bash\n#!/bin/bash\n# restore-zapomni.sh\n\nBACKUP_FILE=$1\n\nif [ -z \"$BACKUP_FILE\" ]; then\n    echo \"Usage: ./restore-zapomni.sh <backup-file.rdb>\"\n    exit 1\nfi\n\n# 1. Stop Zapomni and services\ndocker-compose down\n\n# 2. Restore RDB file\ncp \"$BACKUP_FILE\" ./data/falkordb/dump.rdb\n\n# 3. Restart services\ndocker-compose up -d\n\n# 4. Verify data integrity\nsleep 5\ndocker exec zapomni_falkordb redis-cli PING\n\n# 5. Check database stats\ndocker exec zapomni_falkordb redis-cli DBSIZE\n\necho \"Restore completed. Database is ready.\"\n```\n\n---\n\n## Future Technical Roadmap\n\n### Phase 5: Multi-Transport Support (Month 3)\n\n**HTTP Transport**:\n- FastAPI server for HTTP endpoints\n- RESTful API compatible with MCP\n- Authentication (API keys)\n- Rate limiting\n- CORS configuration\n\n**SSE Transport**:\n- Server-Sent Events for streaming responses\n- Real-time updates\n- Progress streaming for long operations\n\n**WebSocket** (Optional):\n- Bidirectional communication\n- Real-time notifications\n- Graph updates\n\n---\n\n### Phase 6: Performance Optimization (Month 4-5)\n\n**GPU Acceleration**:\n- CUDA support for embeddings\n- GPU-based vector search (FAISS-GPU)\n- Batch processing optimization\n\n**Model Optimization**:\n- Quantized models (4-bit, 8-bit)\n- Model distillation for faster inference\n- Custom fine-tuned models\n\n**Distributed Processing**:\n- Multi-process embedding generation\n- Parallel graph building\n- Load balancing\n\n---\n\n### Phase 7: Advanced Features (Month 6+)\n\n**Multi-Modal**:\n- Image embeddings (CLIP)\n- Audio transcription and search\n- Video frame analysis\n- Cross-modal search\n\n**Federated Graphs**:\n- Merge multiple user graphs\n- Shared knowledge bases\n- Privacy-preserving aggregation\n\n**Plugin System**:\n- Custom document processors\n- Custom embedders\n- Custom entity extractors\n- Community plugin marketplace\n\n---\n\n**Document Status**: Draft v1.0\n**Created**: 2025-11-22\n**Authors**: Tony + Claude Code\n**Last Updated**: 2025-11-22\n**Next Steps**: Review, approval via steering workflow, proceed to structure.md\n\n---\n\n**Document Length**: ~1250 lines\n**Estimated Reading Time**: 60-75 minutes\n**Target Audience**: Technical contributors, system architects, senior engineers\n",
  "fileStats": {
    "size": 106172,
    "lines": 3428,
    "lastModified": "2025-11-22T20:42:21.025Z"
  },
  "comments": []
}
{
  "id": "snapshot_1763844829562_dgc0624wr",
  "approvalId": "approval_1763844829559_b6ctwuxbv",
  "approvalTitle": "Structure Document (structure.md) - Project Organization & Conventions",
  "version": 1,
  "timestamp": "2025-11-22T20:53:49.562Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Project Structure: Zapomni\n\n**Document Version**: 1.0\n**Created**: 2025-11-22\n**Authors**: Tony + Claude Code\n**Status**: Draft\n**Alignment**: Consistent with product.md and tech.md\n\n---\n\n## Executive Summary\n\n### Purpose of This Document\n\nThis document provides the **definitive guide to Zapomni's codebase organization, conventions, and development workflows**. It serves as the primary reference for:\n\n- **New developers** setting up local development environments\n- **Contributors** understanding code organization and contribution guidelines\n- **Maintainers** making architectural decisions and reviewing PRs\n- **Future Claude agents** navigating the codebase and maintaining consistency\n\nThe goal is **actionable practicality**: every section includes concrete examples, copy-paste ready configurations, and clear rationale. This is not abstract theory—this is how we build Zapomni.\n\n### Key Organizational Principles\n\n1. **Monorepo Structure**: Single repository with three distinct Python packages in `/home/dev/zapomni/src/`\n2. **Package Separation**: Clean separation between MCP server, core logic, and database layers\n3. **Convention Over Configuration**: Sensible defaults, minimal setup required\n4. **Easy Onboarding**: New developer from zero to running tests in < 30 minutes\n\n### Project Philosophy\n\n- **Local-First**: All code runs locally, zero cloud dependencies\n- **Privacy by Design**: Data never leaves the machine\n- **Performance from Day One**: Measure, optimize, maintain speed budgets\n- **Developer-Friendly**: Clear errors, good docs, fast feedback loops\n- **Open Source**: Transparent development, welcoming community\n\n---\n\n## Directory Layout\n\n### Complete Project Structure\n\n```\n/home/dev/zapomni/                          # Project root\n├── .spec-workflow/                         # Spec workflow system\n│   ├── specs/                              # Feature specifications\n│   ├── steering/                           # Steering documents (this file)\n│   │   ├── product.md                      # Product vision ✅ APPROVED\n│   │   ├── tech.md                         # Technical architecture ✅ APPROVED\n│   │   └── structure.md                    # THIS DOCUMENT\n│   ├── templates/                          # Spec templates\n│   └── approvals/                          # Approval tracking\n│\n├── research/                               # Research reports (preserved)\n│   ├── 00_final_synthesis.md               # Implementation roadmap\n│   ├── 01_tech_stack_infrastructure.md     # Tech evaluation\n│   ├── 02_mcp_solutions_architectures.md   # MCP patterns\n│   └── 03_best_practices_patterns.md       # RAG best practices\n│\n├── src/                                    # Source code (main packages)\n│   ├── zapomni_mcp/                        # MCP server package\n│   │   ├── __init__.py                     # Package exports\n│   │   ├── server.py                       # Main MCP server entry point\n│   │   ├── tools/                          # MCP tool implementations\n│   │   │   ├── __init__.py                 # Tool registry\n│   │   │   ├── add_memory.py               # Phase 1: add_memory tool\n│   │   │   ├── search_memory.py            # Phase 1: search_memory tool\n│   │   │   ├── get_stats.py                # Phase 1: get_stats tool\n│   │   │   ├── build_graph.py              # Phase 2: build_graph tool\n│   │   │   ├── get_related.py              # Phase 2: get_related tool\n│   │   │   ├── graph_status.py             # Phase 2: graph_status tool\n│   │   │   ├── index_codebase.py           # Phase 3: index_codebase tool\n│   │   │   ├── delete_memory.py            # Phase 3: delete_memory tool\n│   │   │   ├── clear_all.py                # Phase 3: clear_all tool\n│   │   │   └── export_graph.py             # Phase 3: export_graph tool\n│   │   ├── schemas/                        # Pydantic models for validation\n│   │   │   ├── __init__.py                 # Schema exports\n│   │   │   ├── requests.py                 # Request schemas\n│   │   │   └── responses.py                # Response schemas\n│   │   ├── config.py                       # Configuration management\n│   │   └── logging.py                      # Logging setup (structlog)\n│   │\n│   ├── zapomni_core/                       # Core processing logic\n│   │   ├── __init__.py                     # Package exports\n│   │   ├── processors/                     # Document processors\n│   │   │   ├── __init__.py                 # Processor registry\n│   │   │   ├── base.py                     # Base processor interface\n│   │   │   ├── text_processor.py           # Text/markdown processing\n│   │   │   ├── pdf_processor.py            # PDF processing (PyMuPDF)\n│   │   │   └── code_processor.py           # Code AST processing (Phase 3)\n│   │   ├── embeddings/                     # Embedding generation\n│   │   │   ├── __init__.py                 # Embedder registry\n│   │   │   ├── ollama_embedder.py          # Ollama client (nomic-embed-text)\n│   │   │   ├── sentence_transformer.py     # Fallback embedder (all-MiniLM)\n│   │   │   └── cache.py                    # Semantic cache (Phase 2)\n│   │   ├── extractors/                     # Entity & relationship extraction\n│   │   │   ├── __init__.py                 # Extractor registry\n│   │   │   ├── entity_extractor.py         # Hybrid SpaCy + LLM (Phase 2)\n│   │   │   └── relationship_extractor.py   # Relationship detection (Phase 2)\n│   │   ├── search/                         # Search engines\n│   │   │   ├── __init__.py                 # Search registry\n│   │   │   ├── vector_search.py            # Vector similarity search (Phase 1)\n│   │   │   ├── bm25_search.py              # BM25 keyword search (Phase 2)\n│   │   │   ├── hybrid_search.py            # Fusion (RRF) (Phase 2)\n│   │   │   ├── reranker.py                 # Cross-encoder reranking (Phase 2)\n│   │   │   └── graph_search.py             # Graph traversal queries (Phase 2)\n│   │   ├── chunking/                       # Document chunking\n│   │   │   ├── __init__.py                 # Chunker registry\n│   │   │   ├── semantic_chunker.py         # LangChain RecursiveTextSplitter\n│   │   │   └── ast_chunker.py              # Code AST chunking (Phase 3)\n│   │   ├── tasks/                          # Background tasks\n│   │   │   ├── __init__.py                 # Task manager exports\n│   │   │   ├── task_manager.py             # Async task queue (Phase 2)\n│   │   │   └── status_tracker.py           # Progress tracking (Phase 2)\n│   │   └── utils/                          # Utilities\n│   │       ├── __init__.py                 # Utility exports\n│   │       ├── text_utils.py               # Text processing utilities\n│   │       └── validation.py               # Input validation helpers\n│   │\n│   └── zapomni_db/                         # Database integrations\n│       ├── __init__.py                     # Package exports\n│       ├── falkordb/                       # FalkorDB client\n│       │   ├── __init__.py                 # Client exports\n│       │   ├── client.py                   # Main FalkorDB client wrapper\n│       │   ├── schema.py                   # Schema definitions (nodes, edges)\n│       │   ├── queries.py                  # Cypher query templates\n│       │   └── migrations.py               # Schema migrations (future)\n│       ├── redis_cache/                    # Redis semantic cache\n│       │   ├── __init__.py                 # Cache exports\n│       │   └── cache_client.py             # Redis client wrapper\n│       └── models.py                       # Shared data models (Pydantic)\n│\n├── tests/                                  # Test suite\n│   ├── __init__.py                         # Test package\n│   ├── unit/                               # Unit tests (70% of tests)\n│   │   ├── __init__.py\n│   │   ├── test_chunker.py                 # Chunking tests\n│   │   ├── test_embedder.py                # Embedding tests\n│   │   ├── test_search.py                  # Search algorithm tests\n│   │   ├── test_entity_extractor.py        # Entity extraction tests\n│   │   └── ...                             # One test file per module\n│   ├── integration/                        # Integration tests (25% of tests)\n│   │   ├── __init__.py\n│   │   ├── test_falkordb_client.py         # FalkorDB integration\n│   │   ├── test_ollama_client.py           # Ollama integration\n│   │   ├── test_mcp_server.py              # MCP server integration\n│   │   └── ...\n│   ├── e2e/                                # End-to-end tests (5% of tests)\n│   │   ├── __init__.py\n│   │   └── test_full_workflow.py           # Complete add→search workflow\n│   ├── fixtures/                           # Test fixtures and data\n│   │   ├── sample_docs.py                  # Sample documents\n│   │   ├── mock_embeddings.py              # Mock embedding data\n│   │   └── test_data/                      # Static test files\n│   │       ├── sample.pdf\n│   │       ├── sample.md\n│   │       └── sample_code.py\n│   └── conftest.py                         # Pytest configuration and fixtures\n│\n├── docs/                                   # Documentation\n│   ├── README.md                           # Main documentation entry point\n│   ├── architecture/                       # Architecture docs\n│   │   ├── overview.md                     # System overview\n│   │   ├── mcp_protocol.md                 # MCP protocol integration\n│   │   └── data_flow.md                    # Data flow diagrams\n│   ├── api/                                # API reference\n│   │   ├── tools.md                        # MCP tools reference\n│   │   └── python_api.md                   # Python API (if exposed)\n│   ├── guides/                             # How-to guides\n│   │   ├── quick_start.md                  # 5-minute quickstart\n│   │   ├── installation.md                 # Detailed installation\n│   │   ├── configuration.md                # Configuration guide\n│   │   └── contributing.md                 # Contribution guidelines\n│   └── benchmarks/                         # Performance benchmarks\n│       └── results.md                      # Benchmark results\n│\n├── docker/                                 # Docker configurations\n│   ├── docker-compose.yml                  # Development environment\n│   ├── docker-compose.prod.yml             # Production (optional)\n│   ├── Dockerfile.falkordb                 # Custom FalkorDB image (if needed)\n│   └── .env.example                        # Environment variables template\n│\n├── scripts/                                # Utility scripts\n│   ├── setup.sh                            # Initial setup script\n│   ├── dev.sh                              # Start dev environment\n│   ├── test.sh                             # Run all tests\n│   ├── benchmark.sh                        # Run benchmarks\n│   └── cleanup.sh                          # Clean up docker volumes\n│\n├── .github/                                # GitHub workflows (future)\n│   └── workflows/\n│       ├── test.yml                        # CI tests\n│       └── publish.yml                     # PyPI publish\n│\n├── pyproject.toml                          # Python project config\n├── README.md                               # Project README\n├── LICENSE                                 # Apache 2.0 license\n├── .gitignore                              # Git ignore rules\n├── .env.example                            # Environment template\n├── .python-version                         # Python version (3.10+)\n├── .pre-commit-config.yaml                 # Pre-commit hooks\n└── AGENT_WORKFLOW.md                       # Agent workflow rules\n```\n\n### Directory Purpose Explanations\n\n#### Root Level\n\n**`.spec-workflow/`**: Spec workflow system\n- `specs/`: Feature specifications (requirements, design, tasks)\n- `steering/`: High-level steering documents (product, tech, structure)\n- `templates/`: Templates for specs and steering docs\n- `approvals/`: Approval tracking for documents\n\n**`research/`**: Research documents (preserved for reference)\n- Final synthesis and technical research reports\n- **Purpose**: Historical context, decision rationale, future reference\n- **Status**: Read-only archive, not actively maintained\n\n**`src/`**: Source code packages (main development here)\n- Three distinct Python packages: `zapomni_mcp`, `zapomni_core`, `zapomni_db`\n- **Rationale**: Clean separation of concerns, modularity, testability\n\n**`tests/`**: All tests (unit, integration, e2e)\n- 70% unit, 25% integration, 5% e2e (test pyramid)\n- Mirrors `src/` structure for easy navigation\n\n**`docs/`**: User and developer documentation\n- Architecture, API reference, guides, benchmarks\n- **Audience**: Users (quick start), developers (API), contributors (guides)\n\n**`docker/`**: Docker configurations for services\n- FalkorDB, Redis, Ollama (future: containerized)\n- Development and production compose files\n\n**`scripts/`**: Automation scripts\n- Setup, development, testing, benchmarking\n- **Goal**: One-command operations\n\n**`.github/`**: CI/CD workflows (future)\n- Automated testing, linting, publishing\n- **Phase**: Post-MVP\n\n#### Source Packages (`src/`)\n\n**`zapomni_mcp/`**: MCP server implementation\n- **Purpose**: MCP protocol handling, tool definitions, stdio transport\n- **Responsibilities**: Thin layer that delegates to `zapomni_core`\n- **Key Files**:\n  - `server.py`: Main entry point, server setup, tool registration\n  - `tools/`: Individual MCP tool implementations (one file per tool)\n  - `schemas/`: Pydantic request/response schemas\n  - `config.py`: Configuration loading (environment variables)\n  - `logging.py`: Structured logging setup (stderr only)\n\n**`zapomni_core/`**: Core business logic and processing\n- **Purpose**: Document processing, search, entity extraction, task management\n- **Responsibilities**: All business logic, algorithms, processing pipelines\n- **Key Modules**:\n  - `processors/`: Document processing (PDF, text, code)\n  - `embeddings/`: Embedding generation (Ollama, fallback)\n  - `search/`: Search implementations (vector, BM25, hybrid, graph)\n  - `extractors/`: Entity and relationship extraction\n  - `chunking/`: Text and code chunking strategies\n  - `tasks/`: Background task management\n  - `utils/`: Shared utilities\n\n**`zapomni_db/`**: Database client implementations\n- **Purpose**: Database abstractions, storage layer\n- **Responsibilities**: FalkorDB client, Redis cache, data models\n- **Key Modules**:\n  - `falkordb/`: FalkorDB client, schema, queries\n  - `redis_cache/`: Redis cache client\n  - `models.py`: Shared Pydantic data models\n\n#### Separation Rationale\n\n**Why Three Packages?**\n\n1. **zapomni_mcp**: MCP-specific code\n   - Easy to swap transport layer (stdio → HTTP)\n   - Clear boundary between protocol and logic\n   - Testable in isolation\n\n2. **zapomni_core**: Reusable business logic\n   - Can be used standalone (without MCP)\n   - Clean algorithms independent of transport\n   - Maximum testability (pure functions)\n\n3. **zapomni_db**: Database abstraction\n   - Easy to swap backends (FalkorDB → ChromaDB + Neo4j)\n   - Clean separation of storage concerns\n   - Mockable for testing\n\n**Benefits**:\n- **Modularity**: Change one layer without affecting others\n- **Testability**: Mock dependencies easily\n- **Reusability**: Core logic can be used in other contexts\n- **Clarity**: Clear responsibilities per package\n\n---\n\n## Module Organization\n\n### Package: zapomni_mcp\n\n**Purpose**: MCP protocol implementation and server\n\n**Design Pattern**: Thin adapter layer that delegates to `zapomni_core`\n\n#### Key Files\n\n**`server.py`**: Main entry point\n```python\n\"\"\"Zapomni MCP Server - Local-first AI Memory System\n\nThis is the main entry point for the Zapomni MCP server. It sets up the\nstdio transport, registers all tools, and handles incoming MCP requests.\n\nUsage:\n    python -m zapomni_mcp.server\n\nEnvironment Variables:\n    FALKORDB_HOST: FalkorDB host (default: localhost)\n    FALKORDB_PORT: FalkorDB port (default: 6379)\n    OLLAMA_HOST: Ollama API URL (default: http://localhost:11434)\n    LOG_LEVEL: Logging level (default: INFO)\n\"\"\"\n\nimport asyncio\nimport sys\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\n\nfrom .config import settings\nfrom .logging import setup_logging\nfrom .tools import register_tools\n\n# Setup logging to stderr (stdout reserved for MCP)\nsetup_logging(level=settings.log_level)\n\nasync def main():\n    \"\"\"Main entry point for Zapomni MCP server.\"\"\"\n    # Create MCP server\n    server = Server(\"zapomni-memory\")\n\n    # Register all tools\n    register_tools(server)\n\n    # Start stdio transport\n    async with stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            server.create_initialization_options()\n        )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n**`tools/__init__.py`**: Tool registry\n```python\n\"\"\"MCP Tool Registry\n\nRegisters all available tools with the MCP server.\n\"\"\"\n\nfrom mcp.server import Server\n\nfrom .add_memory import add_memory\nfrom .search_memory import search_memory\nfrom .get_stats import get_stats\n\ndef register_tools(server: Server) -> None:\n    \"\"\"Register all MCP tools with the server.\n\n    Args:\n        server: MCP Server instance\n    \"\"\"\n    # Phase 1: Essential tools\n    server.tool()(add_memory)\n    server.tool()(search_memory)\n    server.tool()(get_stats)\n\n    # Phase 2: Knowledge graph tools (conditional import)\n    try:\n        from .build_graph import build_graph\n        from .get_related import get_related\n        from .graph_status import graph_status\n\n        server.tool()(build_graph)\n        server.tool()(get_related)\n        server.tool()(graph_status)\n    except ImportError:\n        pass  # Phase 2 tools not yet implemented\n```\n\n**`tools/add_memory.py`**: Individual tool implementation\n```python\n\"\"\"add_memory MCP Tool\n\nStore information in memory system with automatic chunking and embedding.\n\"\"\"\n\nfrom typing import Dict, Any, Optional\nimport structlog\n\nfrom ..schemas.requests import AddMemoryRequest\nfrom ..schemas.responses import AddMemoryResponse\nfrom zapomni_core.processors import DocumentProcessor\nfrom zapomni_db.falkordb import FalkorDBClient\nfrom ..config import settings\n\nlogger = structlog.get_logger()\n\nasync def add_memory(\n    text: str,\n    metadata: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    \"\"\"Store information in memory system.\n\n    Processes the provided text by chunking, embedding, and storing in FalkorDB.\n    Returns a unique memory ID for later retrieval.\n\n    Args:\n        text: The text content to remember. Must be non-empty.\n        metadata: Optional metadata to attach (tags, source, date, etc.)\n\n    Returns:\n        Dictionary with:\n            - status: \"success\" or \"error\"\n            - memory_id: UUID string identifying the stored memory\n            - chunks_created: Number of chunks generated\n            - text_preview: First 100 chars of text\n\n    Raises:\n        ValueError: If text is empty or exceeds max length (10MB).\n        DatabaseError: If FalkorDB storage fails.\n\n    Example:\n        >>> result = await add_memory(\n        ...     \"RAG systems benefit from hybrid search\",\n        ...     {\"tags\": [\"rag\", \"research\"], \"source\": \"paper.pdf\"}\n        ... )\n        >>> print(result[\"memory_id\"])\n        '550e8400-e29b-41d4-a716-446655440000'\n    \"\"\"\n    logger.info(\"add_memory_called\", text_length=len(text), has_metadata=bool(metadata))\n\n    try:\n        # Validate request\n        request = AddMemoryRequest(text=text, metadata=metadata or {})\n\n        # Initialize processor (delegates to core logic)\n        processor = DocumentProcessor(\n            embedder=None,  # Uses default from config\n            db=FalkorDBClient(\n                host=settings.falkordb_host,\n                port=settings.falkordb_port,\n                graph_name=settings.graph_name\n            )\n        )\n\n        # Process and store\n        memory_id = await processor.add(request.text, request.metadata)\n\n        # Return success response\n        response = AddMemoryResponse(\n            status=\"success\",\n            memory_id=str(memory_id),\n            chunks_created=len(processor.last_chunks),  # Set by processor\n            text_preview=text[:100]\n        )\n\n        logger.info(\n            \"add_memory_success\",\n            memory_id=memory_id,\n            chunks=response.chunks_created\n        )\n\n        return response.dict()\n\n    except ValueError as e:\n        logger.error(\"add_memory_validation_error\", error=str(e))\n        return {\"status\": \"error\", \"message\": str(e)}\n\n    except Exception as e:\n        logger.exception(\"add_memory_failed\", error=str(e))\n        return {\"status\": \"error\", \"message\": f\"Unexpected error: {e}\"}\n```\n\n**`schemas/requests.py`**: Request validation\n```python\n\"\"\"Request Schemas\n\nPydantic models for validating MCP tool inputs.\n\"\"\"\n\nfrom typing import Dict, Any, Optional, List\nfrom pydantic import BaseModel, Field, validator\n\nclass AddMemoryRequest(BaseModel):\n    \"\"\"Request schema for add_memory tool.\"\"\"\n\n    text: str = Field(\n        ...,\n        min_length=1,\n        max_length=10_000_000,  # 10MB\n        description=\"Text content to remember\"\n    )\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Optional metadata (tags, source, date, etc.)\"\n    )\n\n    @validator(\"text\")\n    def validate_text(cls, v):\n        \"\"\"Ensure text is not just whitespace.\"\"\"\n        if not v.strip():\n            raise ValueError(\"Text cannot be empty or whitespace-only\")\n        return v.strip()\n\nclass SearchMemoryRequest(BaseModel):\n    \"\"\"Request schema for search_memory tool.\"\"\"\n\n    query: str = Field(\n        ...,\n        min_length=1,\n        max_length=1000,\n        description=\"Natural language search query\"\n    )\n    limit: int = Field(\n        default=10,\n        ge=1,\n        le=100,\n        description=\"Maximum number of results to return\"\n    )\n    filters: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"Optional filters (date_from, date_to, tags, source, etc.)\"\n    )\n```\n\n**`config.py`**: Configuration management\n```python\n\"\"\"Configuration Management\n\nCentralized configuration using Pydantic Settings.\nLoads from environment variables with sensible defaults.\n\"\"\"\n\nfrom pydantic_settings import BaseSettings\nfrom pydantic import Field\n\nclass Settings(BaseSettings):\n    \"\"\"Zapomni configuration settings.\"\"\"\n\n    # FalkorDB\n    falkordb_host: str = Field(\n        default=\"localhost\",\n        env=\"FALKORDB_HOST\",\n        description=\"FalkorDB host address\"\n    )\n    falkordb_port: int = Field(\n        default=6379,\n        env=\"FALKORDB_PORT\",\n        description=\"FalkorDB port\"\n    )\n    graph_name: str = Field(\n        default=\"zapomni_memory\",\n        env=\"GRAPH_NAME\",\n        description=\"FalkorDB graph name\"\n    )\n\n    # Ollama\n    ollama_host: str = Field(\n        default=\"http://localhost:11434\",\n        env=\"OLLAMA_HOST\",\n        description=\"Ollama API URL\"\n    )\n    embedding_model: str = Field(\n        default=\"nomic-embed-text\",\n        env=\"EMBEDDING_MODEL\",\n        description=\"Ollama embedding model\"\n    )\n    llm_model: str = Field(\n        default=\"llama3.1:8b\",\n        env=\"LLM_MODEL\",\n        description=\"Ollama LLM model for entity extraction\"\n    )\n\n    # Performance\n    chunk_size: int = Field(\n        default=512,\n        ge=100,\n        le=2000,\n        env=\"CHUNK_SIZE\",\n        description=\"Text chunk size in tokens\"\n    )\n    chunk_overlap: int = Field(\n        default=50,\n        ge=0,\n        le=500,\n        env=\"CHUNK_OVERLAP\",\n        description=\"Chunk overlap in tokens\"\n    )\n\n    # Logging\n    log_level: str = Field(\n        default=\"INFO\",\n        env=\"LOG_LEVEL\",\n        description=\"Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\"\n    )\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n\n# Singleton instance\nsettings = Settings()\n```\n\n---\n\n### Package: zapomni_core\n\n**Purpose**: Core business logic and processing\n\n**Design Pattern**: Service layer with dependency injection\n\n#### Key Modules\n\n**`processors/text_processor.py`**: Document processing\n```python\n\"\"\"Text Document Processor\n\nHandles text document ingestion: chunking, embedding, storage.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport structlog\n\nfrom ..chunking import SemanticChunker\nfrom ..embeddings import OllamaEmbedder\nfrom ...zapomni_db.falkordb import FalkorDBClient\nfrom ...zapomni_db.models import Chunk, Memory\n\nlogger = structlog.get_logger()\n\nclass TextProcessor:\n    \"\"\"Process text documents for storage in memory system.\n\n    This class handles the complete document ingestion pipeline:\n    1. Text chunking (semantic boundaries)\n    2. Embedding generation (via Ollama)\n    3. Metadata extraction\n    4. FalkorDB storage\n\n    Attributes:\n        chunker: SemanticChunker instance for text chunking\n        embedder: OllamaEmbedder instance for embedding generation\n        db: FalkorDBClient instance for storage\n\n    Example:\n        >>> chunker = SemanticChunker(chunk_size=512, overlap=50)\n        >>> embedder = OllamaEmbedder(model=\"nomic-embed-text\")\n        >>> db = FalkorDBClient(host=\"localhost\", port=6379)\n        >>> processor = TextProcessor(chunker, embedder, db)\n        >>> memory_id = await processor.add(\"Sample text\", {})\n    \"\"\"\n\n    def __init__(\n        self,\n        chunker: SemanticChunker,\n        embedder: OllamaEmbedder,\n        db: FalkorDBClient\n    ):\n        self.chunker = chunker\n        self.embedder = embedder\n        self.db = db\n        self.last_chunks: List[Chunk] = []  # For testing/debugging\n\n    async def add(\n        self,\n        text: str,\n        metadata: Dict[str, Any]\n    ) -> str:\n        \"\"\"Add text document to memory.\n\n        Args:\n            text: Text content to process\n            metadata: Metadata dictionary (tags, source, date, etc.)\n\n        Returns:\n            memory_id: UUID string identifying the stored memory\n\n        Raises:\n            ValueError: If text is invalid\n            DatabaseError: If storage fails\n        \"\"\"\n        logger.info(\"text_processor_add\", text_length=len(text))\n\n        # 1. Chunk text\n        chunks = self.chunker.chunk(text)\n        self.last_chunks = chunks\n        logger.debug(\"text_chunked\", chunk_count=len(chunks))\n\n        # 2. Generate embeddings (batch)\n        chunk_texts = [c.text for c in chunks]\n        embeddings = await self.embedder.embed(chunk_texts)\n        logger.debug(\"embeddings_generated\", count=len(embeddings))\n\n        # 3. Create memory object\n        memory = Memory(\n            text=text,\n            chunks=chunks,\n            embeddings=embeddings,\n            metadata=metadata\n        )\n\n        # 4. Store in FalkorDB\n        memory_id = await self.db.add_memory(memory)\n        logger.info(\"memory_stored\", memory_id=memory_id)\n\n        return memory_id\n```\n\n**`embeddings/ollama_embedder.py`**: Embedding generation\n```python\n\"\"\"Ollama Embedding Generator\n\nGenerates embeddings using Ollama's embedding API.\n\"\"\"\n\nfrom typing import List\nimport httpx\nimport structlog\n\nlogger = structlog.get_logger()\n\nclass OllamaEmbedder:\n    \"\"\"Generate embeddings using Ollama.\n\n    Uses Ollama's embedding API to generate vector embeddings for text.\n    Supports any Ollama embedding model (nomic-embed-text recommended).\n\n    Attributes:\n        host: Ollama API URL (e.g., http://localhost:11434)\n        model: Embedding model name (e.g., nomic-embed-text)\n        timeout: API timeout in seconds\n\n    Example:\n        >>> embedder = OllamaEmbedder(\n        ...     host=\"http://localhost:11434\",\n        ...     model=\"nomic-embed-text\"\n        ... )\n        >>> embeddings = await embedder.embed([\"Hello world\", \"Goodbye\"])\n        >>> len(embeddings[0])  # nomic-embed-text is 768-dimensional\n        768\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"http://localhost:11434\",\n        model: str = \"nomic-embed-text\",\n        timeout: int = 60\n    ):\n        self.host = host\n        self.model = model\n        self.timeout = timeout\n        self.client = httpx.AsyncClient(timeout=timeout)\n\n    async def embed(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Generate embeddings for texts.\n\n        Args:\n            texts: List of text strings to embed\n\n        Returns:\n            List of embedding vectors (each is List[float])\n\n        Raises:\n            httpx.HTTPError: If Ollama API request fails\n        \"\"\"\n        embeddings = []\n\n        for text in texts:\n            response = await self.client.post(\n                f\"{self.host}/api/embeddings\",\n                json={\"model\": self.model, \"prompt\": text}\n            )\n            response.raise_for_status()\n\n            data = response.json()\n            embeddings.append(data[\"embedding\"])\n\n        logger.debug(\"ollama_embed\", count=len(embeddings), model=self.model)\n        return embeddings\n\n    async def close(self):\n        \"\"\"Close HTTP client.\"\"\"\n        await self.client.aclose()\n```\n\n**`search/vector_search.py`**: Vector similarity search\n```python\n\"\"\"Vector Similarity Search\n\nImplements vector similarity search using FalkorDB's vector index.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport structlog\n\nfrom ...zapomni_db.falkordb import FalkorDBClient\nfrom ...zapomni_db.models import SearchResult\n\nlogger = structlog.get_logger()\n\nclass VectorSearch:\n    \"\"\"Vector similarity search engine.\n\n    Performs cosine similarity search using FalkorDB's HNSW vector index.\n\n    Attributes:\n        db: FalkorDBClient instance\n        similarity_threshold: Minimum similarity score (0-1)\n\n    Example:\n        >>> search = VectorSearch(db=db_client, similarity_threshold=0.5)\n        >>> query_embedding = [0.1, 0.2, ...]  # 768-dimensional\n        >>> results = await search.search(\n        ...     query_embedding=query_embedding,\n        ...     limit=10,\n        ...     filters={\"tags\": [\"python\"]}\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        db: FalkorDBClient,\n        similarity_threshold: float = 0.5\n    ):\n        self.db = db\n        self.similarity_threshold = similarity_threshold\n\n    async def search(\n        self,\n        query_embedding: List[float],\n        limit: int = 10,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[SearchResult]:\n        \"\"\"Perform vector similarity search.\n\n        Args:\n            query_embedding: Query embedding vector\n            limit: Maximum number of results\n            filters: Optional filters (tags, date_from, date_to, etc.)\n\n        Returns:\n            List of SearchResult objects, sorted by similarity (descending)\n        \"\"\"\n        logger.info(\"vector_search\", limit=limit, has_filters=bool(filters))\n\n        # Delegate to database\n        results = await self.db.vector_search(\n            query_embedding=query_embedding,\n            limit=limit,\n            filters=filters or {},\n            min_similarity=self.similarity_threshold\n        )\n\n        logger.info(\"vector_search_complete\", results=len(results))\n        return results\n```\n\n---\n\n### Package: zapomni_db\n\n**Purpose**: Database client implementations\n\n**Design Pattern**: Repository pattern with abstraction layer\n\n#### Key Modules\n\n**`falkordb/client.py`**: FalkorDB client\n```python\n\"\"\"FalkorDB Client\n\nMain interface to FalkorDB database (unified vector + graph).\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport uuid\nfrom falkordb import FalkorDB\nimport structlog\n\nfrom ..models import Memory, SearchResult, Entity\n\nlogger = structlog.get_logger()\n\nclass FalkorDBClient:\n    \"\"\"FalkorDB client for unified vector + graph storage.\n\n    Provides high-level interface to FalkorDB for storing memories,\n    performing vector search, and querying knowledge graph.\n\n    Attributes:\n        host: FalkorDB host (default: localhost)\n        port: FalkorDB port (default: 6379)\n        graph_name: Graph name (default: zapomni_memory)\n        db: FalkorDB connection instance\n        graph: Graph instance\n\n    Example:\n        >>> client = FalkorDBClient(\n        ...     host=\"localhost\",\n        ...     port=6379,\n        ...     graph_name=\"zapomni_memory\"\n        ... )\n        >>> await client.add_memory(memory)\n        '550e8400-e29b-41d4-a716-446655440000'\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        graph_name: str = \"zapomni_memory\"\n    ):\n        self.host = host\n        self.port = port\n        self.graph_name = graph_name\n\n        # Connect to FalkorDB\n        self.db = FalkorDB(host=host, port=port)\n        self.graph = self.db.select_graph(graph_name)\n\n        # Initialize schema (idempotent)\n        self._init_schema()\n\n        logger.info(\n            \"falkordb_connected\",\n            host=host,\n            port=port,\n            graph=graph_name\n        )\n\n    def _init_schema(self):\n        \"\"\"Initialize graph schema (nodes, edges, indexes).\"\"\"\n        # Create vector index for embeddings\n        self.graph.query(\"\"\"\n            CREATE VECTOR INDEX FOR (c:Chunk) ON (c.embedding)\n            OPTIONS {dimension: 768, similarityFunction: 'cosine'}\n        \"\"\")\n\n        # Create property indexes\n        self.graph.query(\"CREATE INDEX FOR (m:Memory) ON (m.id)\")\n        self.graph.query(\"CREATE INDEX FOR (e:Entity) ON (e.name)\")\n        self.graph.query(\"CREATE INDEX FOR (d:Document) ON (d.id)\")\n\n        logger.debug(\"falkordb_schema_initialized\")\n\n    async def add_memory(self, memory: Memory) -> str:\n        \"\"\"Store memory with chunks and embeddings.\n\n        Args:\n            memory: Memory object with text, chunks, embeddings, metadata\n\n        Returns:\n            memory_id: UUID string\n        \"\"\"\n        memory_id = str(uuid.uuid4())\n\n        # Create Memory node\n        self.graph.query(\n            \"\"\"\n            CREATE (m:Memory {\n                id: $id,\n                text: $text,\n                tags: $tags,\n                source: $source,\n                timestamp: datetime()\n            })\n            \"\"\",\n            {\n                \"id\": memory_id,\n                \"text\": memory.text,\n                \"tags\": memory.metadata.get(\"tags\", []),\n                \"source\": memory.metadata.get(\"source\", \"\"),\n            }\n        )\n\n        # Create Chunk nodes with embeddings\n        for i, (chunk, embedding) in enumerate(zip(memory.chunks, memory.embeddings)):\n            chunk_id = f\"{memory_id}_chunk_{i}\"\n\n            self.graph.query(\n                \"\"\"\n                MATCH (m:Memory {id: $memory_id})\n                CREATE (c:Chunk {\n                    id: $chunk_id,\n                    text: $text,\n                    index: $index,\n                    embedding: $embedding\n                })\n                CREATE (m)-[:HAS_CHUNK]->(c)\n                \"\"\",\n                {\n                    \"memory_id\": memory_id,\n                    \"chunk_id\": chunk_id,\n                    \"text\": chunk.text,\n                    \"index\": i,\n                    \"embedding\": embedding\n                }\n            )\n\n        logger.info(\n            \"memory_added\",\n            memory_id=memory_id,\n            chunks=len(memory.chunks)\n        )\n\n        return memory_id\n\n    async def vector_search(\n        self,\n        query_embedding: List[float],\n        limit: int = 10,\n        filters: Dict[str, Any] = None,\n        min_similarity: float = 0.5\n    ) -> List[SearchResult]:\n        \"\"\"Perform vector similarity search.\n\n        Args:\n            query_embedding: Query vector (768-dimensional)\n            limit: Max results\n            filters: Optional metadata filters\n            min_similarity: Minimum similarity threshold (0-1)\n\n        Returns:\n            List of SearchResult objects\n        \"\"\"\n        # Build Cypher query with filters\n        where_clause = \"\"\n        if filters:\n            # TODO: Build WHERE clause from filters\n            pass\n\n        # Vector search query\n        query = f\"\"\"\n            CALL db.idx.vector.queryNodes(\n                'Chunk',\n                'embedding',\n                {limit},\n                $query_embedding\n            ) YIELD node, score\n            WHERE score >= $min_similarity\n            MATCH (m:Memory)-[:HAS_CHUNK]->(node)\n            RETURN m.id as memory_id,\n                   node.text as text,\n                   score as similarity,\n                   m.tags as tags,\n                   m.source as source,\n                   m.timestamp as timestamp\n            ORDER BY score DESC\n        \"\"\"\n\n        result = self.graph.query(query, {\n            \"query_embedding\": query_embedding,\n            \"min_similarity\": min_similarity\n        })\n\n        # Convert to SearchResult objects\n        results = [\n            SearchResult(\n                memory_id=row[0],\n                text=row[1],\n                similarity_score=row[2],\n                tags=row[3],\n                source=row[4],\n                timestamp=row[5]\n            )\n            for row in result.result_set\n        ]\n\n        logger.debug(\"vector_search_complete\", results=len(results))\n        return results\n```\n\n**`models.py`**: Shared data models\n```python\n\"\"\"Shared Data Models\n\nPydantic models used across packages.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\n\nclass Chunk(BaseModel):\n    \"\"\"Text chunk model.\"\"\"\n    text: str\n    index: int\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\nclass Memory(BaseModel):\n    \"\"\"Memory model (document + chunks + embeddings).\"\"\"\n    text: str\n    chunks: List[Chunk]\n    embeddings: List[List[float]]\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\nclass SearchResult(BaseModel):\n    \"\"\"Search result model.\"\"\"\n    memory_id: str\n    text: str\n    similarity_score: float\n    tags: List[str] = Field(default_factory=list)\n    source: str = \"\"\n    timestamp: Optional[datetime] = None\n\nclass Entity(BaseModel):\n    \"\"\"Knowledge graph entity model.\"\"\"\n    name: str\n    type: str  # PERSON, ORG, TECHNOLOGY, etc.\n    description: str = \"\"\n    confidence: float = 1.0\n```\n\n---\n\n## Coding Conventions\n\n### Python Style Guide\n\n**Standard**: PEP 8 + Black + Type Hints\n\n**Tools**:\n- **Black**: Code formatting (line length 100)\n- **isort**: Import sorting\n- **mypy**: Type checking (strict mode)\n- **flake8**: Linting\n- **pylint**: Additional linting (optional)\n\n**Configuration**: See `pyproject.toml` section below\n\n### File Naming\n\n**Modules**: `snake_case.py`\n\n✅ **Good**:\n```\ntext_processor.py\nollama_embedder.py\nvector_search.py\n```\n\n❌ **Bad**:\n```\nTextProcessor.py\nOllamaEmbedder.py\nvectorSearch.py\n```\n\n**Classes**: `PascalCase`\n\n✅ **Good**:\n```python\nclass DocumentProcessor:\nclass OllamaEmbedder:\nclass FalkorDBClient:\n```\n\n❌ **Bad**:\n```python\nclass document_processor:\nclass ollama_embedder:\nclass falkordb_client:\n```\n\n**Functions/Variables**: `snake_case`\n\n✅ **Good**:\n```python\ndef add_memory(text: str) -> str:\n    memory_id = uuid.uuid4()\n    chunk_size = 512\n```\n\n❌ **Bad**:\n```python\ndef AddMemory(text: str) -> str:\n    memoryID = uuid.uuid4()\n    chunkSize = 512\n```\n\n**Constants**: `UPPER_SNAKE_CASE`\n\n✅ **Good**:\n```python\nMAX_CHUNK_SIZE = 512\nDEFAULT_EMBEDDING_MODEL = \"nomic-embed-text\"\nVECTOR_DIMENSIONS = 768\n```\n\n❌ **Bad**:\n```python\nmax_chunk_size = 512\ndefaultEmbeddingModel = \"nomic-embed-text\"\n```\n\n### Type Hints\n\n**Requirement**: 100% coverage for public APIs\n\n**Examples**:\n\n```python\nfrom typing import List, Dict, Optional, Any, Union\n\n# Function signatures\ndef chunk_text(\n    text: str,\n    chunk_size: int = 512,\n    overlap: int = 50\n) -> List[str]:\n    \"\"\"Chunk text into smaller pieces.\"\"\"\n    ...\n\n# Complex types\nfrom pydantic import BaseModel\n\nclass Memory(BaseModel):\n    id: str\n    text: str\n    embedding: List[float]\n    metadata: Optional[Dict[str, Any]] = None\n    tags: List[str] = []\n\n# Async functions\nasync def embed(self, texts: List[str]) -> List[List[float]]:\n    \"\"\"Generate embeddings asynchronously.\"\"\"\n    ...\n\n# Generic types\nfrom typing import TypeVar, Generic\n\nT = TypeVar('T')\n\nclass Cache(Generic[T]):\n    def get(self, key: str) -> Optional[T]:\n        ...\n```\n\n**mypy Configuration** (`pyproject.toml`):\n```toml\n[tool.mypy]\npython_version = \"3.10\"\nstrict = true\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_any_unimported = false\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\ncheck_untyped_defs = true\n```\n\n### Docstrings\n\n**Standard**: Google Style\n\n**Required For**:\n- All public functions\n- All classes\n- All modules (module-level docstring)\n\n**Function Docstring Example**:\n```python\ndef add_memory(text: str, metadata: Optional[Dict[str, Any]] = None) -> str:\n    \"\"\"Store information in memory system.\n\n    Processes the provided text by chunking, embedding, and storing in FalkorDB.\n    Returns a unique memory ID for later retrieval.\n\n    Args:\n        text: The text content to remember. Must be non-empty.\n        metadata: Optional metadata to attach (tags, source, date, etc.)\n\n    Returns:\n        A UUID string identifying the stored memory.\n\n    Raises:\n        ValueError: If text is empty or exceeds max length (10MB).\n        DatabaseError: If FalkorDB storage fails.\n\n    Example:\n        >>> memory_id = add_memory(\"Important information\", {\"source\": \"notes\"})\n        >>> print(memory_id)\n        '550e8400-e29b-41d4-a716-446655440000'\n    \"\"\"\n    ...\n```\n\n**Class Docstring Example**:\n```python\nclass TextProcessor:\n    \"\"\"Processes text documents for storage in memory system.\n\n    This class handles the complete document ingestion pipeline:\n    1. Text chunking (semantic boundaries)\n    2. Embedding generation (via Ollama)\n    3. Metadata extraction\n    4. FalkorDB storage\n\n    Attributes:\n        chunker: SemanticChunker instance for text chunking\n        embedder: OllamaEmbedder instance for embedding generation\n        db: FalkorDBClient instance for storage\n\n    Example:\n        >>> processor = TextProcessor(chunker, embedder, db)\n        >>> memory_id = await processor.add(\"Sample text\", {})\n    \"\"\"\n    ...\n```\n\n**Module Docstring Example**:\n```python\n\"\"\"Text Document Processor\n\nThis module provides the TextProcessor class for handling text document\ningestion into the Zapomni memory system.\n\nUsage:\n    from zapomni_core.processors import TextProcessor\n\n    processor = TextProcessor(chunker, embedder, db)\n    memory_id = await processor.add(text, metadata)\n\"\"\"\n```\n\n### Import Order\n\n**Standard**: isort with Black compatibility\n\n**Order**:\n1. Standard library\n2. Third-party packages\n3. Local application imports\n\n**Example**:\n```python\n# Standard library\nimport os\nimport sys\nfrom typing import List, Dict, Optional\n\n# Third-party\nimport numpy as np\nfrom pydantic import BaseModel\nfrom mcp.server import Server\n\n# Local\nfrom zapomni_core.processors import DocumentProcessor\nfrom zapomni_db.falkordb import FalkorDBClient\nfrom .schemas import AddMemoryRequest\n```\n\n**isort Configuration** (`pyproject.toml`):\n```toml\n[tool.isort]\nprofile = \"black\"\nline_length = 100\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\n```\n\n### Error Handling\n\n**Pattern**: Explicit exception handling with context\n\n**Good** ✅:\n```python\nimport structlog\n\nlogger = structlog.get_logger()\n\ntry:\n    embedding = await embedder.embed(text)\nexcept OllamaConnectionError as e:\n    logger.error(\"ollama_connection_failed\", error=str(e), text_length=len(text))\n    raise EmbeddingError(f\"Failed to generate embedding: {e}\") from e\nexcept OllamaTimeoutError as e:\n    logger.warn(\"ollama_timeout_retrying_with_fallback\", error=str(e))\n    embedding = await fallback_embedder.embed(text)\n```\n\n**Bad** ❌:\n```python\ntry:\n    embedding = await embedder.embed(text)\nexcept Exception as e:  # Too broad\n    print(f\"Error: {e}\")  # Don't use print\n    pass  # Don't silently fail\n```\n\n**Custom Exceptions**:\n```python\n# zapomni_core/exceptions.py\n\nclass ZapomniError(Exception):\n    \"\"\"Base exception for Zapomni.\"\"\"\n    pass\n\nclass EmbeddingError(ZapomniError):\n    \"\"\"Embedding generation failed.\"\"\"\n    pass\n\nclass DatabaseError(ZapomniError):\n    \"\"\"Database operation failed.\"\"\"\n    pass\n\nclass ValidationError(ZapomniError):\n    \"\"\"Input validation failed.\"\"\"\n    pass\n```\n\n### Logging\n\n**Standard**: Structured logging with structlog\n\n**Configuration**:\n```python\n# zapomni_mcp/logging.py\n\nimport sys\nimport structlog\n\ndef setup_logging(level: str = \"INFO\"):\n    \"\"\"Setup structured logging to stderr.\"\"\"\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer()\n        ],\n        logger_factory=structlog.PrintLoggerFactory(file=sys.stderr),\n        wrapper_class=structlog.BoundLogger,\n        context_class=dict,\n        cache_logger_on_first_use=True,\n    )\n```\n\n**Usage**:\n```python\nimport structlog\n\nlogger = structlog.get_logger()\n\n# Good: Structured ✅\nlogger.info(\n    \"memory_added\",\n    memory_id=memory_id,\n    chunk_count=len(chunks),\n    duration_ms=duration\n)\n\n# Bad: Unstructured ❌\nlogger.info(f\"Added memory {memory_id} with {len(chunks)} chunks in {duration}ms\")\n```\n\n**Log Levels**:\n- **DEBUG**: Detailed debugging (off in production)\n- **INFO**: Normal operations (default)\n- **WARNING**: Degraded performance, fallbacks used\n- **ERROR**: Operation failed, needs attention\n- **CRITICAL**: System failure, immediate action required\n\n**Why stderr?**: MCP protocol uses stdout for communication, so all logging must go to stderr.\n\n### Async/Await\n\n**Pattern**: Use async I/O for all database and network calls\n\n**Good** ✅:\n```python\nasync def add_memory(text: str) -> str:\n    embedding = await embedder.embed(text)  # Network I/O\n    memory_id = await db.store(embedding)  # Database I/O\n    return memory_id\n```\n\n**Bad** ❌:\n```python\ndef add_memory(text: str) -> str:\n    embedding = embedder.embed(text)  # Blocking\n    memory_id = db.store(embedding)  # Blocking\n    return memory_id\n```\n\n**Running Async**:\n```python\n# In MCP server (already async context)\nmemory_id = await add_memory(text)\n\n# In scripts (create async context)\nimport asyncio\nmemory_id = asyncio.run(add_memory(text))\n\n# In tests (pytest-asyncio)\n@pytest.mark.asyncio\nasync def test_add_memory():\n    memory_id = await add_memory(\"test\")\n    assert memory_id is not None\n```\n\n---\n\n## Configuration Management\n\n### Environment Variables\n\n**File**: `.env` (local, gitignored) + `.env.example` (template, committed)\n\n**Template** (`.env.example`):\n```bash\n# FalkorDB Configuration\nFALKORDB_HOST=localhost\nFALKORDB_PORT=6379\nGRAPH_NAME=zapomni_memory\n\n# Ollama Configuration\nOLLAMA_HOST=http://localhost:11434\nEMBEDDING_MODEL=nomic-embed-text\nLLM_MODEL=llama3.1:8b\n\n# Redis Cache (Phase 2)\nREDIS_HOST=localhost\nREDIS_PORT=6380\nREDIS_TTL_SECONDS=86400\n\n# Performance Tuning\nCHUNK_SIZE=512\nCHUNK_OVERLAP=50\nEMBEDDING_BATCH_SIZE=32\nSEARCH_LIMIT_DEFAULT=10\n\n# Logging\nLOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL\n\n# Features (Phase-gated)\nENABLE_HYBRID_SEARCH=false  # Phase 2\nENABLE_KNOWLEDGE_GRAPH=false  # Phase 2\nENABLE_CODE_INDEXING=false  # Phase 3\n```\n\n### Pydantic Settings\n\n**Implementation** (`zapomni_mcp/config.py` - already shown above)\n\n**Usage**:\n```python\nfrom zapomni_mcp.config import settings\n\n# Use in code\nclient = FalkorDBClient(\n    host=settings.falkordb_host,\n    port=settings.falkordb_port,\n    graph_name=settings.graph_name\n)\n\nembedder = OllamaEmbedder(\n    host=settings.ollama_host,\n    model=settings.embedding_model\n)\n\nchunker = SemanticChunker(\n    chunk_size=settings.chunk_size,\n    overlap=settings.chunk_overlap\n)\n```\n\n### Docker Compose Configuration\n\n**File**: `docker/docker-compose.yml`\n\n```yaml\nversion: '3.8'\n\nservices:\n  falkordb:\n    image: falkordb/falkordb:latest\n    container_name: zapomni-falkordb\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - falkordb_data:/data\n    environment:\n      - FALKORDB_ARGS=--save 60 1\n    restart: unless-stopped\n\n  redis-cache:\n    image: redis:7-alpine\n    container_name: zapomni-redis-cache\n    ports:\n      - \"6380:6379\"\n    volumes:\n      - redis_data:/data\n    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru\n    restart: unless-stopped\n\n  # Ollama (future: if containerized)\n  # ollama:\n  #   image: ollama/ollama:latest\n  #   container_name: zapomni-ollama\n  #   ports:\n  #     - \"11434:11434\"\n  #   volumes:\n  #     - ollama_data:/root/.ollama\n  #   restart: unless-stopped\n\nvolumes:\n  falkordb_data:\n  redis_data:\n  # ollama_data:\n```\n\n**Usage**:\n```bash\n# Start all services\ndocker-compose -f docker/docker-compose.yml up -d\n\n# View logs\ndocker-compose -f docker/docker-compose.yml logs -f\n\n# Stop services\ndocker-compose -f docker/docker-compose.yml down\n\n# Clean up (WARNING: deletes data)\ndocker-compose -f docker/docker-compose.yml down -v\n```\n\n---\n\n## Testing Organization\n\n### Test Structure\n\n**Pyramid**: 70% unit, 25% integration, 5% E2E\n\n**Directory Mapping**:\n```\nsrc/zapomni_core/processors/text_processor.py\n→ tests/unit/test_text_processor.py\n\nsrc/zapomni_db/falkordb/client.py\n→ tests/integration/test_falkordb_client.py\n\nEnd-to-end workflow\n→ tests/e2e/test_full_workflow.py\n```\n\n### Unit Tests (70%)\n\n**Characteristics**:\n- Fast (< 1ms per test)\n- No external dependencies (mock everything)\n- Test single function/class\n- High coverage (90%+)\n\n**Example**:\n```python\n# tests/unit/test_chunker.py\n\nimport pytest\nfrom zapomni_core.chunking import SemanticChunker\n\ndef test_chunk_text_basic():\n    \"\"\"Test basic text chunking.\"\"\"\n    chunker = SemanticChunker(chunk_size=100, overlap=10)\n    text = \"A\" * 250\n\n    chunks = chunker.chunk(text)\n\n    assert len(chunks) == 3  # 250 chars / 100 per chunk\n    assert len(chunks[0].text) == 100\n    assert chunks[0].index == 0\n\ndef test_chunk_text_overlap():\n    \"\"\"Test chunking with overlap.\"\"\"\n    chunker = SemanticChunker(chunk_size=100, overlap=20)\n    text = \"A\" * 100 + \"B\" * 100\n\n    chunks = chunker.chunk(text)\n\n    # Verify overlap\n    assert \"A\" in chunks[1].text  # Overlap from first chunk\n    assert \"B\" in chunks[1].text\n\ndef test_chunk_text_empty():\n    \"\"\"Test chunking empty text raises ValueError.\"\"\"\n    chunker = SemanticChunker(chunk_size=100, overlap=10)\n\n    with pytest.raises(ValueError, match=\"empty\"):\n        chunker.chunk(\"\")\n\n@pytest.mark.parametrize(\"chunk_size,expected_chunks\", [\n    (100, 3),\n    (200, 2),\n    (500, 1),\n])\ndef test_chunk_text_sizes(chunk_size, expected_chunks):\n    \"\"\"Test different chunk sizes.\"\"\"\n    chunker = SemanticChunker(chunk_size=chunk_size, overlap=0)\n    text = \"A\" * 250\n\n    chunks = chunker.chunk(text)\n\n    assert len(chunks) == expected_chunks\n```\n\n### Integration Tests (25%)\n\n**Characteristics**:\n- Slower (< 100ms per test)\n- Use real services (FalkorDB, Ollama via Docker)\n- Test interactions between components\n- Medium coverage (70%+)\n\n**Example**:\n```python\n# tests/integration/test_falkordb_client.py\n\nimport pytest\nfrom zapomni_db.falkordb import FalkorDBClient\nfrom zapomni_db.models import Memory, Chunk\n\n@pytest.fixture\ndef db_client():\n    \"\"\"FalkorDB test instance (Docker).\"\"\"\n    client = FalkorDBClient(\n        host=\"localhost\",\n        port=6379,\n        graph_name=\"test_graph\"\n    )\n    yield client\n    # Cleanup\n    client.graph.query(\"MATCH (n) DETACH DELETE n\")\n\n@pytest.mark.asyncio\nasync def test_add_and_search_memory(db_client):\n    \"\"\"Test full add → search workflow.\"\"\"\n    # Add memory\n    embedding = [0.1] * 768\n    memory = Memory(\n        text=\"Test memory\",\n        chunks=[Chunk(text=\"Test chunk\", index=0)],\n        embeddings=[embedding],\n        metadata={\"source\": \"test\"}\n    )\n\n    memory_id = await db_client.add_memory(memory)\n\n    assert memory_id is not None\n\n    # Search\n    results = await db_client.vector_search(\n        query_embedding=embedding,\n        limit=10\n    )\n\n    assert len(results) == 1\n    assert results[0].text == \"Test chunk\"\n    assert results[0].memory_id == memory_id\n\n@pytest.mark.asyncio\nasync def test_vector_search_with_filters(db_client):\n    \"\"\"Test vector search with metadata filters.\"\"\"\n    # Add multiple memories with different tags\n    for tag in [\"python\", \"javascript\", \"rust\"]:\n        embedding = [0.1 * ord(tag[0])] * 768  # Different embeddings\n        memory = Memory(\n            text=f\"Memory about {tag}\",\n            chunks=[Chunk(text=f\"{tag} content\", index=0)],\n            embeddings=[embedding],\n            metadata={\"tags\": [tag]}\n        )\n        await db_client.add_memory(memory)\n\n    # Search with filter\n    results = await db_client.vector_search(\n        query_embedding=[0.1 * ord('p')] * 768,\n        limit=10,\n        filters={\"tags\": [\"python\"]}\n    )\n\n    assert len(results) == 1\n    assert \"python\" in results[0].tags\n```\n\n### E2E Tests (5%)\n\n**Characteristics**:\n- Slowest (< 5s per test)\n- Full workflow from MCP call to result\n- All services running\n- Low coverage (critical paths only)\n\n**Example**:\n```python\n# tests/e2e/test_full_workflow.py\n\nimport pytest\nimport asyncio\nfrom zapomni_mcp.server import create_server\nfrom mcp.client import Client\n\n@pytest.mark.e2e\n@pytest.mark.asyncio\nasync def test_add_and_search_workflow():\n    \"\"\"Test full MCP workflow: add → search.\"\"\"\n    # Setup MCP server and client\n    server = create_server()\n    # (In real test, would use stdio pipes)\n\n    # Add memory via MCP\n    add_response = await server.call_tool(\"add_memory\", {\n        \"text\": \"Paris is the capital of France\",\n        \"metadata\": {\"source\": \"geography\"}\n    })\n\n    assert add_response[\"status\"] == \"success\"\n    memory_id = add_response[\"memory_id\"]\n\n    # Wait for processing\n    await asyncio.sleep(0.5)\n\n    # Search for it\n    search_response = await server.call_tool(\"search_memory\", {\n        \"query\": \"What is the capital of France?\",\n        \"limit\": 5\n    })\n\n    assert search_response[\"status\"] == \"success\"\n    assert len(search_response[\"results\"]) >= 1\n    assert any(r[\"memory_id\"] == memory_id for r in search_response[\"results\"])\n\n@pytest.mark.e2e\n@pytest.mark.asyncio\nasync def test_stats_workflow():\n    \"\"\"Test get_stats returns valid statistics.\"\"\"\n    server = create_server()\n\n    # Add some memories first\n    for i in range(5):\n        await server.call_tool(\"add_memory\", {\n            \"text\": f\"Test memory {i}\",\n            \"metadata\": {}\n        })\n\n    # Get stats\n    stats_response = await server.call_tool(\"get_stats\", {})\n\n    assert stats_response[\"status\"] == \"success\"\n    assert stats_response[\"statistics\"][\"total_memories\"] >= 5\n    assert \"database_size_mb\" in stats_response[\"statistics\"]\n```\n\n### Test Fixtures\n\n**Shared Fixtures** (`tests/conftest.py`):\n```python\nimport pytest\nimport asyncio\nfrom zapomni_db.falkordb import FalkorDBClient\nfrom zapomni_core.embeddings import OllamaEmbedder\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for async tests.\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"session\")\ndef docker_services():\n    \"\"\"Start Docker services for tests.\"\"\"\n    # Use pytest-docker or manual docker-compose\n    import subprocess\n    subprocess.run([\"docker-compose\", \"-f\", \"docker/docker-compose.yml\", \"up\", \"-d\"])\n    yield\n    subprocess.run([\"docker-compose\", \"-f\", \"docker/docker-compose.yml\", \"down\"])\n\n@pytest.fixture\nasync def db_client(docker_services):\n    \"\"\"FalkorDB client fixture.\"\"\"\n    client = FalkorDBClient(\n        host=\"localhost\",\n        port=6379,\n        graph_name=\"test_graph\"\n    )\n    yield client\n    # Cleanup\n    client.graph.query(\"MATCH (n) DETACH DELETE n\")\n\n@pytest.fixture\nasync def embedder(docker_services):\n    \"\"\"Ollama embedder fixture.\"\"\"\n    embedder = OllamaEmbedder(\n        host=\"http://localhost:11434\",\n        model=\"nomic-embed-text\"\n    )\n    yield embedder\n    await embedder.close()\n\n@pytest.fixture\ndef sample_text():\n    \"\"\"Sample text for testing.\"\"\"\n    return \"\"\"\n    The Python programming language was created by Guido van Rossum in 1991.\n    It is known for its simple, readable syntax and powerful standard library.\n    Python is widely used in web development, data science, and machine learning.\n    \"\"\"\n```\n\n### Running Tests\n\n**Commands**:\n```bash\n# All tests\npytest\n\n# Unit tests only (fast)\npytest tests/unit\n\n# Integration tests (requires Docker)\npytest tests/integration\n\n# E2E tests (slow, requires full stack)\npytest tests/e2e\npytest -m e2e\n\n# With coverage\npytest --cov=src --cov-report=html\n\n# Specific test\npytest tests/unit/test_chunker.py::test_chunk_text_basic\n\n# Parallel execution (requires pytest-xdist)\npytest -n auto\n\n# Verbose output\npytest -v\n\n# Stop on first failure\npytest -x\n\n# Run only failed tests from last run\npytest --lf\n```\n\n**Configuration** (`pyproject.toml`):\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\nmarkers = [\n    \"unit: Unit tests (fast, no external dependencies)\",\n    \"integration: Integration tests (medium speed, uses services)\",\n    \"e2e: End-to-end tests (slow, full stack)\",\n]\naddopts = [\n    \"--strict-markers\",\n    \"--tb=short\",\n    \"--cov-report=term-missing\",\n]\nasyncio_mode = \"auto\"\n```\n\n---\n\n## Documentation Structure\n\n### User Documentation (`docs/`)\n\n**README.md**: Quick start, installation, basic usage\n\n```markdown\n# Zapomni - Local-First AI Memory\n\nZapomni is a local-first MCP memory server that gives AI agents intelligent,\ncontextual, and private long-term memory.\n\n## Quick Start\n\n### 1. Install Services\n\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull models\nollama pull nomic-embed-text\nollama pull llama3.1:8b\n\n# Start FalkorDB (Docker)\ndocker run -d -p 6379:6379 falkordb/falkordb\n```\n\n### 2. Install Zapomni\n\n```bash\npip install zapomni-mcp\n```\n\n### 3. Configure Claude\n\nAdd to `~/.config/claude/config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"zapomni\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"zapomni_mcp.server\"],\n      \"env\": {\n        \"FALKORDB_HOST\": \"localhost\",\n        \"OLLAMA_HOST\": \"http://localhost:11434\"\n      }\n    }\n  }\n}\n```\n\n### 4. Use\n\n```\nUser: Remember that Python was created by Guido van Rossum in 1991\nClaude: [Calls add_memory tool]\n  ✓ Memory stored with ID: 550e8400-...\n\nUser: Who created Python?\nClaude: [Calls search_memory tool]\n  Python was created by Guido van Rossum in 1991.\n```\n\n## Learn More\n\n- [Installation Guide](guides/installation.md)\n- [Configuration](guides/configuration.md)\n- [API Reference](api/tools.md)\n- [Architecture](architecture/overview.md)\n```\n\n**guides/**: How-to guides for common tasks\n- `quick_start.md`: 5-minute setup\n- `installation.md`: Detailed installation steps\n- `configuration.md`: Configuration options\n- `contributing.md`: Contribution guidelines\n\n**api/**: API reference for MCP tools\n- `tools.md`: All MCP tools with parameters, examples\n- `python_api.md`: Python API (if exposed for non-MCP use)\n\n### Developer Documentation\n\n**architecture/**: System design docs\n- `overview.md`: High-level architecture\n- `mcp_protocol.md`: MCP integration details\n- `data_flow.md`: Data flow diagrams\n\n**benchmarks/**: Performance benchmarks\n- `results.md`: Benchmark results, charts\n\n### Code Documentation\n\n**Docstrings**: Google style in code (already covered)\n**Type hints**: Self-documenting types (already covered)\n\n### Keeping Docs Updated\n\n**When to Update**:\n- **New feature**: Update `guides/`, `api/`\n- **Architecture change**: Update `architecture/`\n- **Performance change**: Update `benchmarks/`\n- **API change**: Update `api/`\n\n**Doc Review**: Part of PR checklist (see below)\n\n---\n\n## Development Workflow\n\n### Initial Setup (New Developer)\n\n**Prerequisites**:\n```bash\n# 1. Python 3.10+\npython --version  # Should be 3.10 or newer\n\n# 2. Docker Desktop\ndocker --version\n\n# 3. Git\ngit --version\n```\n\n**Setup Steps**:\n```bash\n# 1. Clone repository\ngit clone https://github.com/your-org/zapomni.git\ncd zapomni\n\n# 2. Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# OR\n.venv\\Scripts\\activate  # Windows\n\n# 3. Install dependencies\npip install -e \".[dev]\"  # Editable install with dev dependencies\n\n# 4. Install pre-commit hooks\npre-commit install\n\n# 5. Copy environment template\ncp .env.example .env\n# Edit .env with your settings (defaults work for local dev)\n\n# 6. Start services (FalkorDB, Redis)\n./scripts/dev.sh\n\n# 7. Pull Ollama models\nollama pull nomic-embed-text\nollama pull llama3.1:8b\n\n# 8. Run tests to verify setup\npytest tests/unit  # Should pass\n```\n\n**Expected Time**: < 30 minutes\n\n### Daily Development Workflow\n\n**1. Start Services**:\n```bash\n./scripts/dev.sh  # Starts Docker services (FalkorDB, Redis)\n```\n\n**2. Make Changes**:\n```bash\n# Edit code in src/\n# Edit tests in tests/\n```\n\n**3. Run Tests** (continuous):\n```bash\n# Unit tests (fast feedback loop)\npytest tests/unit -v\n\n# Specific module\npytest tests/unit/test_chunker.py -v\n\n# Watch mode (re-run on file change)\npytest-watch tests/unit\n```\n\n**4. Pre-Commit Checks** (automatic):\n```bash\n# Triggers on git commit\n# Runs: black, isort, mypy, flake8\ngit add .\ngit commit -m \"feat: add semantic chunking\"\n# Pre-commit hooks run automatically\n```\n\n**5. Manual Checks** (before PR):\n```bash\n# Full test suite\npytest\n\n# Type checking\nmypy src/\n\n# Code coverage\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n\n# Linting\nflake8 src/ tests/\n```\n\n### Git Workflow\n\n**Branching Strategy**:\n- `main`: Stable, production-ready\n- `feature/*`: New features (e.g., `feature/hybrid-search`)\n- `fix/*`: Bug fixes (e.g., `fix/embedding-timeout`)\n- `docs/*`: Documentation updates\n\n**Commit Messages**: Conventional Commits\n\n**Format**:\n```\ntype(scope): description\n\n[optional body]\n\n[optional footer]\n```\n\n**Types**:\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation\n- `refactor`: Code refactoring\n- `test`: Adding tests\n- `chore`: Tooling, dependencies\n\n**Examples**:\n```bash\nfeat(mcp): add search_memory tool\nfix(embedder): handle Ollama timeout gracefully\ndocs(api): update MCP tools reference\nrefactor(search): extract RRF fusion to separate function\ntest(chunker): add edge case tests for empty input\nchore(deps): upgrade FalkorDB client to 4.1.0\n```\n\n**Good Commit Messages** ✅:\n```\nfeat(search): implement hybrid BM25 + vector search\n\n- Add BM25 keyword search using rank-bm25 library\n- Implement Reciprocal Rank Fusion (RRF) for result merging\n- Add cross-encoder reranking for top-K refinement\n- Achieves 3.4x better accuracy vs vector-only (benchmark)\n\nCloses #42\n```\n\n**Bad Commit Messages** ❌:\n```\nfixed stuff\nupdated files\nwip\nasdf\n```\n\n### Pull Request Process\n\n**Steps**:\n1. Create feature branch: `git checkout -b feature/my-feature`\n2. Make changes + write tests\n3. Run pre-commit: `pre-commit run --all-files`\n4. Run full test suite: `pytest`\n5. Push branch: `git push origin feature/my-feature`\n6. Create PR on GitHub\n7. Request review\n8. Address feedback\n9. Merge after approval\n\n**PR Checklist**:\n- [ ] Tests added/updated (unit + integration)\n- [ ] Documentation updated (API, guides, architecture)\n- [ ] Type hints added to new functions\n- [ ] Docstrings added (Google style)\n- [ ] Pre-commit hooks passing\n- [ ] All tests passing (unit, integration, e2e)\n- [ ] No decrease in code coverage\n- [ ] CHANGELOG.md updated (if user-facing change)\n\n**PR Template** (`.github/pull_request_template.md`):\n```markdown\n## Description\n\nBrief description of what this PR does.\n\n## Type of Change\n\n- [ ] Bug fix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n- [ ] Documentation update\n\n## Testing\n\nHow was this tested?\n\n- [ ] Unit tests\n- [ ] Integration tests\n- [ ] E2E tests\n- [ ] Manual testing\n\n## Checklist\n\n- [ ] Tests pass locally (`pytest`)\n- [ ] Pre-commit hooks pass (`pre-commit run --all-files`)\n- [ ] Documentation updated\n- [ ] Type hints added\n- [ ] Docstrings added\n- [ ] CHANGELOG.md updated (if needed)\n\n## Related Issues\n\nCloses #123\n```\n\n### Release Process (Future)\n\n**Versioning**: Semantic Versioning (MAJOR.MINOR.PATCH)\n\n**Steps**:\n1. Update version in `pyproject.toml`\n2. Update `CHANGELOG.md` with release notes\n3. Create git tag: `git tag v0.1.0`\n4. Push tag: `git push origin v0.1.0`\n5. GitHub Actions builds and publishes to PyPI (automated)\n\n**CHANGELOG.md Format**:\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/),\nand this project adheres to [Semantic Versioning](https://semver.org/).\n\n## [0.2.0] - 2025-12-01\n\n### Added\n- Hybrid search (BM25 + vector + reranking)\n- Semantic caching with 68% hit rate\n- Knowledge graph construction (entities + relationships)\n\n### Changed\n- Improved chunking algorithm (semantic boundaries)\n- Upgraded FalkorDB to 4.1.0\n\n### Fixed\n- Ollama timeout handling\n- Memory leak in embedding cache\n\n## [0.1.0] - 2025-11-22\n\n### Added\n- Initial MVP release\n- add_memory, search_memory, get_stats tools\n- FalkorDB integration\n- Ollama embedding support\n```\n\n---\n\n## Code Review Guidelines\n\n### What Reviewers Check\n\n**Functionality**:\n- Does it work as intended?\n- Are edge cases handled?\n- Are there tests?\n- Do tests pass?\n\n**Code Quality**:\n- Type hints present?\n- Docstrings clear?\n- Follows coding conventions?\n- Error handling appropriate?\n\n**Architecture**:\n- Fits with existing design?\n- Doesn't duplicate existing code?\n- Proper layer separation (MCP → Core → DB)?\n- Swappable components respected?\n\n**Performance**:\n- No obvious performance issues?\n- Async I/O used appropriately?\n- Database queries optimized?\n\n**Security**:\n- Input validation?\n- No injection vulnerabilities?\n- Secrets not hardcoded?\n\n### Review Process\n\n**Time Expectations**:\n- Small PR (< 100 lines): 1 day\n- Medium PR (100-500 lines): 2 days\n- Large PR (> 500 lines): Split into smaller PRs\n\n**Approval Required**: 1 reviewer minimum\n\n**Merge**: Squash and merge (clean history)\n\n**Review Comments**:\n- **Blocking**: Must be addressed before merge\n- **Non-blocking**: Suggestions, can be addressed later\n- **Nit**: Minor style issues\n\n**Example Review Comment** (good):\n```markdown\n**Blocking**: This function lacks error handling for database failures.\n\nSuggestion: Add try-except block and return meaningful error:\n\n```python\ntry:\n    result = await db.query(...)\nexcept DatabaseError as e:\n    logger.error(\"query_failed\", error=str(e))\n    raise\n```\n```\n\n---\n\n## Continuous Integration (Future)\n\n### GitHub Actions\n\n**Workflow**: `.github/workflows/test.yml`\n\n```yaml\nname: Test\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      falkordb:\n        image: falkordb/falkordb:latest\n        ports:\n          - 6379:6379\n\n      redis:\n        image: redis:7-alpine\n        ports:\n          - 6380:6379\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -e \".[dev]\"\n\n      - name: Run linters\n        run: |\n          black --check src/ tests/\n          isort --check src/ tests/\n          flake8 src/ tests/\n          mypy src/\n\n      - name: Run tests\n        run: |\n          pytest --cov=src --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n```\n\n**On Push**:\n1. Run linters (black, isort, flake8, mypy)\n2. Run unit tests\n3. Run integration tests (Docker services)\n4. Upload coverage to Codecov\n\n**On PR**:\n- Same as push\n- Require passing before merge\n\n**On Tag** (Release):\n1. Build package\n2. Run full test suite\n3. Publish to PyPI (if tests pass)\n\n---\n\n## Project Configuration Files\n\n### pyproject.toml\n\n**Complete Configuration** (`pyproject.toml`):\n```toml\n[build-system]\nrequires = [\"setuptools>=65.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"zapomni-mcp\"\nversion = \"0.1.0\"\ndescription = \"Local-first MCP memory server for AI agents\"\nauthors = [\n    {name = \"Tony\", email = \"your-email@example.com\"}\n]\nreadme = \"README.md\"\nlicense = {text = \"Apache-2.0\"}\nrequires-python = \">=3.10\"\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\nkeywords = [\"mcp\", \"rag\", \"memory\", \"ai\", \"agents\", \"local-first\"]\n\ndependencies = [\n    # MCP\n    \"mcp>=0.1.0\",\n\n    # Database\n    \"falkordb>=4.0.0\",\n    \"redis>=5.0.0\",\n\n    # LLM & Embeddings\n    \"httpx>=0.25.0\",  # For Ollama API\n\n    # Processing\n    \"langchain>=0.1.0\",\n    \"sentence-transformers>=2.2.0\",\n    \"spacy>=3.7.0\",\n    \"rank-bm25>=0.2.2\",\n\n    # Document Processing\n    \"pymupdf>=1.23.0\",  # PDF\n    \"python-docx>=1.0.0\",  # DOCX\n    \"trafilatura>=1.6.0\",  # HTML\n\n    # Utilities\n    \"pydantic>=2.5.0\",\n    \"pydantic-settings>=2.1.0\",\n    \"structlog>=23.2.0\",\n    \"python-dotenv>=1.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    # Testing\n    \"pytest>=7.4.0\",\n    \"pytest-asyncio>=0.21.0\",\n    \"pytest-cov>=4.1.0\",\n    \"pytest-watch>=4.2.0\",\n    \"pytest-xdist>=3.5.0\",\n\n    # Linting & Formatting\n    \"black>=23.12.0\",\n    \"isort>=5.13.0\",\n    \"flake8>=7.0.0\",\n    \"mypy>=1.8.0\",\n    \"pylint>=3.0.0\",\n\n    # Pre-commit\n    \"pre-commit>=3.6.0\",\n\n    # Documentation\n    \"mkdocs>=1.5.0\",\n    \"mkdocs-material>=9.5.0\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/your-org/zapomni\"\nDocumentation = \"https://zapomni.readthedocs.io\"\nRepository = \"https://github.com/your-org/zapomni\"\nIssues = \"https://github.com/your-org/zapomni/issues\"\n\n[project.scripts]\nzapomni-mcp = \"zapomni_mcp.server:main\"\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.black]\nline-length = 100\ntarget-version = ['py310']\ninclude = '\\.pyi?$'\nextend-exclude = '''\n/(\n  # directories\n  \\.eggs\n  | \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nline_length = 100\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\n\n[tool.mypy]\npython_version = \"3.10\"\nstrict = true\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_any_unimported = false\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\ncheck_untyped_defs = true\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"falkordb.*\",\n    \"rank_bm25.*\",\n    \"trafilatura.*\",\n]\nignore_missing_imports = true\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\nmarkers = [\n    \"unit: Unit tests (fast, no external dependencies)\",\n    \"integration: Integration tests (medium speed, uses services)\",\n    \"e2e: End-to-end tests (slow, full stack)\",\n]\naddopts = [\n    \"--strict-markers\",\n    \"--tb=short\",\n    \"--cov-report=term-missing\",\n]\nasyncio_mode = \"auto\"\n\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\n    \"*/tests/*\",\n    \"*/conftest.py\",\n    \"*/__pycache__/*\",\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if __name__ == .__main__.:\",\n    \"if TYPE_CHECKING:\",\n    \"@abstractmethod\",\n]\n```\n\n### .gitignore\n\n```gitignore\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Virtual environments\n.venv\nvenv/\nENV/\nenv/\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n.tox/\n.hypothesis/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n*~\n\n# OS\n.DS_Store\nThumbs.db\n\n# Environment\n.env\n.env.local\n\n# Logs\n*.log\nlogs/\n\n# Database\n*.db\n*.sqlite\n\n# Docker\ndocker-compose.override.yml\n\n# Documentation\ndocs/_build/\nsite/\n\n# Temporary\ntmp/\ntemp/\n*.tmp\n```\n\n### .pre-commit-config.yaml\n\n```yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n      - id: check-merge-conflict\n\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n        language_version: python3.10\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n        args: [\"--max-line-length=100\", \"--extend-ignore=E203\"]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.8.0\n    hooks:\n      - id: mypy\n        additional_dependencies: [pydantic>=2.5.0]\n```\n\n---\n\n## Summary\n\n### Key Takeaways\n\n1. **Monorepo Structure**: Three packages (`zapomni_mcp`, `zapomni_core`, `zapomni_db`) for clean separation\n2. **Conventions Matter**: Black, isort, mypy, type hints, docstrings—consistency is key\n3. **Testing is Essential**: 70% unit, 25% integration, 5% E2E\n4. **Documentation is Code**: Keep docs updated, examples everywhere\n5. **Developer Experience**: Fast feedback loops, clear errors, easy onboarding\n\n### Next Steps\n\n1. **Review and Approve** this document\n2. **Create Project Structure**: Run initial setup\n3. **Begin Development**: Start with Phase 1 (MVP)\n4. **Iterate**: Refine conventions based on experience\n\n---\n\n**Document Status**: Draft v1.0\n**Created**: 2025-11-22\n**Authors**: Tony + Claude Code\n**Last Updated**: 2025-11-22\n\n**Total Lines**: 1800+\n**Total Code Examples**: 50+\n**Total Configurations**: 10+\n\n**Ready for Review**: Yes ✅\n",
  "fileStats": {
    "size": 75755,
    "lines": 2833,
    "lastModified": "2025-11-22T20:53:39.276Z"
  },
  "comments": []
}